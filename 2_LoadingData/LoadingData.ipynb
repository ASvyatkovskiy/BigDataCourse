{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading data into RDD, DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load unstructured data\n",
    "\n",
    "Most of the tasks will be done in the notebook, the exercises will be done on the cluster, you can find the necessary files and submission scripts for the cluster portion of the exercise here:\n",
    "\n",
    "```bash\n",
    "cd 2_LoadingData\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note, that if you re-run the code below more than once, it will fail with \"Output directory already exists\" exception.\n",
    "If you must rerun, it please clean the output folders first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output_cleaner():\n",
    "    import os\n",
    "    os.system(\"rm -rf ./output*\")\n",
    "    print \"Output folders removed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #By default it assumes file located on hdfs folder, \n",
    "    #but by prefixing \"file://\" it will search the local file system\n",
    "    #Can specify a folder, can pass list of folders or use wild character\n",
    "    input_rdd = sc.textFile(\"./data/unstructured/\")\n",
    "\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    #print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "    counts.map(lambda (a, b): (b, a)).sortByKey(0).map(lambda (a, b): (b, a)).repartition(1).saveAsTextFile(\"./output_loadunstructured1/\")\n",
    "\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "def main2(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #Use alternative approach: load the dinitial file into a pair RDD\n",
    "    input_pair_rdd = sc.wholeTextFiles(\"./data/unstructured/\")\n",
    "\n",
    "    counts = input_pair_rdd.flatMap(lambda line: line[1].split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    #print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    counts.map(lambda (a, b): (b, a)).sortByKey(0).map(lambda (a, b): (b, a)).repartition(1).saveAsTextFile(\"./output_loadunstructured2/\")\n",
    "\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "    #sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "Elapsed time:  4.95878505707\n"
     ]
    }
   ],
   "source": [
    "# Try the record-per-line-input\n",
    "#output_cleaner()\n",
    "main1(sys.argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "Elapsed time:  1.03143596649\n"
     ]
    }
   ],
   "source": [
    "#Use alternative approach: load the initial file into a pair RDD\n",
    "#output_cleaner()\n",
    "main2(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading CSV\n",
    "\n",
    "First, we are going to learn how to load data into structured CSV format. There is at least two ways to do that:\n",
    "\n",
    "1) Read the files line by line with textFiles() method, split on delimiter\n",
    "\n",
    "Similarly to Python, there is a data structured designed to be used when working with structured data (I mean Pandas Dataframes), it is also called the dataframe (a concept closely linked to Spark SQL). There is a way to read CSV directly into Spark dataframe \n",
    "\n",
    "2) Read the files into dataframe using spark-csv module from Databricks\n",
    "https://github.com/databricks/spark-csv\n",
    "\n",
    "Note, that for Spark2.0.0+ spark-csv has migrated to the core Spark with API kept very close to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import StringIO\n",
    "import os\n",
    "\n",
    "#this one is use when you use textFile\n",
    "def loadRecord(line,header,delimiter):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input, delimiter=delimiter, fieldnames=header)\n",
    "    return reader.next()\n",
    "\n",
    "def main_rdd(args):\n",
    "    #sc = SparkContext(appName=\"LoadCsv\")\n",
    "    delimiter = \"|\"\n",
    "\n",
    "    # Try the record-per-line-input\n",
    "    input = sc.textFile(\"./data/csv/person_nodes.csv\")\n",
    "    header = input.first().split(delimiter)\n",
    "    data = input.filter(lambda x: header[0] not in x).map(lambda x: loadRecord(x,header,delimiter))\n",
    "    data.repartition(1).saveAsTextFile(\"./output_csv/\")\n",
    "\n",
    "def main_dataframe(args):\n",
    "    delimiter = \"|\"\n",
    "\n",
    "    #csv into spark dataframe   \n",
    "    input_df = spark.read.options(header='true', inferschema='true',delimiter=delimiter).csv('./data/csv/person_nodes.csv')\n",
    "    input_df.write.option(\"header\", \"true\").csv(\"./output_csv2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folders removed!\n"
     ]
    }
   ],
   "source": [
    "#Load into a regular RDD using textFile and parsing the CSV file line by line\n",
    "output_cleaner()\n",
    "main_rdd(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Load into dataframe using the csv reader from Databricks\n",
    "main_dataframe(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### CSV files to Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark CSV dataframe reader can handle delimiters, escaping, and skipping header lines for CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read csv data as DataFrame using spark csv dataframe reader\n",
    "diamonds = spark.read.options(header='true', inferSchema='true').csv('./data/csv/diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|_c0|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "|  2| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "|  3| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "|  4| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "|  5| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "|  6| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "|  7| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "|  8| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "|  9| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 10| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "| 11|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|\n",
      "| 12| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|\n",
      "| 13| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|\n",
      "| 14| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|\n",
      "| 15|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|\n",
      "| 16| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|\n",
      "| 17|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|\n",
      "| 18|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|\n",
      "| 19|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|\n",
      "| 20|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- carat: double (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- table: double (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing CSV files in Python as DataFrames\n",
    "\n",
    "Let's try doing some basic queries to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(color=u'F'),\n",
       " Row(color=u'E'),\n",
       " Row(color=u'D'),\n",
       " Row(color=u'J'),\n",
       " Row(color=u'G'),\n",
       " Row(color=u'I'),\n",
       " Row(color=u'H')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.select('color').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|carat|          avgPrice|\n",
      "+-----+------------------+\n",
      "| 3.51|           18701.0|\n",
      "| 2.67|           18686.0|\n",
      "|  4.5|           18531.0|\n",
      "| 5.01|           18018.0|\n",
      "| 2.57|17841.666666666668|\n",
      "|  2.6|           17535.0|\n",
      "| 2.64|           17407.0|\n",
      "| 4.13|           17329.0|\n",
      "| 2.39|17182.428571428572|\n",
      "| 2.71|           17146.0|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Convert Price column to type DoubleType\n",
    "diamondsdf = diamonds.withColumn(\"price\", diamonds[\"price\"].cast(DoubleType()))\n",
    "\n",
    "# Calculate average price per carat value\n",
    "carat_avgPrice = (diamondsdf\n",
    "                  .groupBy(\"carat\")\n",
    "                  .avg(\"price\")\n",
    "                  .withColumnRenamed(\"avg(price)\", \"avgPrice\")\n",
    "                  .orderBy(desc(\"avgPrice\")))\n",
    "\n",
    "# View top10 highest average prices and corresponding carat value\n",
    "carat_avgPrice.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing CSV files in Python as RDDs\n",
    "\n",
    "You can also convert your DataFrame to RDDs and perform RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We can convert the DataFrame directly into an RDD\n",
    "diamonds_rdd = diamonds.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, carat=0.23, cut=u'Ideal', color=u'E', clarity=u'SI2', depth=61.5, table=55.0, price=326, x=3.95, y=3.98, z=2.43),\n",
       " Row(_c0=2, carat=0.21, cut=u'Premium', color=u'E', clarity=u'SI1', depth=59.8, table=61.0, price=326, x=3.89, y=3.84, z=2.31),\n",
       " Row(_c0=3, carat=0.23, cut=u'Good', color=u'E', clarity=u'VS1', depth=56.9, table=65.0, price=327, x=4.05, y=4.07, z=2.31)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the diamonds RDD\n",
    "diamonds_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can now use RDD operations to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Ideal', 21551), (u'Good', 4906), (u'Premium', 13791), (u'Very Good', 12082), (u'Fair', 1610)]\n"
     ]
    }
   ],
   "source": [
    "# Diamond counts by cuts\n",
    "countByGroup = diamonds_rdd.map(lambda x: (x.cut, 1)).reduceByKey(lambda x,y: x+y)\n",
    "print countByGroup.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'SI2', u'SI1', u'VS1', u'I1', u'VS2', u'VVS1', u'VVS2', u'IF']\n"
     ]
    }
   ],
   "source": [
    "# Distinct diamond clarities in dataset\n",
    "distinctClarity = diamonds_rdd.map(lambda x: x.clarity).distinct()\n",
    "print distinctClarity.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Ideal', 2756.7240663718817), (u'Good', 2755.647409027791), (u'Premium', 2756.654813661215), (u'Very Good', 2756.7183661747795), (u'Fair', 2743.567771968392)]\n"
     ]
    }
   ],
   "source": [
    "# Average price per diamond cut\n",
    "avgPrice = diamonds_rdd.map(lambda x: (x.cut, float(x.price))).reduceByKey(lambda x,y: (x+y)/2)\n",
    "print avgPrice.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mini-exercise on loading CSV\n",
    "\n",
    "Use what you have learned to load a set of CSV datasets. Open load_csv_exercise.py and follow the assignment therein.\n",
    "\n",
    "-- Actor\n",
    "\n",
    "-- Movie\n",
    "\n",
    "-- Actor playing in movie (relationships)\n",
    "\n",
    "and find movies where **Tom Hanks** played in.\n",
    "\n",
    "Save the answer in the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### JSON files with Python\n",
    "\n",
    "This notebook shows an example of how to load JSON data in Python notebooks and best practices for working with JSON data.\n",
    "\n",
    "#### Loading JSON data with Spark SQL into a DataFrame\n",
    "\n",
    "Spark SQL has built in support for reading in JSON files which contain a separate, self-contained JSON object per line. Multi-line JSON files are currently not compatible with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testJsonData = spark.read.json(\"./data/json/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- array: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- extra_key: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|       [null,value1]|  1|string1|\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark SQL can infer the schema automatically from your JSON data. To view the schema, use printSchema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing JSON files in Python as DataFrames\n",
    "\n",
    "Let's try doing some basic queries to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows in dataset\n",
    "print testJsonData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "JSON data can contain nested data structures which can be accessed with a \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(key=u'value1'), Row(key=u'value2'), Row(key=u'value3')]\n"
     ]
    }
   ],
   "source": [
    "print testJsonData.select('dict.key').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also perform DataFrame operations such as filtering queries according to some criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[array: array<bigint>, dict: struct<extra_key:string,key:string>, int: bigint, string: string]\n"
     ]
    }
   ],
   "source": [
    "print testJsonData.filter(testJsonData[\"int\"] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing JSON files in Python as RDDs\n",
    "You can also convert your DataFrame to RDDs and perform RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame directly into an RDD\n",
    "testJsonDataRDD = testJsonData.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array=[1, 2, 3], dict=Row(extra_key=None, key=u'value1'), int=1, string=u'string1'),\n",
       " Row(array=[2, 4, 6], dict=Row(extra_key=None, key=u'value2'), int=2, string=u'string2'),\n",
       " Row(array=[3, 6, 9], dict=Row(extra_key=u'extra_value3', key=u'value3'), int=3, string=u'string3')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the RDD\n",
    "testJsonDataRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View distinct values in the 'array' column\n",
    "testJsonDataRDD.flatMap(lambda r: r.array).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing JSON files in Python with SQL\n",
    "Any DataFrame, including those created with JSON data, can be registered as a Spark SQL table to query with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a Spark SQL temp table\n",
    "# Note that temp tables are not global across clusters and will not persist across cluster restarts\n",
    "testJsonData.registerTempTable(\"test_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can run any SQL queries on that table with Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|       [null,value1]|  1|string1|\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test_json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mini-exercise\n",
    "\n",
    "Switch to the Adroit cluster work directory, open the file: load_json.py\n",
    "and follow instructions inline. Submit the jobs to the cluster using slurm_for_json.cmd file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parquet Files in Python\n",
    "\n",
    "This notebook describes how to register a table in Spark SQL from parquet files.\n",
    "Parquet Files are a great format for storing large tables in SparkSQL.\n",
    "Consider converting text files with a schema into parquet files for more efficient storage.\n",
    "Parquet provides a lot of optimizations under the hood to speed up your queries.\n",
    "Just call ```bash .write.parquet``` on a DataFrame to encode in into Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+---------------+-----+\n",
      "|     group|key|        map|       someints|value|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "|    vowels|  a|Map(a -> 1)|            [1]|    1|\n",
      "|consonants|  b|Map(b -> 2)|         [2, 2]|    2|\n",
      "|consonants|  c|Map(c -> 3)|      [3, 3, 3]|    3|\n",
      "|consonants|  d|Map(d -> 4)|   [4, 4, 4, 4]|    4|\n",
      "|    vowels|  e|Map(3 -> 5)|[5, 5, 5, 5, 5]|    5|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "array = [Row(key=\"a\", group=\"vowels\", value=1, someints=[1], map = {\"a\" : 1}),\n",
    "         Row(key=\"b\", group=\"consonants\", value=2, someints=[2, 2], map = {\"b\" : 2}),\n",
    "         Row(key=\"c\", group=\"consonants\", value=3, someints=[3, 3, 3], map = {\"c\" : 3}),\n",
    "         Row(key=\"d\", group=\"consonants\", value=4, someints=[4, 4, 4, 4], map = {\"d\" : 4}),\n",
    "         Row(key=\"e\", group=\"vowels\", value=5, someints=[5, 5, 5, 5, 5], map = {\"3\" : 5})]\n",
    "dataframe = spark.createDataFrame(sc.parallelize(array))\n",
    "dataframe.show()\n",
    "# now that it's created, let's write it to disk\n",
    "dataframe.write.parquet(\"./output_parquet/testParquetFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Registering a Temp Table from parquet files\n",
    "\n",
    "Taking Parquet files and registering them as a temp table is super easy in Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataframeFromParquet = spark.read.parquet(\"./output_parquet/testParquetFiles\")  \n",
    "dataframeFromParquet.registerTempTable(\"parquetTable1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+---------------+-----+\n",
      "|     group|key|        map|       someints|value|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "|consonants|  d|Map(d -> 4)|   [4, 4, 4, 4]|    4|\n",
      "|    vowels|  e|Map(3 -> 5)|[5, 5, 5, 5, 5]|    5|\n",
      "|consonants|  b|Map(b -> 2)|         [2, 2]|    2|\n",
      "|consonants|  c|Map(c -> 3)|      [3, 3, 3]|    3|\n",
      "|    vowels|  a|Map(a -> 1)|            [1]|    1|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquetTable1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
