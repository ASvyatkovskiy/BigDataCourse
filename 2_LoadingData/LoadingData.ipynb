{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data into RDD, DataFrame\n",
    "\n",
    "\n",
    "In this section, we will explore loading basic data types into RDDs and Dataframes.\n",
    "\n",
    "I assume you have copied some additional jars to be able to use a third-part spark-csv library from Databricks as described in the README, and restarted your notebook like this:\n",
    "\n",
    "```bash\n",
    "IPYTHON_OPTS=\"notebook\" $SPARK_HOME/bin/pyspark --jars $SPARK_HOME/lib_managed/jars/spark-csv_2.10-1.3.0.jar,$SPARK_HOME/lib_managed/jars/commons-csv-1.2.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load unstructured data\n",
    "\n",
    "Most of the tasks will be done in the notebook, the exercises will be done on the cluster, you can find the necessary files and submission scripts here:\n",
    "\n",
    "```bash\n",
    "cd 2_LoadingData\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that if you re-run the code below more than once, it will fail with \"Output directory already exists\" exception.\n",
    "If you must rerun, it please clean the output folders first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_cleaner():\n",
    "    import os\n",
    "    os.system(\"rm -rf ./output*\")\n",
    "    print \"Output folders removed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #By default it assumes file located on hdfs folder, \n",
    "    #but by prefixing \"file://\" it will search the local file system\n",
    "    #Can specify a folder, can pass list of folders or use wild character\n",
    "    input_rdd = sc.textFile(\"./data/unstructured/\")\n",
    "\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    #print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "    counts.map(lambda (a, b): (b, a)).sortByKey(0).map(lambda (a, b): (b, a)).repartition(1).saveAsTextFile(\"./output_loadunstructured1/\")\n",
    "\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "def main2(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #Use alternative approach: load the dinitial file into a pair RDD\n",
    "    input_pair_rdd = sc.wholeTextFiles(\"./data/unstructured/\")\n",
    "\n",
    "    counts = input_pair_rdd.flatMap(lambda line: line[1].split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    #print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    counts.map(lambda (a, b): (b, a)).sortByKey(0).map(lambda (a, b): (b, a)).repartition(1).saveAsTextFile(\"./output_loadunstructured2/\")\n",
    "\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "    #sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "Elapsed time:  1.56508588791\n"
     ]
    }
   ],
   "source": [
    "# Try the record-per-line-input\n",
    "#output_cleaner()\n",
    "main1(sys.argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "Elapsed time:  1.06222510338\n"
     ]
    }
   ],
   "source": [
    "#Use alternative approach: load the initial file into a pair RDD\n",
    "#output_cleaner()\n",
    "main2(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV\n",
    "\n",
    "First, we are going to learn how to load data into structured CSV format. There is at least two ways to do that:\n",
    "\n",
    "1) Read the files line by line with textFiles() method, split on delimiter\n",
    "\n",
    "Similarly to Python, there is a data structured designed to be used when working with structured data (I mean Pandas Dataframes), it is also called the dataframe (a concept closely linked to Spark SQL). There is a way to read CSV directly into Spark dataframe \n",
    "\n",
    "2) Read the files into dataframe using spark-csv module from Databricks\n",
    "https://github.com/databricks/spark-csv\n",
    "\n",
    "You do not need to install it, I did the work for you by adding the pre-built Jars into the appropriate /lib folder...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#from pyspark.sql import SQLContext\n",
    "import csv\n",
    "import sys\n",
    "import StringIO\n",
    "import os\n",
    "\n",
    "#this one is use when you use textFile\n",
    "def loadRecord(line,header,delimiter):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input, delimiter=delimiter, fieldnames=header)\n",
    "    return reader.next()\n",
    "\n",
    "def main_rdd(args):\n",
    "    #sc = SparkContext(appName=\"LoadCsv\")\n",
    "    delimiter = \"|\"\n",
    "\n",
    "    # Try the record-per-line-input\n",
    "    input = sc.textFile(\"./data/csv/person_nodes.csv\")\n",
    "    header = input.first().split(delimiter)\n",
    "    data = input.filter(lambda x: header[0] not in x).map(lambda x: loadRecord(x,header,delimiter))\n",
    "    data.repartition(1).saveAsTextFile(\"./output_csv/\")\n",
    "\n",
    "def main_dataframe(args):\n",
    "    #sc = SparkContext(appName=\"LoadCsv\")\n",
    "\n",
    "    delimiter = \"|\"\n",
    "\n",
    "    #csv into spark dataframe   \n",
    "    #this requires using the databricks/spark-csv\n",
    "    #spark-submit --packages com.databricks:spark-csv_2.10:1.3.0\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    input_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true',delimiter=delimiter).load('./data/csv/person_nodes.csv')\n",
    "    input_df.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"./output_csv2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folders removed!\n"
     ]
    }
   ],
   "source": [
    "#Load into a regular RDD using textFile and parsing the CSV file line by line\n",
    "output_cleaner()\n",
    "main_rdd(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load into dataframe using the csv reader from Databricks\n",
    "main_dataframe(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV files to Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use the Spark CSV package to parse CSV files. The Spark CSV package can handle delimiters, escaping, and skipping header lines for CSV files. The open source code for Spark CSV is available here: http://spark-packages.org/package/databricks/spark-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read csv data as DataFrame using spark-csv package\n",
    "diamonds = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferSchema='true').load('./data/csv/diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|   |carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "|  2| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "|  3| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "|  4| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "|  5| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "|  6| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "|  7| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "|  8| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "|  9| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 10| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "| 11|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|\n",
      "| 12| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|\n",
      "| 13| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|\n",
      "| 14| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|\n",
      "| 15|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|\n",
      "| 16| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|\n",
      "| 17|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|\n",
      "| 18|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|\n",
      "| 19|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|\n",
      "| 20|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- : integer (nullable = true)\n",
      " |-- carat: double (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- table: double (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyzing CSV files in Python as DataFrames\n",
    "\n",
    "Let's try doing some basic queries to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(color=u'D'),\n",
       " Row(color=u'E'),\n",
       " Row(color=u'F'),\n",
       " Row(color=u'G'),\n",
       " Row(color=u'H'),\n",
       " Row(color=u'I'),\n",
       " Row(color=u'J')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.select('color').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|carat|          avgPrice|\n",
      "+-----+------------------+\n",
      "| 3.51|           18701.0|\n",
      "| 2.67|           18686.0|\n",
      "|  4.5|           18531.0|\n",
      "| 5.01|           18018.0|\n",
      "| 2.57|17841.666666666668|\n",
      "|  2.6|           17535.0|\n",
      "| 2.64|           17407.0|\n",
      "| 4.13|           17329.0|\n",
      "| 2.39|17182.428571428572|\n",
      "| 2.71|           17146.0|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Convert Price column to type DoubleType\n",
    "diamondsdf = diamonds.withColumn(\"price\", diamonds[\"price\"].cast(DoubleType()))\n",
    "\n",
    "# Calculate average price per carat value\n",
    "carat_avgPrice = (diamondsdf\n",
    "                  .groupBy(\"carat\")\n",
    "                  .avg(\"price\")\n",
    "                  .withColumnRenamed(\"avg(price)\", \"avgPrice\")\n",
    "                  .orderBy(desc(\"avgPrice\")))\n",
    "\n",
    "# View top10 highest average prices and corresponding carat value\n",
    "carat_avgPrice.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyzing CSV files in Python as RDDs\n",
    "\n",
    "You can also convert your DataFrame to RDDs and perform RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can convert the DataFrame directly into an RDD\n",
    "diamonds_rdd = diamonds.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(=1, carat=0.23, cut=u'Ideal', color=u'E', clarity=u'SI2', depth=61.5, table=55.0, price=326, x=3.95, y=3.98, z=2.43),\n",
       " Row(=2, carat=0.21, cut=u'Premium', color=u'E', clarity=u'SI1', depth=59.8, table=61.0, price=326, x=3.89, y=3.84, z=2.31),\n",
       " Row(=3, carat=0.23, cut=u'Good', color=u'E', clarity=u'VS1', depth=56.9, table=65.0, price=327, x=4.05, y=4.07, z=2.31)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the diamonds RDD\n",
    "diamonds_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can now use RDD operations to analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Ideal', 21551), (u'Premium', 13791), (u'Very Good', 12082), (u'Fair', 1610), (u'Good', 4906)]\n"
     ]
    }
   ],
   "source": [
    "# Diamond counts by cuts\n",
    "countByGroup = diamonds_rdd.map(lambda x: (x.cut, 1)).reduceByKey(lambda x,y: x+y)\n",
    "print countByGroup.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distinct diamond clarities in dataset\n",
    "distinctClarity = diamonds_rdd.map(lambda x: x.clarity).distinct()\n",
    "print distinctClarity.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Ideal', 9898.37827251098), (u'Premium', 9898.418552178284), (u'Very Good', 9893.841959614465), (u'Fair', 9801.31414059171), (u'Good', 9780.548613842624)]\n"
     ]
    }
   ],
   "source": [
    "# Average price per diamond cut\n",
    "avgPrice = diamonds_rdd.map(lambda x: (x.cut, float(x.price))).reduceByKey(lambda x,y: (x+y)/2)\n",
    "print avgPrice.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mini-exercise on loading CSV\n",
    "\n",
    "Use what you have learned to load a set of CSV datasets. Open load_csv_exercise.py and follow the assignment therein.\n",
    "\n",
    "-- Actor\n",
    "\n",
    "-- Movie\n",
    "\n",
    "-- Actor playing in movie (relationships)\n",
    "\n",
    "and find movies where **Tom Hanks** played in.\n",
    "\n",
    "Save the answer in the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON files with Python\n",
    "\n",
    "This notebook shows an example of how to load JSON data in Python notebooks and best practices for working with JSON data.\n",
    "\n",
    "#### Loading JSON data with Spark SQL into a DataFrame\n",
    "\n",
    "Spark SQL has built in support for reading in JSON files which contain a separate, self-contained JSON object per line. Multi-line JSON files are currently not compatible with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testJsonData = sqlContext.read.format('json').load(\"./data/json/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- array: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- extra_key: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|       [null,value1]|  1|string1|\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Spark SQL can infer the schema automatically from your JSON data. To view the schema, use printSchema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyzing JSON files in Python as DataFrames\n",
    "\n",
    "Let's try doing some basic queries to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows in dataset\n",
    "print testJsonData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON data can contain nested data structures which can be accessed with a \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(key=u'value1'), Row(key=u'value2'), Row(key=u'value3')]\n"
     ]
    }
   ],
   "source": [
    "print testJsonData.select('dict.key').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform DataFrame operations such as filtering queries according to some criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[array: array<bigint>, dict: struct<extra_key:string,key:string>, int: bigint, string: string]\n"
     ]
    }
   ],
   "source": [
    "print testJsonData.filter(testJsonData[\"int\"] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing JSON files in Python as RDDs\n",
    "You can also convert your DataFrame to RDDs and perform RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame directly into an RDD\n",
    "testJsonDataRDD = testJsonData.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(array=[1, 2, 3], dict=Row(extra_key=None, key=u'value1'), int=1, string=u'string1'),\n",
       " Row(array=[2, 4, 6], dict=Row(extra_key=None, key=u'value2'), int=2, string=u'string2'),\n",
       " Row(array=[3, 6, 9], dict=Row(extra_key=u'extra_value3', key=u'value3'), int=3, string=u'string3')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the RDD\n",
    "testJsonDataRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 1, 3, 9]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View distinct values in the 'array' column\n",
    "testJsonDataRDD.flatMap(lambda r: r.array).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing JSON files in Python with SQL\n",
    "Any DataFrame, including those created with JSON data, can be registered as a Spark SQL table to query with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Spark SQL temp table\n",
    "# Note that temp tables are not global across clusters and will not persist across cluster restarts\n",
    "testJsonData.registerTempTable(\"test_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run any SQL queries on that table with Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|       [null,value1]|  1|string1|\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM test_json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-exercise\n",
    "\n",
    "Switch to the Adroit cluster work directory, open the file: load_json.py\n",
    "and follow instructions inline. Submit the jobs to the cluster using slurm_for_json.cmd file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Best Practice Tip: Specify a schema for your JSON input data.\n",
    "\n",
    "By specifying a schema, you can speed up your Spark job by cutting down the time Spark uses to infer the schema.\n",
    "In addition, if you have a lot of keys that you don't care about, you can filter for only the keys you need.\n",
    "Finally, too many keys in your JSON data can trigger an OOM error on your Spark Driver when you infer the schema.\n",
    "The first step is to create a data point with your ideal schema from your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonSchema = sqlContext.jsonRDD(sc.parallelize([\"\"\"{\"string\":\"string1\",\"int\":1,\"dict\": {\"key\": \"value1\"}}\"\"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonSchema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the JSON data and specify a schema using the inferred schema above.\n",
    "You can specify the schema directory by following the upstream Spark SQL Guide. If there are too many missing values, Spark may not be able to accurately infer the schema therefore directly providing the schema is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testJsonDataWithoutExtraKey = sqlContext.read.format('json').schema(jsonSchema.schema).load(\"./data/json/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonDataWithoutExtraKey.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying a schema, we excluded the 'array' column and the 'extra_key' in the 'dict' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+\n",
      "|    dict|int| string|\n",
      "+--------+---+-------+\n",
      "|[value1]|  1|string1|\n",
      "|[value2]|  2|string2|\n",
      "|[value3]|  3|string3|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonDataWithoutExtraKey.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice Tip: ETL your JSON data to parquet for faster query speeds.\n",
    "\n",
    "With a very large data set, ETL-ing your data to parquet can speed up your query speeds. Adjust the number of partitions to write parquet files that are at least 128MB in size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testJsonData.write.format(\"parquet\").save(\"./output_parquet/testJsonParquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folders removed!\n"
     ]
    }
   ],
   "source": [
    "#If you need to clean the output folders\n",
    "output_cleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Files in Python\n",
    "\n",
    "This notebook describes how to register a table in Spark SQL from parquet files.\n",
    "Parquet Files are a great format for storing large tables in SparkSQL.\n",
    "Consider converting text files with a schema into parquet files for more efficient storage.\n",
    "Parquet provides a lot of optimizations under the hood to speed up your queries.\n",
    "Just call ```bash .write.parquet``` on a DataFrame to encode in into Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+---------------+-----+\n",
      "|     group|key|        map|       someints|value|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "|    vowels|  a|Map(a -> 1)|            [1]|    1|\n",
      "|consonants|  b|Map(b -> 2)|         [2, 2]|    2|\n",
      "|consonants|  c|Map(c -> 3)|      [3, 3, 3]|    3|\n",
      "|consonants|  d|Map(d -> 4)|   [4, 4, 4, 4]|    4|\n",
      "|    vowels|  e|Map(3 -> 5)|[5, 5, 5, 5, 5]|    5|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "array = [Row(key=\"a\", group=\"vowels\", value=1, someints=[1], map = {\"a\" : 1}),\n",
    "         Row(key=\"b\", group=\"consonants\", value=2, someints=[2, 2], map = {\"b\" : 2}),\n",
    "         Row(key=\"c\", group=\"consonants\", value=3, someints=[3, 3, 3], map = {\"c\" : 3}),\n",
    "         Row(key=\"d\", group=\"consonants\", value=4, someints=[4, 4, 4, 4], map = {\"d\" : 4}),\n",
    "         Row(key=\"e\", group=\"vowels\", value=5, someints=[5, 5, 5, 5, 5], map = {\"3\" : 5})]\n",
    "dataframe = sqlContext.createDataFrame(sc.parallelize(array))\n",
    "dataframe.show()\n",
    "# now that it's created, let's write it to disk\n",
    "dataframe.write.parquet(\"./output_parquet/testParquetFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Registering a Temp Table from parquet files\n",
    "\n",
    "Taking Parquet files and registering them as a temp table is super easy in Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframeFromParquet = sqlContext.read.parquet(\"./output_parquet/testParquetFiles\")  \n",
    "dataframeFromParquet.registerTempTable(\"parquetTable1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+---------------+-----+\n",
      "|     group|key|        map|       someints|value|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "|    vowels|  a|Map(a -> 1)|            [1]|    1|\n",
      "|consonants|  b|Map(b -> 2)|         [2, 2]|    2|\n",
      "|consonants|  c|Map(c -> 3)|      [3, 3, 3]|    3|\n",
      "|consonants|  d|Map(d -> 4)|   [4, 4, 4, 4]|    4|\n",
      "|    vowels|  e|Map(3 -> 5)|[5, 5, 5, 5, 5]|    5|\n",
      "+----------+---+-----------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM parquetTable1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
