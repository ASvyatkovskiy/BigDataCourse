{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data into RDD, DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tasks in this section will be done in the notebook, some will be done on the cluster, you can find the necessary files and submission scripts here:\n",
    "\n",
    "```bash\n",
    "cd 2_LoadingData\n",
    "```\n",
    "\n",
    "By default, Spark will not override the output folder. For `RDDs`, one can pass a config parameter to alter that `.config(\"spark.hadoop.validateOutputSpecs\", \"false\")` for `DataFrame`s we will explicitely set the write mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize spark session and spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "try:\n",
    "    sc\n",
    "except NameError:    \n",
    "    spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").appName(\"BD course\").config(\"spark.hadoop.validateOutputSpecs\", \"false\").getOrCreate()\n",
    "    sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load unstructured data\n",
    "\n",
    "One of the first things we need to learn is how to read the data into Spark RDDs and dataframes. Spark provides a battery-included API for reading structured data in most data formats (CSV, JSON, Parquet) as well as unstructured data (plain text files, server logs, etc).\n",
    "\n",
    "### Reading plain text\n",
    "\n",
    "`textFile` and `wholeTextFiles` are functions to read in plain unstructured text.\n",
    "\n",
    "1. `textFile` reads data line by line creating an RDD where each entry corresponds to a line (kind of like readlines() in Python)\n",
    "1. `wholeTextFiles` reads the whole file into a pair RDD: (file path, context of the whole file as string)\n",
    "\n",
    "\n",
    "Following code demonstrate that on an example of the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #By default it assumes file located on hdfs folder, \n",
    "    #but by prefixing \"file://\" it will search the local file system\n",
    "    #Can specify a folder, can pass list of folders or use wild character\n",
    "    input_rdd = sc.textFile(\"./data/unstructured/\")\n",
    "\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print (\"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\")\n",
    "    print (counts.takeOrdered(10, key=lambda x: -x[1]))\n",
    "\n",
    "    counts.map(lambda x: (x[1],x[0])).sortByKey(0).map(lambda x: (x[1],x[0])).repartition(1).saveAsTextFile(\"./output_loadunstructured1/\")\n",
    " \n",
    "    end = time.time()\n",
    "    print (\"Elapsed time: \", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "[('the', 22635), ('of', 11167), ('and', 11086), ('to', 10707), ('a', 10433), ('I', 10183), ('in', 7006), ('that', 6911), ('was', 6779), ('his', 4955)]\n",
      "Elapsed time:  1.7705183029174805\n"
     ]
    }
   ],
   "source": [
    "# Try the record-per-line-input\n",
    "main1(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2(args):\n",
    "    start = time.time()\n",
    "\n",
    "    #Use alternative approach: load the dinitial file into a pair RDD\n",
    "    input_pair_rdd = sc.wholeTextFiles(\"./data/unstructured/\")\n",
    "\n",
    "    counts = input_pair_rdd.flatMap(lambda line: line[1].split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print (\"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\")\n",
    "    print (counts.takeOrdered(10, key=lambda x: -x[1]))\n",
    "    counts.map(lambda x: (x[1],x[0])).sortByKey(0).map(lambda x: (x[1],x[0])).repartition(1).saveAsTextFile(\"./output_loadunstructured2/\")\n",
    "\n",
    "    end = time.time()\n",
    "    print (\"Elapsed time: \", (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "[('the', 22635), ('of', 11167), ('and', 11086), ('to', 10707), ('a', 10433), ('I', 10183), ('in', 7006), ('that', 6911), ('was', 6779), ('his', 4955)]\n",
      "Elapsed time:  1.3675246238708496\n"
     ]
    }
   ],
   "source": [
    "#Use alternative approach: load the initial file into a pair RDD\n",
    "main2(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV\n",
    "\n",
    "Next, we are going to learn how to load data into structured format like CSV. There is at least two ways to do that:\n",
    "\n",
    "1. Read the files line by line with `textFiles()` method, split on delimiter (not recommended). This will produced an RDD which is a data structure optimized for row-oriented analysis and functional primitives like `map` and `filter`\n",
    "1. Read the CSV files using the built in `DataFrameReader` (recommended). This will produce a dataframe, which is a data structure optimized for column-oriented analysis and relational primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import os\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "\n",
    "#this one is use when you use textFile\n",
    "def loadRecord(line,header,delimiter):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO(line)\n",
    "    reader = csv.DictReader(input, delimiter=delimiter, fieldnames=header)\n",
    "    return next(reader)\n",
    "\n",
    "def main_rdd(args):\n",
    "    #sc = SparkContext(appName=\"LoadCsv\")\n",
    "    delimiter = \"|\"\n",
    "\n",
    "    # Try the record-per-line-input\n",
    "    input = sc.textFile(\"./data/csv/person_nodes.csv\")\n",
    "    header = input.first().split(delimiter)\n",
    "    data = input.filter(lambda x: header[0] not in x).map(lambda x: loadRecord(x,header,delimiter))\n",
    "    data.repartition(1).saveAsTextFile(\"./output_csv/\")\n",
    "\n",
    "def main_dataframe(args):\n",
    "    delimiter = \"|\"\n",
    "\n",
    "    #csv into spark dataframe   \n",
    "    input_df = spark.read.options(header='true', inferschema='true',delimiter=delimiter).csv('./data/csv/person_nodes.csv')\n",
    "    input_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./output_csv2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load into a regular RDD using textFile and parsing the CSV file line by line\n",
    "main_rdd(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load into dataframe using the csv reader from Databricks\n",
    "main_dataframe(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: analyzing a diamonds dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In a simialr way as before, we are going to read the CSV file into dataframe.\n",
    "Spark DataFrameReader can handle delimiters, escaping, and can optionally skip header line for CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv data as DataFrame using spark csv dataframe reader\n",
    "diamonds = spark.read.options(header='true', inferSchema='true').csv('./data/csv/diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|_c0|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "|  2| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "|  3| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "|  4| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "|  5| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "|  6| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "|  7| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "|  8| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "|  9| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 10| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- carat: double (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- table: double (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try doing some basic queries to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|color|\n",
      "+-----+\n",
      "|    F|\n",
      "|    E|\n",
      "|    D|\n",
      "|    J|\n",
      "|    G|\n",
      "|    I|\n",
      "|    H|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.select('color').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us try to estimate an average price per carat. As you have noticed, the price column is an integer. This can result in a loss of precision as we do averaging. So first of all we will cast this column to double type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Convert Price column to type DoubleType\n",
    "diamondsdf = diamonds.withColumn(\"price\", diamonds[\"price\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use \"groupby-aggregate function\" to calculate the average. This creates a column with default name \"avg(price)\" which we rename to something easier to type. Finally, we order output by price in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|carat|          avgPrice|\n",
      "+-----+------------------+\n",
      "| 3.51|           18701.0|\n",
      "| 2.67|           18686.0|\n",
      "|  4.5|           18531.0|\n",
      "| 5.01|           18018.0|\n",
      "| 2.57|17841.666666666668|\n",
      "|  2.6|           17535.0|\n",
      "| 2.64|           17407.0|\n",
      "| 4.13|           17329.0|\n",
      "| 2.39|17182.428571428572|\n",
      "| 2.71|           17146.0|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average price per carat value\n",
    "carat_avgPrice = (diamondsdf\n",
    "                  .groupBy(\"carat\")\n",
    "                  .avg(\"price\")\n",
    "                  .withColumnRenamed(\"avg(price)\", \"avgPrice\")\n",
    "                  .orderBy(desc(\"avgPrice\")))\n",
    "\n",
    "# View top10 highest average prices and corresponding carat value\n",
    "carat_avgPrice.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analyzing CSV files in Python as RDDs (not recommended approach)\n",
    "\n",
    "In principle, one can use `RDD` to analyze structured data as well, but it seems to be less concenient, especially if the logic of your analysis can be expressed using SQL-like relational primitives.\n",
    "\n",
    "We will now convert our diamonds DataFrame into RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert the DataFrame directly into an RDD\n",
    "diamonds_rdd = diamonds.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, carat=0.23, cut='Ideal', color='E', clarity='SI2', depth=61.5, table=55.0, price=326, x=3.95, y=3.98, z=2.43),\n",
       " Row(_c0=2, carat=0.21, cut='Premium', color='E', clarity='SI1', depth=59.8, table=61.0, price=326, x=3.89, y=3.84, z=2.31),\n",
       " Row(_c0=3, carat=0.23, cut='Good', color='E', clarity='VS1', depth=56.9, table=65.0, price=327, x=4.05, y=4.07, z=2.31)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the diamonds RDD\n",
    "diamonds_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can now use RDD operations to analyze the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ideal', 21551), ('Premium', 13791), ('Good', 4906), ('Very Good', 12082), ('Fair', 1610)]\n"
     ]
    }
   ],
   "source": [
    "# Diamond counts by cuts\n",
    "countByGroup = diamonds_rdd.map(lambda x: (x.cut, 1)).reduceByKey(lambda x,y: x+y)\n",
    "print (countByGroup.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SI2', 'SI1', 'VS1', 'VS2', 'VVS2', 'VVS1', 'I1', 'IF']\n"
     ]
    }
   ],
   "source": [
    "# Distinct diamond clarities in dataset\n",
    "distinctClarity = diamonds_rdd.map(lambda x: x.clarity).distinct()\n",
    "print (distinctClarity.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ideal', 2756.7240663718817), ('Premium', 2756.654813661215), ('Good', 2755.647409027791), ('Very Good', 2756.7183661747795), ('Fair', 2743.567771968392)]\n"
     ]
    }
   ],
   "source": [
    "# Average price per diamond cut\n",
    "avgPrice = diamonds_rdd.map(lambda x: (x.cut, float(x.price))).reduceByKey(lambda x,y: (x+y)/2)\n",
    "print (avgPrice.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise: load a CSV file and analyze it\n",
    "\n",
    "Use what you have learned to load a set of `CSV` datasets. Open **load_csv_exercise.py** and follow the assignment therein.\n",
    "\n",
    "1. Actor\n",
    "1. Movie\n",
    "1. Actor playing in movie (relationships)\n",
    "\n",
    "and find movies where **Tom Hanks** played in.\n",
    "\n",
    "Save the answer in the `JSON` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading JSON\n",
    "\n",
    "The best and probably the only reasonable way to load JSON files is using the Spark DataFrameReader.\n",
    "Spark SQL has built in support for reading in JSON files which contain a separate, self-contained JSON object per line. \n",
    "\n",
    "**Note: Multi-line JSON files are currently not compatible with Spark SQL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "testJsonData = spark.read.json(\"./data/json/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- array: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- extra_key: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|       [null,value1]|  1|string1|\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Spark SQL can infer the schema automatically from your JSON data. To view the schema, use `printSchema`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's now try doing some basic queries to understand the dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows in dataset\n",
    "print (testJsonData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON data can contain nested data structures which can be accessed with a \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   key|\n",
      "+------+\n",
      "|value1|\n",
      "|value2|\n",
      "|value3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.select('dict.key').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform DataFrame operations such as filtering queries according to some criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.filter(testJsonData[\"int\"] > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing JSON files in Python with SQL\n",
    "Any DataFrame, including those created with JSON data, can be registered as a Spark SQL table to query with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark SQL temp table\n",
    "# Note that temp tables are not global across clusters and will not persist across cluster restarts\n",
    "testJsonData.registerTempTable(\"test_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run any SQL queries on that table with Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|       [null,value1]|  1|string1|\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test_json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-exercise\n",
    "\n",
    "Switch to the Adroit cluster work directory, open the file: **load_json.py**\n",
    "and follow instructions inline. Submit the jobs to the cluster using **slurm_for_json.cmd** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
