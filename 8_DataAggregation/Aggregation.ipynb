{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aggregation\n",
    "\n",
    "## Key-value RDD\n",
    "\n",
    "Before we continue with the hands-on part let us consider the schematic representation of the word-count using `groupByKey` and `reductByKey` approach. This is a classic example that helps underdstand the shuffle stage better and the effect of map-side combiner:\n",
    "\n",
    "<img src=\"./debug/groupbykey.png\">\n",
    "\n",
    "<img src=\"./debug/reducebykey.png\">\n",
    "\n",
    "### Example: Calculating mean\n",
    "\n",
    "Let us now continue with a simple toy example of calculating a mean. We will consider a dataset of baby names (a small fraction of it), and will try to calculate an average age for each name.\n",
    "\n",
    "To calculate mean in a distributed setting we can keep track of sum of elements and the number of elements in each partition, and then combine those.\n",
    "\n",
    "This is very easy to achieve with `aggregateByKey` action. The `aggregateByKey` takes following 3 arguments:\n",
    "  1. zero for aggregation buffer within a partition. Since we are calculating sum of elements in each partition and the number of elements our zero is a following tuple: (0,0)\n",
    "  1. `increment` function performing summation and count within each partition\n",
    "  1. `combine` function to combine results from all partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import scala.collection.mutable.ListBuffer\n",
    "\n",
    "var babyNames = sc.parallelize(List((\"David\", 6), (\"Abby\", 4), (\"David\", 5), (\"Abby\", 5)))\n",
    "\n",
    "var zero = (0,0)\n",
    "\n",
    "//takes buffer and current value in the RDD\n",
    "def increment(buffer: Tuple2[Int,Int], value: Int) : Tuple2[Int,Int] = {\n",
    "    (buffer._1 + value, buffer._2 + 1)\n",
    "}\n",
    "//takes buffers from various partitions after work is done in each \n",
    "def combine(buffer1: Tuple2[Int,Int], buffer2: Tuple2[Int,Int]) : Tuple2[Int,Int] = {\n",
    "    (buffer1._1 + buffer2._1, buffer1._2 + buffer2._2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Abby,(9,2))\n",
      "(David,(11,2))\n"
     ]
    }
   ],
   "source": [
    "val after_aggregation = babyNames.aggregateByKey(zero)(increment,combine)\n",
    "for (d <- after_aggregation.take(2)) {\n",
    "    println(d)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Abby,4.5)\n",
      "(David,5.5)\n"
     ]
    }
   ],
   "source": [
    "val result = after_aggregation.mapValues(sum_and_count => 1.0 * sum_and_count._1 / sum_and_count._2)\n",
    "for (d <- result.collect()) {\n",
    "    println(d)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dataframe groupBy aggregations\n",
    "\n",
    "If you are working with dataframes, there is no need to convert to `RDD` or `PairRDD` to perform some aggregation operation. \n",
    "\n",
    "\n",
    "Spark SQL has many powerful aggregates, and thanks to its optimizer it can be easy to combine many aggregates into one single action/query. Like with Pandas dataframes, `groupBy` returns a special `GroupedData` object on which we can ask for a certain aggregation to be performed. \n",
    "\n",
    "`min`, `max`, `avg`, and `sum` are all implemented as convenience functions directly on `GroupedData`, and more can be specified by providing the expressions to `agg`. Once you specify the aggregates you want to compute, you can get the results back as a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|David|  6|\n",
      "| Abby|  4|\n",
      "|David|  5|\n",
      "| Abby|  5|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Aggregation\").getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "val babyNamesDF = babyNames.toDF(\"Name\",\"Age\")\n",
    "babyNamesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| Name|avg(Age)|\n",
      "+-----+--------+\n",
      "| Abby|     4.5|\n",
      "|David|     5.5|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val result = babyNamesDF.groupBy(\"Name\").avg(\"Age\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+\n",
      "|summary| Name|              Age|\n",
      "+-------+-----+-----------------+\n",
      "|  count|    4|                4|\n",
      "|   mean| null|              5.0|\n",
      "| stddev| null|0.816496580927726|\n",
      "|    min| Abby|                4|\n",
      "|    max|David|                6|\n",
      "+-------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "babyNamesDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Recalculate average age of children using the dataframe `agg` API\n",
    "Hint: you would need to import\n",
    "```scala\n",
    "import org.apache.spark.sql.functions.avg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined aggregation functions on Dataframes (UDAF)\n",
    "\n",
    "We have seen that there is a number of basic aggregation functions available in Spark to work on grouped data.\n",
    "What if we need to solve an advanced proble, i.e. if we work with Spark ML and dealing with some columns containing vectors?\n",
    "\n",
    "The solution would be to use `UDAF`, which allows you to derive a class from `UserDefinedAggregateFunction`, and overload a set of methods to achieve desired performance. Following is an example UDAF to calculate (again) a mean of a collection of elements:\n",
    "\n",
    "```scala\n",
    "class MyUDAF() extends UserDefinedAggregateFunction {\n",
    " \n",
    "  // Input Data Type Schema\n",
    "  def inputSchema: StructType = StructType(Array(StructField(\"item\", DoubleType)))\n",
    " \n",
    "  // Intermediate Schema\n",
    "  def bufferSchema = StructType(Array(\n",
    "    StructField(\"sum\", DoubleType),\n",
    "    StructField(\"cnt\", LongType)\n",
    "  ))\n",
    " \n",
    "  // Returned Data Type .\n",
    "  def dataType: DataType = DoubleType\n",
    " \n",
    "  // Self-explaining\n",
    "  def deterministic = true\n",
    " \n",
    "  // This function is called whenever key changes\n",
    "  def initialize(buffer: MutableAggregationBuffer) = {\n",
    "    buffer(0) = 0.toDouble // set sum to zero\n",
    "    buffer(1) = 0L // set number of items to 0\n",
    "  }\n",
    " \n",
    "  // Iterate over each entry of a group\n",
    "  def update(buffer: MutableAggregationBuffer, input: Row) = {\n",
    "    buffer(0) = buffer.getDouble(0) + input.getDouble(0)\n",
    "    buffer(1) = buffer.getLong(1) + 1\n",
    "  }\n",
    " \n",
    "  // Merge two partial aggregates\n",
    "  def merge(buffer1: MutableAggregationBuffer, buffer2: Row) = {\n",
    "    buffer1(0) = buffer1.getDouble(0) + buffer2.getDouble(0)\n",
    "    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)\n",
    "  }\n",
    " \n",
    "  // Called after all the entries are exhausted.\n",
    "  def evaluate(buffer: Row) = {\n",
    "    buffer.getDouble(0)/buffer.getLong(1).toDouble\n",
    "  }\n",
    " \n",
    "}\n",
    "```\n",
    "\n",
    "More advanced example of UDAF will be in a cluster portion of the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Aggregator class\n",
    "\n",
    "Similarly to UDAF for dataframes, one can define advanced aggregators for dataset API.\n",
    "\n",
    "Aggregators provide a mechanism for adding up all of the elements in a DataSet (or in each group of a GroupedDataset), returning a single result. An Aggregator is similar to a UDAF, but the interface is expressed in terms of JVM objects instead of as a Row. Any class that extends `Aggregator[A, B, C]` can be used, where:\n",
    "  1. A - specifies the input type to the aggregator\n",
    "  1. B - specifies the intermediate type durring aggregation\n",
    "  1. C - specifies the final type output by the aggregation\n",
    "  \n",
    "\n",
    "Below is a simple Aggregator that calclulates an average:\n",
    "\n",
    "```scala\n",
    "case class AggData(name: String, age: Int)\n",
    "\n",
    "class Average extends Aggregator[AggData, Tuple2[Int,Int], Double] with Serializable {\n",
    "  def zero: Tuple2[Int,Int] = (0,0)                                                 // The initial value for aggregation within each partitions.\n",
    "  def reduce(buffer: Tuple2[Int,Int], value: AggData) = (b._1 + a.age, b._2 + 1)    // Add an element to the running total, this is what we called `increment` method before\n",
    "  def merge(buffer1: Tuple2[Int,Int], buffer2: Tuple2[Int,Int]) = b1 + b2            // Merge intermediate values, this is what we called `combine` method.\n",
    "  def finish(buffer: Tuple2[Int,Int]) = {  \n",
    "      buffer._1/buffer._2                // Return the final result. Can do various map/filter transformations on this step as well!\n",
    "  }\n",
    "}.toColumn\n",
    "```            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
