{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY1\n",
    "\n",
    "\n",
    "# From Python to Spark (PySpark)\n",
    "\n",
    "PySpark is a Python API for Apache Spark\n",
    "\n",
    "Getting started with Spark for Python programmers is particularly easy if one is using a certain programming style: \n",
    "maps, filters and lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda functions in Python\n",
    "\n",
    "Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, using a construct called \"lambda\".\n",
    "\n",
    "Sometimes you need to pass a function as an argument, or you want to do a short but complex operation multiple times. You could define your function the normal way, or you could make a lambda function, a mini-function that returns the result of a single expression. The two definitions are completely identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##traditional named function\n",
    "def add(a,b): return a+b\n",
    "\n",
    "##lambda function\n",
    "add2 = lambda a,b: a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the lambda function is that it is in itself an expression, and can be used inside another statement. Here's an example using the map function, which calls a function on every element in a list, and returns a list of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "squares = map(lambda a: a*a, [1,2,3,4,5])\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: mapping the list in Python\n",
    "\n",
    "To get started with the exercise:\n",
    "\n",
    "```bash\n",
    "cd from_python_to_spark/\n",
    "```\n",
    "and inspect the exercise1.py file therein.\n",
    "\n",
    "Suppose you need to perform a transformation on a list of element. For instance, to calculate a square of each element of the list. One way to write this in Python would be as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of squares:  [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    squares.append(number*number)\n",
    "    # Now, squares should have [1,4,9,16,25]\n",
    "print \"List of squares: \", squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonic way\n",
    "\n",
    "Python provides a few ways to re-write the same piece of code in a more compact form: list comprehensions and with the map. Python programmers who do not use Spark typically prefer the list comprehensions to the map. But using the map is what allows you to adjust to Spark way of programming the easiest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of squares:  [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, numbers)\n",
    "#Now, squares should have [1,4,9,16,25]\n",
    "print \"List of squares: \", squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: filtering the list in Python\n",
    "\n",
    "What if you're more interested in filtering the list? Say you want to remove every element with a value equal to or greater than 4? (Okay, so the examples aren't very realistic. Whatever...) A Python neophyte might write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers under 4 only:  [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "numbers_under_4 = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        numbers_under_4.append(number)\n",
    "        # Now, numbers_under_4 contains [1,4,9]\n",
    "print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could reduce the size of the code with the filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers under 4 only:  [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "numbers_under_4 = filter(lambda x: x < 4, numbers)\n",
    "# Now, numbers_under_4 contains [1,2,3]\n",
    "print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Pandas data structures and functionality\n",
    "\n",
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e., when everything fits into memory. The basic new \"noun\" in pandas is the **data frame**. As a part of pre-exercises, you have received an iPython notebook with some Pandas case study. \n",
    "\n",
    "It's like a table, with rows and columns (e.g., as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. (Like in SQL, columns are homogeneous.)\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "  \n",
    "## Map and filter in Pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>49</td>\n",
       "      <td>103.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>113</td>\n",
       "      <td>603.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>124.04</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "0           8           75                  NaN        1        1   \n",
       "1          28           49               103.01        1        1   \n",
       "2          40            3                  NaN        1        1   \n",
       "3          39          113               603.00        3        3   \n",
       "4          12          105               124.04        2        2   \n",
       "\n",
       "   PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA  \n",
       "0          100      258      258          100  \n",
       "1          100       71       71          100  \n",
       "2          100      215      215          100  \n",
       "3          100      206      206          100  \n",
       "4          100      303      303          100  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../preexercise/data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9\n",
       "1    29\n",
       "2    41\n",
       "3    40\n",
       "4    13\n",
       "Name: State_Code2, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"State_Code2\"] = df[\"State_Code\"].apply(lambda x: x+1)\n",
    "\n",
    "df[\"State_Code2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "      <th>State_Code2</th>\n",
       "      <th>County_Code2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>113</td>\n",
       "      <td>603.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>124.04</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "      <td>9808.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>103</td>\n",
       "      <td>207.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>119</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "3          39          113               603.00        3        3   \n",
       "4          12          105               124.04        2        2   \n",
       "5          12           86              9808.00        1        1   \n",
       "7          12          103               207.00        2        2   \n",
       "8          36          119                30.00        1        1   \n",
       "\n",
       "   PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA  State_Code2 County_Code2  \n",
       "3          100      206      206          100           40           39  \n",
       "4          100      303      303          100           13           12  \n",
       "5          100      188      188          100           13           12  \n",
       "7          100      100      100          100           13           12  \n",
       "8          100      354      354          100           37           36  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['County_Code'] > 75]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: from Python to PySpark\n",
    "\n",
    "Say I want to map and filter a list at the same time. In other words, I'd like to see the square of each element in the list where said element is under 4. Once more, the Python neophyte way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        squares.append(number*number)\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before re-writing it in PySpark, re-write it using map and filter expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, filter(lambda x: x < 4, numbers))\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "#I do not need to create the Spark Context in the notebook, but you do...\n",
    "#sc = SparkContext(\"My First App\")\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "squares_rdd = numbers_rdd.filter(lambda x: x < 4).map(lambda x: x*x)\n",
    "print squares_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Spark jobs via Slurm\n",
    "\n",
    "Before starting with the exercise2.py, you need to make sure the scratch is set up.\n",
    "Look for your scratch folder:\n",
    "\n",
    "```bash\n",
    "ls -l /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "create it if necessary:\n",
    "```bash\n",
    "mkdir /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "Define an environmental variable to store its location:\n",
    "\n",
    "```bash\n",
    "export SCRATCH_PATH=\"/scratch/network/<your_username>\"\n",
    "``` \n",
    "\n",
    "The Slurm submission file for Spark job will look like:\n",
    "\n",
    "```bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -t 00:05:00\n",
    "#SBATCH --ntasks-per-node 2\n",
    "#SBATCH --cpus-per-task 3\n",
    "\n",
    "module load spark/hadoop2.6/1.4.1\n",
    "spark-start\n",
    "echo $MASTER\n",
    "\n",
    "spark-submit --total-executor-cores 2 exercise2.py\n",
    "```\n",
    "\n",
    "Monitor the progress of your Spark application:\n",
    "\n",
    "```bash\n",
    "squeue -u alexeys\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "            219838       all slurm_fo  alexeys  R       0:04      1 adroit-06\n",
    "```             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations in Python and PySpark (quick look forward)\n",
    "\n",
    "It is Pythonic to operate on lists - elementwise operations (maps), filtering, etc. In many other languages, starting with Lisp but extending to many \"functional\" programming languages, a different style is preferred:\n",
    "\n",
    "The idea is that if `f` is a function, then one thinks of the application\n",
    ">          \n",
    "    list   |---->   [ f(x) for x in list ]\n",
    "\n",
    "on lists as a function of _two_ arguments: `f` and `list`.  The idea of viewing the function `f` as a parameter is typical in functional programming languages, and can be taken as a definition of the later term.\n",
    "\n",
    "Some common idioms in this style, with Pythonic equivalents, are:\n",
    "\n",
    "- `map(f, list) === [ f(x) for x in list ]`: Apply `f` element-wise to `list`.\n",
    "- `filter(f, list) === [ x for x in list if f(x) ]`: Filter `list` using `f`.\n",
    "- `flatMap(f, list) === [ f(x) for y in list for x in y ]`: Here `f` is a function that eats elements (of the type contained in list) and spits out lists, and `flatMap` first applies f element-wise to the elements of `list` and then _flattens_ or _concatenates_ the resulting lists.  It is sometimes also called `concatMap`.\n",
    "- `reduce(f, list[, initial])`: Here `f` is a function of _two_ variables, and folds over the list applying `f` to the \"accumulator\" and the next value in the list.  That is, it performs the following recursion\n",
    "\n",
    "$$    a_{-1} = \\mathrm{initial} $$\n",
    "$$    a_i = f(a_{i-1}, \\mathrm{list}_i) $$\n",
    "\n",
    "with the with the final answer being $a_{\\mathrm{len}(\\mathrm{list})-1}$.  (If initial is omitted, just start with $a_0 = \\mathrm{list}_0$.)  For instance,\n",
    ">           \n",
    "    reduce(lambda x,y: x+y, [1,2,3,4]) = ((1+2)+3)+4 = 10\n",
    "    \n",
    "    \n",
    "### Remark:\n",
    "This is where the name \"map reduce\" comes from.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark core fundamentals\n",
    "(slides)\n",
    "\n",
    "# Anatomy of the Spark application\n",
    "\n",
    "\n",
    "# Spark transformations and actions\n",
    "\n",
    "\n",
    "# Working with key-value pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading data into RDD\n",
    "\n",
    "To start the exercise, change into loading_data folder:\n",
    "```bash\n",
    "cd loading_data\n",
    "```\n",
    "\n",
    "In the root folder you will find a set of files starting with `load`, and a few folders. \n",
    "Let us inspect the load1_unstructured.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'', u'', u'', u'', u'                        THE ADVENTURES OF SHERLOCK HOLMES', u'', u'                               Arthur Conan Doyle', u'', u'', u'']\n",
      "Input dataset has  53271  lines\n",
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "[(u'the', 22635), (u'of', 11167), (u'and', 11086), (u'to', 10707), (u'a', 10433), (u'I', 10183), (u'in', 7006), (u'that', 6911), (u'was', 6779), (u'his', 4955)]\n",
      "Elapsed time:  1.72066187859\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #By default it assumes file located on hdfs folder, \n",
    "    #but by prefixing \"file://\" it will search the local file system\n",
    "    #Can specify a folder, can pass list of folders or use wild character\n",
    "    input_rdd = sc.textFile(\"../loading_data/unstructured/\")\n",
    "\n",
    "    #inspect it, understand how it is structured (list of strings-lines)\n",
    "    print input_rdd.take(10)\n",
    "    print \"Input dataset has \", input_rdd.count(), \" lines\"\n",
    "\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "def main2(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    #Use alternative approach: load the dinitial file into a pair RDD\n",
    "    input_pair_rdd = sc.wholeTextFiles(\"../loading_data/unstructured/\")\n",
    "\n",
    "    #inspect it, understand how it is structured (list of strings-lines)\n",
    "    print input_pair_rdd.take(3)\n",
    "    print \"Input dataset has \", input_pair_rdd.count(), \" files\"\n",
    "    counts = input_pair_rdd.flatMap(lambda line: line[1].split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Try the record-per-line-input\n",
    "    main1(sys.argv)\n",
    "    #Use alternative approach: load the initial file into a pair RDD\n",
    "    #main2(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV\n",
    "\n",
    "Next, we are going to learn how to load data in structured CSV format. There is at least two ways to do that:\n",
    "\n",
    "1) Read the files line by line with textFiles() method, split on delimiter\n",
    "\n",
    "Similarly to Python, there is a data structured designed to be used when working with structured data (I mean Pandas Dataframes), it is also called the dataframe (a concept closely linked to Spark SQL). There is a way to read CSV directly into Spark dataframe \n",
    "\n",
    "2) Read the files into dataframe using spark-csv module from Databricks\n",
    "https://github.com/databricks/spark-csv\n",
    "\n",
    "You do not need to install it, I did the work for you by adding the build Jars into the appropriate /lib folder...\n",
    "\n",
    "### Load CSV\n",
    "\n",
    "### Mini-exercise on loading CSV\n",
    "\n",
    "Use what you have learned in the load2_csv.py exercise to load a set of CSV datasets:\n",
    "\n",
    "-- Actor\n",
    "\n",
    "-- Movie\n",
    "\n",
    "-- Actor playing in movie (relationships)\n",
    "\n",
    "and find movies where **Tom Hanks** played in.\n",
    "\n",
    "Save the answer in the JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "Let us consider a very simple machine learning example of logistic regression.\n",
    "Logistic regression is an iterative machine learning algorithm that seeks to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. It can be used to classify messages into spam vs non-spam, for example. Because the algorithm applies the same MapReduce operation repeatedly to the same dataset, it benefits greatly from caching the input in RAM across iterations.\n",
    "\n",
    "## Non-MLlib implementation\n",
    "\n",
    "First, let us consider the non-MLlib implementation and try to evaluate the effect of caching and partitioning on the perfromance. We're going to try to learn the rule that y(x) = 1 if x < fraction_positive, 0 otherwise\n",
    "\n",
    "Our training sample will be generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 10**3\n",
    "fraction_positive = 0.5\n",
    "\n",
    "def y(x):\n",
    "    return 1 if x < fraction_positive else 0\n",
    "\n",
    "def generate_sample():\n",
    "    sample_X = np.arange(0, 1, 1.0/N)\n",
    "    np.random.shuffle( sample_X) # In-place shuffle!\n",
    "    sample_Y = map(y, sample_X)\n",
    "    return (sample_X, sample_Y)\n",
    "\n",
    "(sample_X, sample_Y) = generate_sample()\n",
    "\n",
    "\n",
    "## By hand.  This is the example code taken from the Spark Examples on the website.\n",
    "#  This is much slower than the above code, so I'm not going to even run it (or extract predictions, or test it..)\n",
    "start = time.time()\n",
    "def logistic_by_hand(ITERATIONS,nparts):\n",
    "    points = ( sc.parallelize( zip(sample_X, sample_Y), nparts)\n",
    "                 .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "                 .cache() )\n",
    "    w = np.random.ranf(size = 2) # current separating plane\n",
    "    print \"Original random plane: %s\" % w\n",
    "    for i in xrange(ITERATIONS):\n",
    "        gradient = points.map(\n",
    "            lambda pt: (1 / (1 + np.exp(-pt.label*(w.dot(pt.features)))) - 1) * pt.label * pt.features\n",
    "        ).reduce(lambda a, b: a + b)\n",
    "        w -= gradient\n",
    "    print \"Final separating plane: %s\" % w\n",
    "\n",
    "logistic_by_hand(20,3)\n",
    "end = time.time()\n",
    "print \"Elapsed time: \", (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib based implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.896\n",
      "Elapsed time:  15.8722097874\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "## Using MLLib and it's data structures.  This is fairly quick.\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "points = ( sc.parallelize( zip(sample_X, sample_Y), 3)\n",
    "             .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "             .cache() )\n",
    "model = LogisticRegressionWithSGD.train(points)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = points.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(points.count())\n",
    "print \"Accuracy on training set: %s\" % (1 - trainErr)\n",
    "end = time.time()\n",
    "print \"Elapsed time: \", (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Play with the `fraction_positive` parameter: What happens to the accuracy measure as `fraction_positive` gets below 0.30 or above 0.70? (You should be somewhat disappointed with the results!)  What do you think is happening, and can you improve on it?\n",
    "1. Play with the \"by hand\" version (.. after lowering N to say 10**4 or so): Figure out what it's actually doing and how to use it to get results.  How much slower than the MLLib version does it seem to be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY 2\n",
    "\n",
    "## Spark SQL and DataFrames\n",
    "\n",
    "http://spark.apache.org/sql/\n",
    "\n",
    "Let us go back to a load4_json.py example from yesterday. Here, we load a small JSON file, register temporary table in memory and run a SQL-like query on it in a distributed way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|     name|               roles|      title|\n",
      "+---------+--------------------+-----------+\n",
      "|Tom Hanks|Old Salty Dog / M...|Cloud Atlas|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def main_sqlcontext(args):\n",
    "    #Note 2 Contexts: SparkContext and SQL context\n",
    "    #sc = SparkContext(appName=\"LoadJson\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    input = sqlContext.read.json(\"../loading_data/json/\")\n",
    "    input.registerTempTable(\"movies\")\n",
    "    answer = sqlContext.sql(\"SELECT * FROM movies WHERE title = 'Cloud Atlas'\")\n",
    "    answer.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_sqlcontext(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same solution, but using the Pandas dataframe-like syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|     name|               roles|      title|\n",
      "+---------+--------------------+-----------+\n",
      "|Tom Hanks|Old Salty Dog / M...|Cloud Atlas|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main_sqlcontext(args):\n",
    "    #Note 2 Contexts: SparkContext and SQL context\n",
    "    #sc = SparkContext(appName=\"LoadJson\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "\n",
    "    input = sqlContext.read.json(\"../loading_data/json/\")\n",
    "    answer = input.where(input.title ==\"Cloud Atlas\")\n",
    "    answer.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_sqlcontext(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project1: web mining and text processing \n",
    "\n",
    "The project is based on a Kaggle competition which had taken place in the past.\n",
    "\n",
    "The dataset consists of over 300,000 raw HTML files containing text, links, and downloadable images. \n",
    "\n",
    "Given the HTML of websites served to users of StumbleUpon, the challenge was to identify the paid content disguised as just another internet gem.\n",
    "\n",
    "If media companies could better identify poorly designed native ads, they can keep them off your feed and out of your user experience. \n",
    "\n",
    "https://www.kaggle.com/c/dato-native\n",
    "\n",
    "\n",
    "### Analysis workflow\n",
    "\n",
    "1) Perfrom web-scraping of data from raw HTML to JSON\n",
    "\n",
    "\n",
    "2) Extract features for classification (we will focus on text features today)\n",
    "\n",
    "\n",
    "3) Train a classifier and estimate cross-validation error\n",
    "\n",
    "\n",
    "This is where the domain knowledge you have aqcuired during pre-exercises becomes handy! **Quick look into scraping and NLP notebooks**\n",
    "\n",
    "\n",
    "#### Sub-task 1\n",
    "\n",
    "1) Run local web scraper:\n",
    "\n",
    "```bash\n",
    "python scrape_html.py\n",
    "```\n",
    "\n",
    "2) Modify to use Spark libraries sparky_scrape1_html_exercise.py.\n",
    "Then run it with Slurm or directly on the headnode like:\n",
    "\n",
    "```bash\n",
    "/usr/licensed/spark/spark-1.5.2-bin-hadoop2.6/bin/pyspark sparky_scrape1_html_exercise.py\n",
    "```\n",
    "\n",
    "Compare two files.            \n",
    "\n",
    "At least in the beginning, a lot of Spark code you write will be ported from some of the existing Python solutions that you have been using for a while.\n",
    "\n",
    "Inspect the scrape_html.py file. What it does it scrapes text, links, and images from the HTML pages using BeautifulSoup. You are asked to re-write the code in Spark and run it on a cluster.\n",
    "\n",
    "Surprisingly (or not), the amount of changes you make will be minimal.\n",
    "\n",
    "We are going to run this application on a single core (the amount of data will be small), to make sure that the lines in the JSON file go in the same order. Keeping this in mind, we can compare to files by simply running diff:\n",
    "\n",
    "```bash\n",
    "diff chunk.json /scratch/network/alexeys/BigDataCourse/web_dataset_preprocessed2/part-00000\n",
    "```\n",
    "\n",
    "#### Sub-task 2\n",
    "\n",
    "Having performed the scraping, we are all set to go to steps 2) and 3): feature engineering and classification.\n",
    "First, we need to label our data - labels are provided in a separate file, so we need to perfrom a relational JOIN on keys (HTML file ID in this case). I keep positive and negative samples in two different RDDs for the sake of convenience.\n",
    "\n",
    "**The type of learner**: We're going to choose to look at this as a _supervised classification_ problem.  There are also unsupervised approaches, but you have to make choices sometimes.  This means we need some \"marked up\" data:\n",
    "\n",
    "**The training dataset**: The result of web-scraping step, stored as a JSON file\n",
    "\n",
    "We are going to restrict ourselves on the text features and the bag-of-words approach with TF-IDF weighting applied.\n",
    "To prepare text feature, we are going to step through the regular:\n",
    "\n",
    "1) Tokenization, n-grams\n",
    "\n",
    "2) Stemming, stopword removal\n",
    "\n",
    "3) TF calculation and text vectorization\n",
    "\n",
    "4) IDF, TF-IDF calculation\n",
    "\n",
    "\n",
    "\n",
    "#### TF-IDF: term frequency–inverse document frequency\n",
    "\n",
    "With single word vocabularies, we can probably do an okay job of coming up with a reasonable (if short) list of words that distinguish between the two documents.  With n-grams, even for $n=2$, it is better to let a computer help us.  \n",
    "\n",
    "Just using frequencies, as above, is clearly not great.  Both apples the fruit and Apple the company are enjoyed around the world (one of the 2-grams that came up above!).  We would like to find words that are common in one document, not not common in all of them.  This is the goal of the __td-idf weighting__.  A precise definition is:\n",
    "\n",
    "\n",
    "  1. If $d$ denotes a document and $t$ denotes a term, then the _raw term frequency_ $\\mathrm{tf}^{raw}(t,d)$ is\n",
    "  $$ \\mathrm{tf}^{raw}(t,d) = \\text{the number of times the term $t$ occurs in the document $d$} $$\n",
    "  The vector of all term frequencies can optionally be _normalized_ either by dividing by the maximum of ny single word's occurance count ($L^1$) or by the Euclidean length of the vector of word occurance counts ($L^2$).  Scikit-learn by defaults does this second one:\n",
    "  $$ \\mathrm{tf}(t,d) = \\mathrm{tf}^{L^2}(t,d) = \\frac{\\mathrm{tf}^{raw}(t,d)}{\\sqrt{\\sum_t \\mathrm{tf}^{raw}(t,d)^2}} $$\n",
    "  2. If $$ D = \\left\\{ d : d \\in D \\right\\} $$ is the set of possible documents, then  the _inverse document frequency_ is\n",
    "  $$ \\mathrm{idf}^{naive}(t,D) = \\log \\frac{\\# D}{\\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "  = \\log \\frac{\\text{count of all documents}}{\\text{count of those documents containing the term $t$}} $$\n",
    "  with a common variant being\n",
    "  $$ \\mathrm{idf}(t, D) = \\log \\frac{\\# D}{1 + \\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "   = \\log \\frac{\\text{count of all documents}}{1 + \\text{count of those documents containing the term $t$}} $$\n",
    "  (This second one is the default in scikit-learn. Without this tweak we would omit the $1+$ in the denominator and have to worry about dividing by zero if $t$ is not found in any documents.)\n",
    "  3. Finally, the weight that we assign to the term $t$ appearing in document $d$ and depending on the corpus of all documents $D$ is\n",
    "  $$ \\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\mathrm{idf}(t,D) $$\n",
    "  \n",
    "#### Labeled Point format  \n",
    "  \n",
    "After that, we only need to convert our features to the LabeledPoint format, LabledPoint is a class that represents the features and labels of a data point - all MLlib classifier expect data in that format.\n",
    "\n",
    "\n",
    "## Different ML classifiers\n",
    "\n",
    "Finally, we will play with various ML classifiers available on the market.\n",
    "\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "A decision tree is a binary tree.  At each of the internal nodes, it chooses a feature $i$ and a threshold $t$.  Each leaf has a value.  Evaluation of the model is just traversal of the tree from the root.  At each node, for example $j$, we go down the left branch if $X_{ji} \\le t$ and the right branch otherwise.  The value of the model $f(X_{ji})$ is the value at the value at the terminating leaf of this traveral.  Below, we show a picture of this on small decision tree trained on the iris data set.  Notice that each internal node has a decision criterion and each leaf has the breakdown of label classes left at this leaf of the tree.  For a geometric picture of a decision tree, take a look at this [blog post](https://shapeofdata.wordpress.com/2013/07/02/decision-trees/).\n",
    "\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "A random forest is just an ensemble of decision trees.  The predicted value is just the average of the trees (for both regression and classification problems - for classification problems, it is the probabilities that are averaged).  You can adjust `n_estimators` to change the number of trees in the forest.  If each tree is trained on the same subset of data, why aren't they identical?  Two reasons:\n",
    "1. **Subsampling**: each tree is actually trained on a random selected (with replacement) subset (i.e. bootstrap)\n",
    "1. **Maximum Features**: the optimal split comes from a randomly selected subset of the features.  In scikit-learn, this feature is controlled by `max_features`.\n",
    "\n",
    "### Random Forest Training Algorithm and Tuning Parameters\n",
    "\n",
    "A Random Forest is pretty straightforward to train once you know how a Decision Tree works.  In fact, their construction can even be parallelized.  \n",
    "\n",
    "Below, various parameters that affect decision tree and random forest training are discussed. \n",
    "\n",
    "The first two parameters we mention are the most important, and tuning them can often improve performance:\n",
    "\n",
    "**numTrees**: Number of trees in the forest.\n",
    "\n",
    "Increasing the number of trees will decrease the variance in predictions, improving the model’s test-time accuracy.\n",
    "Training time increases roughly linearly in the number of trees.\n",
    "\n",
    "\n",
    "**maxDepth**: Maximum depth of each tree in the forest.\n",
    "Increasing the depth makes the model more expressive and powerful. However, deep trees take longer to train and are also more prone to overfitting.\n",
    "In general, it is acceptable to train deeper trees when using random forests than when using a single decision tree. One tree is more likely to overfit than a random forest (because of the variance reduction from averaging multiple trees in the forest).\n",
    "\n",
    "The next two parameters generally do not require tuning. However, they can be tuned to speed up training.\n",
    "\n",
    "**subsamplingRate**: This parameter specifies the size of the dataset used for training each tree in the forest, as a fraction of the size of the original dataset. The default (1.0) is recommended, but decreasing this fraction can speed up training.\n",
    "\n",
    "**featureSubsetStrategy**: Number of features to use as candidates for splitting at each tree node. The number is specified as a fraction or function of the total number of features. Decreasing this number will speed up training, but can sometimes impact performance if too low.\n",
    "\n",
    "### Linear SVM\n",
    "\n",
    "The canonical Support Vector Machine is the linear one.  Assume we have two groups labeled by $y = \\pm 1$.  Then we are trying to find the line $\\beta$ such that $X \\beta + \\beta_0$ maximially separates the points in our two classes:\n",
    "\n",
    "![SVM Diagram from Hastie et al's The Elements of Statistical Learning](/files/images/svm3.png)\n",
    "\n",
    "If the two classes can be separated by a linear hyperplane (picture on the left), we want to maximize the **margin** $M$ of the **boundary region**.  A little bit of math can show us that finding the largest separation is actually solved by the minimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge 1 \\quad \\mbox{for } j = 1,\\ldots,N\n",
    "$$\n",
    "\n",
    "The picture and the equation are equivalent: in the picture we are setting the margin to be $M$ and finding the largest margin possible.  In the equation, we are setting the margin to be $1$ and finding the smallest $\\beta$ that will make that true.  So $\\beta$ and $M$ are related through $\\| \\beta \\| = \\frac{1}{M}$.  If the two classes cannot be separated (picture on the right), we will have to add a forgiveness terms $\\xi$,\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j) & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\sum_j \\xi_j \\le C\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "for some constant $C$.  The constant $C$ is an important tradeoff.  It corresponds to the total \"forgiveness budget\" (see the last constraint).  The larger $C$, the forgiveness we have and the wider the margin $M$ can be.  We can rewrite the constrained optimization problem as the primal Lagrangian function with Lagrange multipliers $\\alpha_j \\ge 0$, $\\mu_j \\ge 0$, and $\\gamma \\ge 0$,  for each of our three constraints:\n",
    "\n",
    "$$ L_P(\\gamma) = \\min_{\\beta, \\beta_0, \\xi} \\max_{\\alpha, \\mu} \\frac{1}{2} \\| \\beta \\|^2 - \\sum_j \\alpha_j \\left[y_j (X_{j \\cdot} \\cdot \\beta + \\beta_0 - (1-\\xi_j)\\right] - \\sum_j \\mu_j \\xi_j  + \\gamma \\sum_j \\xi_j$$\n",
    "\n",
    "There is a one-to-one correspondence between $\\gamma$ and $C$.  By taking first order conditions, first-order conditions, the dual Lagrangian problem can be formulated as\n",
    "\n",
    "$$\n",
    "L_D(\\gamma) = \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_{j, j'} \\alpha_j \\alpha_{j'} y_j y_{j'} X_{j \\cdot} \\cdot X_{j' \\cdot} \\,. \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    "0 = \\sum_j \\alpha_j y_j \\\\\n",
    "0 \\le \\alpha_j \\le \\gamma & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "This is now a reasonably straightforward quadratic programming problem.  It is solved via [Sequential Minimization Optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization).  Once we have solved this problem for $\\alpha$, we can easily work out the coefficients from\n",
    "\n",
    "$$ \\beta = \\sum_j \\alpha_j y_j X_{j \\cdot} $$\n",
    "\n",
    "**Key takeaways**:\n",
    "1. Critically, only points inside the margin or on the wrong side of the margin ($j$ for which $\\xi_j > 0$) affect the SVM (see the picture).  This is intuitively clear from the picture.  In the dual form, this is because $\\alpha_j$ is the Lagrangian constraint corresponding to $y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j)$ and Complementary Slackness shows tells us that $\\alpha_j > 0$ is non-zero only when the constraint is binding ($y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) = (1-\\xi_j)$), i.e. we're in the boundary region.  This is meaning the **Support Vector** in \"SVM\": only the vectors in the boundary-the **Support Vectors**-contribute to the solution.\n",
    "1. $C$ or $\\gamma$ give a trade-off between the amount of forgiveness and the size of the margin or boundary region.  Hence, it controls how many points affect the SVM (based on the distance from the boundary).\n",
    "\n",
    "Below, we plot out a simple two-class linear SVM on some synthetic data\n",
    "\n",
    "\n",
    "### Non-linear SVM\n",
    "\n",
    "What if we don't believe that our data can be cleanly split by a linear hyperplane?  The common way to incorporate non-linear features is to have a non-linear function $h(X_{j\\cdot})$ (possibly to a higher-dimensional feature space with dimension $p'$ where $p' \\ge p$) and to train on that space.  One intuition is that there's a higher-dimensional space in which the data is has a linear separation and $h$ gives a non-linear mapping into that space.\n",
    "\n",
    "#### Kernel Trick\n",
    "\n",
    "The **Kernel Trick** in SVM tells us that rather than directly computing the (potentially very large) vectors $h(X_{j \\cdot})$, we can just modify the Kernel.  If we use the transformed data $h(X_{j \\cdot})$, the dual Lagrangian would be\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot}) $$\n",
    "\n",
    "We can rewrite\n",
    "\n",
    "$$h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot})  = K(X_{j \\cdot}, X_{j' \\cdot})$$ \n",
    "\n",
    "for some non-linear Kernel $K$.  Our problem then becomes,\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} K(X_{j \\cdot}, X_{j' \\cdot}) $$\n",
    "\n",
    "There's a one-to-one correspondence between Kernel functions and functions $h$ (although $h$'s range may be infinite dimensional).  Some common Kernels include\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Kernel</th>\n",
    "<th>$K(x,x')$</th>\n",
    "<th>Scikit `kernel` parameter</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Linear Kernel</td>\n",
    "<td>$x \\cdot x'$</td>\n",
    "<td>`kernel='linear'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$d$-th Degree Polynomial</td>\n",
    "<td>$(r + c x \\cdot x')^d$</td>\n",
    "<td>`kernel='poly'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Radial Kernel</td>\n",
    "<td>$ \\exp(- c \\|x - x' \\|^2) $</td>\n",
    "<td>`kernel='rbf'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Neural Network Kernel</td>\n",
    "<td>$\\tanh(c x \\cdot x' + r)$</td>\n",
    "<td>`kernel='sigmoid'`</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The benefit of using a Kernel is that we don't have to compute a very high-dimensional (possibly infinite-dimensional) $h$.  All that complexity is just wrapped into the kernel $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running bypassing the scheduler\n",
    "\n",
    "This way is not recommended if you are using Princeton comoputing resources. However, just for educational purposes we will try running bypassing the Slurm scheduler:\n",
    "\n",
    "\n",
    "```bash\n",
    "/usr/licensed/spark/spark-1.5.2-bin-hadoop2.6/bin/pyspark <script_name.py>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Spark applications with Scala API\n",
    "\n",
    "Apache Spark is written in Scala. Scala (along with Python and Java) is among three languages supported by Spark, and in fact Scala functionality is typically added the first to new Spark releases.\n",
    "\n",
    "\n",
    "## Preparing our first Scala Spark application\n",
    "\n",
    "Let us start with an PySpark application we have prepared on one of the previous steps. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "[(u'the', 22635), (u'of', 11167), (u'and', 11086), (u'to', 10707), (u'a', 10433), (u'I', 10183), (u'in', 7006), (u'that', 6911), (u'was', 6779), (u'his', 4955)]\n",
      "Elapsed time:  1.07145404816\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    input_rdd = sc.textFile(\"../loading_data/unstructured/\",10)\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main1(sys.argv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The same application can be re-written in Scala as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.SparkContext._\n",
    "\n",
    "object WordCount {\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    val conf = new SparkConf().setAppName(\"WordCount\")\n",
    "\n",
    "\n",
    "    val textFile = spark.textFile(\"../loading_data/unstructured/\",10)\n",
    "    val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                 .map(word => (word, 1))\n",
    "                 .reduceByKey(_ + _)\n",
    "    println(\"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\")\n",
    "    println(counts.takeOrdered(10).(Ordering[Int].reverse.on(x=>x._2)))\n",
    "        \n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000000.)\n",
    "    spark.stop()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Scala Spark application Q/A\n",
    "\n",
    "Q: So you've written some Spark code in Scala. How do you submit it to Spark and run it?  \n",
    "A: Use `sbt` or `maven` to package it into a Java jar, and submit it to Spark using `spark-submit`\n",
    "\n",
    "Q: What's a Java jar?  \n",
    "A: JAR (Java Archive) is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file to distribute application software or libraries on the Java platform.\n",
    "\n",
    "### Packaging with `sbt`\n",
    "\n",
    "**What is SBT?**  \n",
    "SBT is a modern build tool written in/for Scala, though it is also a general purpose build tool  \n",
    "\n",
    "**Why SBT?**\n",
    "- Good dependency management\n",
    "- Full Scala language support for creating tasks\n",
    "- Launch REPL in project context\n",
    "\n",
    "Create a root directory for your project and run:\n",
    "```bash\n",
    "mkdir -p src/{main,test}/{resources,scala}\n",
    "mkdir lib project\n",
    "```\n",
    "within it. \n",
    "\n",
    "This script will automatically create the proper `sbt` directory structure, which borrows from the Java `maven` directory structure. The script will also generate a template `build.sbt` file at the top of the directory that you should fill out with the appropriate versions and dependencies for your app.\n",
    "\n",
    "Then we can take our Scala code, and put it in the src folder (you should have it in the main folder, so just move it there):\n",
    "\n",
    "```bash\n",
    "mv WordCount.scala src/main/scala/\n",
    "```\n",
    "\n",
    "**Project Layout (Directory structure)**   \n",
    "\n",
    "`project` – project definition files  \n",
    "`project/build/` *yourproject* `.scala` – the main project definition file  \n",
    "`project/build.properties` – project, sbt and scala version definitions  \n",
    "`src/main` – your app code goes here, in a subdirectory indicating the code’s language (e.g. src/main/scala, src/main/java)  \n",
    "`src/main/resources` – static files you want added to your jar (e.g. logging config)  \n",
    "`src/test` – like src/main, but for tests  \n",
    "`lib_managed` – the jar files your project depends on. Populated by sbt update  \n",
    "`target` – the destination for generated stuff (e.g. generated thrift\n",
    "code, class files, jars)  \n",
    "\n",
    "#### `build.sbt`: Dependencies and versioning\n",
    "\n",
    "Example `simple.sbt` (located in the root directory of your project) \n",
    "\n",
    "```scala\n",
    "name := \"WordCount\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.10.4\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "    // Spark dependency\n",
    "    \"org.apache.spark\" % \"spark-core_2.10\" % \"1.4.1\" % \"provided\"\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "#### Assembly.sbt to build a fat Jar\n",
    "\n",
    "Example assembly.sbt located in the /project folder of your project:\n",
    "\n",
    "```scala\n",
    "resolvers += Resolver.url(\"artifactory\", url(\"http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases\"))(Resolver.ivyStylePatterns)\n",
    "\n",
    "addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n",
    "```\n",
    "\n",
    "### Running (submitting a `jar` to Spark)\n",
    "\n",
    "1. Run `sbt assembly` in your project's home directory. The output to console will tell you the name and location of the resulting jar (under `./target`) \n",
    "\n",
    "You should now see the Jar file generated:\n",
    "```bash\n",
    "[alexeys@bd scala_spark] ll target/scala-2.10/\n",
    "total 6968\n",
    "drwxr-xr-x. 2 alexeys cses    4096 Dec  9 10:43 classes\n",
    "-rw-r--r--. 1 alexeys cses 7129172 Dec  9 10:43 WordCount-assembly-1.0.jar\n",
    "```\n",
    "\n",
    "2. In the Slurm batch script, use spark-submit as usual to submit the Spark app, but you would need to specify the --class and the path to jar from the current folder, for instance:\n",
    "\n",
    "```bash\n",
    "spark-submit --class \"WordCount\" --total-executor-cores target/scala-2.10/WordCount-assembly-1.0.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. The MLlib implementation includes a parallelized variant of the k-means++ method called kmeans||. The implementation in MLlib has the following parameters:\n",
    "\n",
    "1) k is the number of desired clusters.\n",
    "\n",
    "2) maxIterations is the maximum number of iterations to run.\n",
    "\n",
    "3) initializationMode specifies either random initialization or initialization via k-means.\n",
    "\n",
    "4) runs is the number of times to run the k-means algorithm (k-means is not guaranteed to find a globally optimal solution, and when run multiple times on a given dataset, the algorithm returns the best clustering result).\n",
    "\n",
    "5) initializationSteps determines the number of steps in the k-means|| algorithm.\n",
    "\n",
    "6) epsilon determines the distance threshold within which we consider k-means to have converged.\n",
    "\n",
    "7) initialModel is an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed.\n",
    "\n",
    "\n",
    "## NYC taxi data\n",
    "\n",
    "We are going to be working with the NYC taxi geographic data of the following format:\n",
    "\n",
    "**vendor_id, pickup_datetime, dropoff_datetime, passenger_count, trip_distance, pickup_longitude, pickup_latitude, rate_code, store_and_fwd_flag, dropoff_longitude, dropoff_latitude, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount**\n",
    "\n",
    "\n",
    "The goal is to determine the NYC taxi activity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
