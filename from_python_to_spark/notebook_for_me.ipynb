{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From python to Spark (PySpark)\n",
    "\n",
    "PySpark is a Python API for Apache Spark\n",
    "\n",
    "Getting started with Spark for Python programmers is particularly easy if one is using a certain programming style: \n",
    "maps, filters and lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda functions in Python\n",
    "\n",
    "Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, using a construct called \"lambda\".\n",
    "\n",
    "Sometimes you need to pass a function as an argument, or you want to do a short but complex operation multiple times. You could define your function the normal way, or you could make a lambda function, a mini-function that returns the result of a single expression. The two definitions are completely identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##traditional named function\n",
    "def add(a,b): return a+b\n",
    "\n",
    "##lambda function\n",
    "add2 = lambda a,b: a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the lambda function is that it is in itself an expression, and can be used inside another statement. Here's an example using the map function, which calls a function on every element in a list, and returns a list of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squares = map(lambda a: a*a, [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: mapping the list in Python\n",
    "\n",
    "Suppose you need to perform a transformation on a list of element. For instance, to calculate a square of each element of the list. One way to write this in Python would be as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    squares.append(number*number)\n",
    "    # Now, squares should have [1,4,9,16,25]\n",
    "print \"List of squares: \", squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonic way\n",
    "\n",
    "Python provides a few ways to re-write the same piece of code in a more compact form: list comprehensions and with the map. Python programmers who do not use Spark typically prefer the list comprehensions to the map. But using the map is what allows you to adjust to Spark way of programming the easiest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, numbers)\n",
    "#Now, squares should have [1,4,9,16,25]\n",
    "print \"List of squares: \", squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: filtering the list in Python\n",
    "\n",
    "What if you're more interested in filtering the list? Say you want to remove every element with a value equal to or greater than 4? (Okay, so the examples aren't very realistic. Whatever...) A Python neophyte might write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "numbers_under_4 = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        numbers_under_4.append(number)\n",
    "        # Now, numbers_under_4 contains [1,4,9]\n",
    "print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could reduce the size of the code with the filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "numbers_under_4 = filter(lambda x: x < 4, numbers)\n",
    "# Now, numbers_under_4 contains [1,2,3]\n",
    "print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Pandas data structures and functionality\n",
    "\n",
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e., when everything fits into memory. The basic new \"noun\" in pandas is the **data frame**. As a part of pre-exercises, you have received an iPython notebook with some Pandas case study. \n",
    "\n",
    "It's like a table, with rows and columns (e.g., as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. (Like in SQL, columns are homogeneous.)\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "  \n",
    "## Map and filter in Pandas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>49</td>\n",
       "      <td>103.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>113</td>\n",
       "      <td>603.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>124.04</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "0           8           75                  NaN        1        1   \n",
       "1          28           49               103.01        1        1   \n",
       "2          40            3                  NaN        1        1   \n",
       "3          39          113               603.00        3        3   \n",
       "4          12          105               124.04        2        2   \n",
       "\n",
       "   PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA  \n",
       "0          100      258      258          100  \n",
       "1          100       71       71          100  \n",
       "2          100      215      215          100  \n",
       "3          100      206      206          100  \n",
       "4          100      303      303          100  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../preexercise/data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9\n",
       "1    29\n",
       "2    41\n",
       "3    40\n",
       "4    13\n",
       "Name: State_Code2, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"State_Code2\"] = df[\"State_Code\"].apply(lambda x: x+1)\n",
    "\n",
    "df[\"State_Code2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "      <th>State_Code2</th>\n",
       "      <th>County_Code2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>113</td>\n",
       "      <td>603.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>105</td>\n",
       "      <td>124.04</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "      <td>9808.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>103</td>\n",
       "      <td>207.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>119</td>\n",
       "      <td>30.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "      <td>100</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "3          39          113               603.00        3        3   \n",
       "4          12          105               124.04        2        2   \n",
       "5          12           86              9808.00        1        1   \n",
       "7          12          103               207.00        2        2   \n",
       "8          36          119                30.00        1        1   \n",
       "\n",
       "   PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA  State_Code2 County_Code2  \n",
       "3          100      206      206          100           40           39  \n",
       "4          100      303      303          100           13           12  \n",
       "5          100      188      188          100           13           12  \n",
       "7          100      100      100          100           13           12  \n",
       "8          100      354      354          100           37           36  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['County_Code'] > 75]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: from Python to PySpark\n",
    "\n",
    "Say I want to map and filter a list at the same time. In other words, I'd like to see the square of each element in the list where said element is under 4. Once more, the Python neophyte way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        squares.append(number*number)\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before re-writing it in PySpark, re-write it using map and filter expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, filter(lambda x: x < 4, numbers))\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I do not need to create the Spark Context in the notebook, but you do...\n",
    "#sc = SparkContext(\"My First App\")\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "squares_rdd = numbers_rdd.filter(lambda x: x < 4).map(lambda x: x*x)\n",
    "print squares_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations in Python and PySpark (quick look forward)\n",
    "\n",
    "It is Pythonic to operate on lists - elementwise operations (maps), filtering, etc. In many other languages, starting with Lisp but extending to many \"functional\" programming languages, a different style is preferred:\n",
    "\n",
    "The idea is that if `f` is a function, then one thinks of the application\n",
    ">          \n",
    "    list   |---->   [ f(x) for x in list ]\n",
    "\n",
    "on lists as a function of _two_ arguments: `f` and `list`.  The idea of viewing the function `f` as a parameter is typical in functional programming languages, and can be taken as a definition of the later term.\n",
    "\n",
    "Some common idioms in this style, with Pythonic equivalents, are:\n",
    "\n",
    "- `map(f, list) === [ f(x) for x in list ]`: Apply `f` element-wise to `list`.\n",
    "- `filter(f, list) === [ x for x in list if f(x) ]`: Filter `list` using `f`.\n",
    "- `flatMap(f, list) === [ f(x) for y in list for x in y ]`: Here `f` is a function that eats elements (of the type contained in list) and spits out lists, and `flatMap` first applies f element-wise to the elements of `list` and then _flattens_ or _concatenates_ the resulting lists.  It is sometimes also called `concatMap`.\n",
    "- `reduce(f, list[, initial])`: Here `f` is a function of _two_ variables, and folds over the list applying `f` to the \"accumulator\" and the next value in the list.  That is, it performs the following recursion\n",
    "\n",
    "$$    a_{-1} = \\mathrm{initial} $$\n",
    "$$    a_i = f(a_{i-1}, \\mathrm{list}_i) $$\n",
    "\n",
    "with the with the final answer being $a_{\\mathrm{len}(\\mathrm{list})-1}$.  (If initial is omitted, just start with $a_0 = \\mathrm{list}_0$.)  For instance,\n",
    ">           \n",
    "    reduce(lambda x,y: x+y, [1,2,3,4]) = ((1+2)+3)+4 = 10\n",
    "    \n",
    "    \n",
    "### Remark:\n",
    "This is where the name \"map reduce\" comes from.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading data into RDD\n",
    "\n",
    "example with unstructured text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with key-value pairs\n",
    "\n",
    "## Example 3: Joins\n",
    "\n",
    "The [Hadoop and Java_Mapreduce](Hadoop_and_Java_Mapreduce.ipynb) discuss how to implement a `join` in MR using \"sum types\" -- this is a data type that can consist of multiple \"types\" of data.  For instance, the output of an intermediate map step might have rows like\n",
    ">          \n",
    "    (\"Business\", (17, \"Joe's Barbershop\"))\n",
    "    (\"Business\", (23, \"Dave's Bistro\"))\n",
    "    (\"Person\", (17, \"Joe Schmoe\"))\n",
    "    (\"Person\", (23, \"David K. Noname\"))\n",
    "\n",
    "Fortunately for users of Spark, Spark includes a built-in `join` implementation.  From the documentation:\n",
    "\n",
    ">         \n",
    "join(otherDataset, [numTasks]):\t When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are also supported through leftOuterJoin and rightOuterJoin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine learning\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "N = 10**6\n",
    "fraction_positive = 0.5\n",
    "\n",
    "def y(x):\n",
    "    return 1 if x < fraction_positive else 0\n",
    "\n",
    "\"\"\"\n",
    "Generate Sample Data:\n",
    "\n",
    "We're going to try to learn the rule that\n",
    "y(x) = 1 if x<fraction_positive, 0 otherwise\n",
    "\n",
    "Our training X's will go evenly from 0 to 1\n",
    "\"\"\"\n",
    "def generate_sample():\n",
    "    sample_X = np.arange(0, 1, 1.0/N)\n",
    "    np.random.shuffle( sample_X) # In-place shuffle!\n",
    "    sample_Y = map(y, sample_X)\n",
    "    return (sample_X, sample_Y)\n",
    "\n",
    "(sample_X, sample_Y) = generate_sample()\n",
    "\n",
    "\n",
    "## Using MLLib and it's data structures.  This is fairly quick.\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "points = ( sc.parallelize( zip(sample_X, sample_Y), 100)\n",
    "             .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "             .cache() )\n",
    "model = LogisticRegressionWithSGD.train(points)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = points.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(points.count())\n",
    "print \"Accuracy on training set: %s\" % (1 - trainErr)\n",
    "\n",
    "\n",
    "## By hand.  This is the example code taken from the Spark Examples on the website.\n",
    "#  This is much slower than the above code, so I'm not going to even run it (or extract predictions, or test it..)\n",
    "\n",
    "def logistic_by_hand():\n",
    "    ITERATIONS=20\n",
    "    points = ( sc.parallelize( zip(sample_X, sample_Y), 20)\n",
    "                 .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "                 .cache() )\n",
    "    w = np.random.ranf(size = 2) # current separating plane\n",
    "    print \"Original random plane: %s\" % w\n",
    "    for i in xrange(ITERATIONS):\n",
    "        gradient = points.map(\n",
    "            lambda pt: (1 / (1 + np.exp(-pt.label*(w.dot(pt.features)))) - 1) * pt.label * pt.features\n",
    "        ).reduce(lambda a, b: a + b)\n",
    "        w -= gradient\n",
    "    print \"Final separating plane: %s\" % w\n",
    "\n",
    "# logistic_by_hand()\n",
    "\n",
    "\n",
    "### Exercises\n",
    "1. Play with the `fraction_positive` parameter: What happens to the accuracy measure as `fraction_positive` gets below 0.30 or above 0.70? (You should be somewhat disappointed with the results!)  What do you think is happening, and can you improve on it?\n",
    "1. Play with the \"by hand\" version (.. after lowering N to say 10**4 or so): Figure out what it's actually doing and how to use it to get results.  How much slower than the MLLib version does it seem to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Spark applications with Scala API\n",
    "\n",
    "Apache Spark is written in Scala. Scala (along with Python and Java) is among three languages supported by Spark, and in fact Scala functionality is typically added the first to new Spark releases.\n",
    "\n",
    "\n",
    "## Preparing our first Scala Spark application\n",
    "\n",
    "Let us start with an PySpark application we have prepared on one of the previous steps. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
      "[(u'the', 22635), (u'of', 11167), (u'and', 11086), (u'to', 10707), (u'a', 10433), (u'I', 10183), (u'in', 7006), (u'that', 6911), (u'was', 6779), (u'his', 4955)]\n",
      "Elapsed time:  1.07145404816\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    input_rdd = sc.textFile(\"../loading_data/unstructured/\",10)\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main1(sys.argv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The same application can be re-written in Scala as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.SparkContext._\n",
    "\n",
    "object WordCount {\n",
    "  def main(args: Array[String]) {\n",
    "\n",
    "    val conf = new SparkConf().setAppName(\"WordCount\")\n",
    "\n",
    "\n",
    "    val textFile = spark.textFile(\"../loading_data/unstructured/\",10)\n",
    "    val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                 .map(word => (word, 1))\n",
    "                 .reduceByKey(_ + _)\n",
    "    println(\"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\")\n",
    "    println(counts.takeOrdered(10).(Ordering[Int].reverse.on(x=>x._2)))\n",
    "        \n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000000.)\n",
    "    spark.stop()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Scala Spark application Q/A\n",
    "\n",
    "Q: So you've written some Spark code in Scala. How do you submit it to Spark and run it?  \n",
    "A: Use `sbt` or `maven` to package it into a Java jar, and submit it to Spark using `spark-submit`\n",
    "\n",
    "Q: What's a Java jar?  \n",
    "A: JAR (Java Archive) is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file to distribute application software or libraries on the Java platform.\n",
    "\n",
    "### Packaging with `sbt`\n",
    "\n",
    "**What is SBT?**  \n",
    "SBT is a modern build tool written in/for Scala, though it is also a general purpose build tool  \n",
    "\n",
    "**Why SBT?**\n",
    "- Good dependency management\n",
    "- Full Scala language support for creating tasks\n",
    "- Launch REPL in project context\n",
    "\n",
    "Create a root directory for your project and run:\n",
    "```bash\n",
    "mkdir -p src/{main,test}/{resources,scala}\n",
    "mkdir lib project\n",
    "```\n",
    "within it. \n",
    "\n",
    "This script will automatically create the proper `sbt` directory structure, which borrows from the Java `maven` directory structure. The script will also generate a template `build.sbt` file at the top of the directory that you should fill out with the appropriate versions and dependencies for your app.\n",
    "\n",
    "Then we can take our Scala code, and put it in the src folder (you should have it in the main folder, so just move it there):\n",
    "\n",
    "```bash\n",
    "mv WordCount.scala src/main/scala/\n",
    "```\n",
    "\n",
    "**Project Layout (Directory structure)**   \n",
    "\n",
    "`project` – project definition files  \n",
    "`project/build/` *yourproject* `.scala` – the main project definition file  \n",
    "`project/build.properties` – project, sbt and scala version definitions  \n",
    "`src/main` – your app code goes here, in a subdirectory indicating the code’s language (e.g. src/main/scala, src/main/java)  \n",
    "`src/main/resources` – static files you want added to your jar (e.g. logging config)  \n",
    "`src/test` – like src/main, but for tests  \n",
    "`lib_managed` – the jar files your project depends on. Populated by sbt update  \n",
    "`target` – the destination for generated stuff (e.g. generated thrift\n",
    "code, class files, jars)  \n",
    "\n",
    "#### `build.sbt`: Dependencies and versioning\n",
    "\n",
    "Example `simple.sbt` (located in the root directory of your project) \n",
    "\n",
    "```scala\n",
    "name := \"WordCount\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.10.4\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "    // Spark dependency\n",
    "    \"org.apache.spark\" % \"spark-core_2.10\" % \"1.4.1\" % \"provided\"\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "#### Assembly.sbt to build a fat Jar\n",
    "\n",
    "Example assembly.sbt located in the /project folder of your project:\n",
    "\n",
    "```scala\n",
    "resolvers += Resolver.url(\"artifactory\", url(\"http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases\"))(Resolver.ivyStylePatterns)\n",
    "\n",
    "addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n",
    "```\n",
    "\n",
    "### Running (submitting a `jar` to Spark)\n",
    "\n",
    "1. Run `sbt assembly` in your project's home directory. The output to console will tell you the name and location of the resulting jar (under `./target`) \n",
    "\n",
    "You should now see the Jar file generated:\n",
    "```bash\n",
    "[alexeys@bd scala_spark2] ll target/scala-2.10/\n",
    "total 6968\n",
    "drwxr-xr-x. 2 alexeys cses    4096 Dec  9 10:43 classes\n",
    "-rw-r--r--. 1 alexeys cses 7129172 Dec  9 10:43 WordCount-assembly-1.0.jar\n",
    "```\n",
    "\n",
    "2. In the Slurm batch script, use spark-submit as usual to submit the Spark app, but you would need to specify the --class and the path to jar from the current folder, for instance:\n",
    "\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --class \"WordCount\" \\\n",
    "  --master local[*] \\\n",
    "  target/scala-2.10/WordCount-assembly-1.0.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
