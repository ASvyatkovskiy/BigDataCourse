{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Python to Spark (PySpark)\n",
    "\n",
    "PySpark is a Python API for Apache Spark\n",
    "\n",
    "Getting started with Spark for Python programmers is particularly easy if one is using a certain programming style: \n",
    "maps, filters and lambda functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda functions in Python\n",
    "\n",
    "Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, using a construct called \"lambda\".\n",
    "\n",
    "Sometimes you need to pass a function as an argument, or you want to do a short but complex operation multiple times. You could define your function the normal way, or you could make a lambda function, a mini-function that returns the result of a single expression. The two definitions are completely identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##traditional named function\n",
    "def add(a,b): return a+b\n",
    "\n",
    "##lambda function\n",
    "add2 = lambda a,b: a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the lambda function is that it is in itself an expression, and can be used inside another statement. Here's an example using the map function, which calls a function on every element in a list, and returns a list of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squares = map(lambda a: a*a, [1,2,3,4,5])\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: mapping the list in Python\n",
    "\n",
    "Suppose you need to perform a transformation on a list of element. For instance, to calculate a square of each element of the list. One way to write this in Python would be as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def simple_squares():\n",
    "    numbers = [1,2,3,4,5]\n",
    "    squares = []\n",
    "    for number in numbers:\n",
    "        squares.append(number*number)\n",
    "        # Now, squares should have [1,4,9,16,25]\n",
    "    print \"List of squares: \", squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exercise0: mapping the list\n",
    "simple_squares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonic way\n",
    "\n",
    "Python provides a few ways to re-write the same piece of code in a more compact form: list comprehensions and with the map. Python programmers who do not use Spark typically prefer the list comprehensions to the map. But using the map is what allows you to adjust to Spark way of programming the easiest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "def python_squares():\n",
    "    ## Pythonic way\n",
    "    numbers = [1,2,3,4,5]\n",
    "    squares = map(square, numbers)\n",
    "    #Now, squares should have [1,4,9,16,25]\n",
    "    print \"List of squares calculated in a Pythonic way: \", squares\n",
    "\n",
    "def python_squares_lambda():\n",
    "    ## Pythonic way\n",
    "    numbers = [1,2,3,4,5]\n",
    "    squares = map(lambda x: x*x, numbers)\n",
    "    #Now, squares should have [1,4,9,16,25]\n",
    "    print \"List of squares calculated in a Pythonic way with lambda: \", squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping the list in a pythonic way\n",
    "python_squares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lambda function \n",
    "python_squares_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: filtering the list in Python\n",
    "\n",
    "What if you're more interested in filtering the list? Say you want to remove every element with a value equal to or greater than 4? (Okay, so the examples aren't very realistic. Whatever...) A Python neophyte might write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_squares():\n",
    "    numbers = [1,2,3,4,5]\n",
    "    numbers_under_4 = []\n",
    "    for number in numbers:\n",
    "        if number < 4:\n",
    "            numbers_under_4.append(number)\n",
    "            # Now, numbers_under_4 contains [1,4,9]\n",
    "    print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could reduce the size of the code with the filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def python_filter_squares_lambda():\n",
    "    numbers = [1,2,3,4,5]\n",
    "    numbers_under_4 = filter(lambda x: x < 4,numbers)\n",
    "    print \"Numbers under 4 only: \",numbers_under_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exercise1: filtering the list\n",
    "filter_squares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filtering the list in a pythonic way with lambda\n",
    "python_filter_squares_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Pandas data structures and functionality\n",
    "\n",
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e., when everything fits into memory. The basic new \"noun\" in pandas is the **data frame**. As a part of pre-exercises, you have received an iPython notebook with some Pandas case study. \n",
    "\n",
    "It's like a table, with rows and columns (e.g., as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. (Like in SQL, columns are homogeneous.)\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "  \n",
    "## Map and filter in Pandas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../preexercise/data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"State_Code2\"] = df[\"State_Code\"].apply(lambda x: x+1)\n",
    "\n",
    "df[\"State_Code2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['County_Code'] > 75]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: from Python to PySpark\n",
    "\n",
    "Say I want to map and filter a list at the same time. In other words, I'd like to see the square of each element in the list where said element is under 4. Once more, the Python neophyte way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        squares.append(number*number)\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before re-writing it in PySpark, re-write it using map and filter expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, filter(lambda x: x < 4, numbers))\n",
    "print squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We do not need to create the Spark Context in the notebook, but we would do in a standalone application...\n",
    "#sc = SparkContext(\"My First App\")\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "squares_rdd = numbers_rdd.filter(lambda x: x < 4).map(lambda x: x*x)\n",
    "print squares_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Spark jobs via Slurm\n",
    "\n",
    "Change into the working directory\n",
    "```bash\n",
    "cd BigDataCourse/1_TransformationsActions\n",
    "```\n",
    "\n",
    "Before starting with the exercise.py, you need to make sure the scratch is set up.\n",
    "Look for your scratch folder:\n",
    "\n",
    "```bash\n",
    "ls -l /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "create it if necessary:\n",
    "```bash\n",
    "mkdir /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "Define an environmental variable to store its location:\n",
    "\n",
    "```bash\n",
    "export SCRATCH_PATH=\"/scratch/network/<your_username>\"\n",
    "``` \n",
    "\n",
    "The Slurm submission file for Spark job will look like:\n",
    "\n",
    "```bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -t 00:05:00\n",
    "#SBATCH --ntasks-per-node 2\n",
    "#SBATCH --cpus-per-task 3\n",
    "\n",
    "module load spark/hadoop2.6/1.6.1\n",
    "spark-start\n",
    "echo $MASTER\n",
    "\n",
    "spark-submit --total-executor-cores 6 exercise.py\n",
    "```\n",
    "\n",
    "Monitor the progress of your Spark application:\n",
    "\n",
    "```bash\n",
    "squeue -u alexeys\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "            219838       all slurm_fo  alexeys  R       0:04      1 adroit-06\n",
    "```             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations in Python and PySpark (quick look forward)\n",
    "\n",
    "It is Pythonic to operate on lists - elementwise operations (maps), filtering, etc. In many other languages, starting with Lisp but extending to many \"functional\" programming languages, a different style is preferred:\n",
    "\n",
    "The idea is that if `f` is a function, then one thinks of the application\n",
    ">          \n",
    "    list   |---->   [ f(x) for x in list ]\n",
    "\n",
    "on lists as a function of _two_ arguments: `f` and `list`.  The idea of viewing the function `f` as a parameter is typical in functional programming languages, and can be taken as a definition of the later term.\n",
    "\n",
    "Some common idioms in this style, with Pythonic equivalents, are:\n",
    "\n",
    "- `map(f, list) === [ f(x) for x in list ]`: Apply `f` element-wise to `list`.\n",
    "- `filter(f, list) === [ x for x in list if f(x) ]`: Filter `list` using `f`.\n",
    "- `flatMap(f, list) === [ f(x) for y in list for x in y ]`: Here `f` is a function that eats elements (of the type contained in list) and spits out lists, and `flatMap` first applies f element-wise to the elements of `list` and then _flattens_ or _concatenates_ the resulting lists.  It is sometimes also called `concatMap`.\n",
    "- `reduce(f, list[, initial])`: Here `f` is a function of _two_ variables, and folds over the list applying `f` to the \"accumulator\" and the next value in the list.  That is, it performs the following recursion\n",
    "\n",
    "$$    a_{-1} = \\mathrm{initial} $$\n",
    "$$    a_i = f(a_{i-1}, \\mathrm{list}_i) $$\n",
    "\n",
    "with the with the final answer being $a_{\\mathrm{len}(\\mathrm{list})-1}$.  (If initial is omitted, just start with $a_0 = \\mathrm{list}_0$.)  For instance,\n",
    ">           \n",
    "    reduce(lambda x,y: x+y, [1,2,3,4]) = ((1+2)+3)+4 = 10\n",
    "    \n",
    "    \n",
    "### Remark:\n",
    "This is where the name \"map reduce\" comes from.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Anatomy of the Spark application\n",
    "\n",
    "\n",
    "# Spark transformations and actions\n",
    "\n",
    "\n",
    "# Working with key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
