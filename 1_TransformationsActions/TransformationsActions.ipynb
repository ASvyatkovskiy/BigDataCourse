{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Python to Spark (PySpark)\n",
    "\n",
    "PySpark is a Python API for Apache Spark\n",
    "\n",
    "Getting started with Spark for Python programmers is particularly easy if one is using functional programming style: \n",
    "`map`s, `filter`s and `lambda` functions (as opposed to `for`- and `while`-loops).\n",
    "\n",
    "\n",
    "# Functional primitives in Python and PySpark\n",
    "\n",
    "In many programming languages, elementwise transformations are applied to a list or an array using the control flow operators like `for` and `while`. In functional programming languages like Scala and concequently Spark a set of primitives like `map`, `filter` and `reduce` are used instead. Some of these are availablle in Python too.\n",
    "\n",
    "Some common functional primitives with Pythonic list-comprehension equivalents, are:\n",
    "\n",
    "- `map(f, list) === [ f(x) for x in list ]`: Apply `f` element-wise to `list`.\n",
    "- `filter(f, list) === [ x for x in list if f(x) ]`: Filter `list` using `f`.\n",
    "- `flatMap(f, list) === [ f(x) for y in list for x in y ]`: Here `f` is a function that eats elements (of the type contained in list) and spits out lists, and `flatMap` first applies f element-wise to the elements of `list` and then _flattens_ or _concatenates_ the resulting lists.  It is sometimes also called `concatMap`.\n",
    "- `reduce(f, list[, initial])`: Here `f` is a function of _two_ variables, and folds over the list applying `f` to the \"accumulator\" and the next value in the list.  That is, it performs the following recursion\n",
    "\n",
    "$$    a_{-1} = \\mathrm{initial} $$\n",
    "$$    a_i = f(a_{i-1}, \\mathrm{list}_i) $$\n",
    "\n",
    "with the with the final answer being $a_{\\mathrm{len}(\\mathrm{list})-1}$.  (If initial is omitted, just start with $a_0 = \\mathrm{list}_0$.)  For instance,\n",
    ">           \n",
    "    reduce(lambda x,y: x+y, [1,2,3,4]) = ((1+2)+3)+4 = 10\n",
    "    \n",
    "    \n",
    "## Remark:\n",
    "This is where the name \"map reduce\" comes from.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda functions in Python\n",
    "\n",
    "Python supports the creation of anonymous functions (i.e. functions that are not bound to a name) at runtime, using a construct called \"lambda\".\n",
    "\n",
    "Sometimes you need to pass a function as an argument, or you want to do a short but complex operation multiple times. You could define your function the normal way, or you could make a lambda function, a mini-function that returns the result of a single expression. The two definitions are completely identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##traditional named function\n",
    "def add(a,b): return a+b\n",
    "\n",
    "##lambda function\n",
    "add2 = lambda a,b: a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the `lambda` function is that it is in itself an expression, and can be used inside another statement. Here's an example using the `map` function, which calls a function on every element in a list, and returns a list of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 9 16 25\n"
     ]
    }
   ],
   "source": [
    "squares = map(lambda a: a*a, [1,2,3,4,5])\n",
    "print (*squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: mapping the list in Python\n",
    "\n",
    "Suppose you need to perform a transformation on a list of element. For instance, to calculate a square of each element of the list. One way to write this in Python would be as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def simple_squares():\n",
    "    numbers = [1,2,3,4,5]\n",
    "    squares = []\n",
    "    for number in numbers:\n",
    "        squares.append(number*number)\n",
    "        # Now, squares should have [1,4,9,16,25]\n",
    "    print (\"List of squares: {}\".format(squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of squares: [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "#Exercise0: mapping the list\n",
    "simple_squares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional way\n",
    "\n",
    "Python provides a few ways to re-write the same piece of code in a more compact form: list comprehensions and with the map. Python programmers who do not use Spark typically prefer the list comprehensions to the map. But using the map is what allows you to adjust to Spark way of programming the easiest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "def python_squares():\n",
    "    ## Pythonic way\n",
    "    numbers = [1,2,3,4,5]\n",
    "    squares = map(square, numbers)\n",
    "    #Now, squares should have [1,4,9,16,25]\n",
    "    print (\"List of squares calculated in a Functional way: \", *squares)\n",
    "\n",
    "def python_squares_lambda():\n",
    "    ## Pythonic way\n",
    "    numbers = [1,2,3,4,5]\n",
    "    squares = map(lambda x: x*x, numbers)\n",
    "    #Now, squares should have [1,4,9,16,25]\n",
    "    print (\"List of squares calculated in a Functional way with lambda: \", *squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of squares calculated in a Functional way:  1 4 9 16 25\n"
     ]
    }
   ],
   "source": [
    "# Mapping the list in a pythonic way\n",
    "python_squares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of squares calculated in a Functional way with lambda:  1 4 9 16 25\n"
     ]
    }
   ],
   "source": [
    "#Lambda function \n",
    "python_squares_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: filtering the list in Python\n",
    "\n",
    "What if you're more interested in filtering the list? Say you want to remove every element with a value equal to or greater than 4? (Okay, so the examples aren't very realistic. Whatever...) A Python neophyte might write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_squares():\n",
    "    numbers = [1,2,3,4,5]\n",
    "    numbers_under_4 = []\n",
    "    for number in numbers:\n",
    "        if number < 4:\n",
    "            numbers_under_4.append(number)\n",
    "            # Now, numbers_under_4 contains [1,4,9]\n",
    "    print (\"Numbers under 4 only: \",numbers_under_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could reduce the size of the code with the filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_filter_squares_lambda():\n",
    "    numbers = [1,2,3,4,5]\n",
    "    numbers_under_4 = filter(lambda x: x < 4,numbers)\n",
    "    print (\"Numbers under 4 only: \",*numbers_under_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers under 4 only:  [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#Exercise1: filtering the list\n",
    "filter_squares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers under 4 only:  1 2 3\n"
     ]
    }
   ],
   "source": [
    "#filtering the list in a pythonic way with lambda\n",
    "python_filter_squares_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas data structures and functionality\n",
    "\n",
    "Below, we're going to explore a dataset of mortgage insurance issued by the *Federal Housing Authority (FHA)*. The data is broken down by census tract and tells us how big of a player the FHA is in each tract (how many homes etc ...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "      <th>GEOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>9613.0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>2184</td>\n",
       "      <td>799</td>\n",
       "      <td>36.58420</td>\n",
       "      <td>1.049961e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55215</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5000</td>\n",
       "      <td>774</td>\n",
       "      <td>76</td>\n",
       "      <td>9.81912</td>\n",
       "      <td>1.003010e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65492</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1495</td>\n",
       "      <td>263</td>\n",
       "      <td>17.59200</td>\n",
       "      <td>1.095031e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33750</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9618.0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>21.4286</td>\n",
       "      <td>1243</td>\n",
       "      <td>333</td>\n",
       "      <td>26.79000</td>\n",
       "      <td>1.039962e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "23999         1.0         49.0               9613.0       16        4   \n",
       "55215         1.0          3.0                102.0        8        1   \n",
       "65492         1.0         27.0                  NaN        1        0   \n",
       "45193         1.0         95.0                311.0       20        3   \n",
       "33750         1.0         39.0               9618.0       14        3   \n",
       "\n",
       "       PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA         GEOID  \n",
       "23999      25.0000     2184      799     36.58420  1.049961e+09  \n",
       "55215      12.5000      774       76      9.81912  1.003010e+09  \n",
       "65492       0.0000       82        0      0.00000           NaN  \n",
       "45193      15.0000     1495      263     17.59200  1.095031e+09  \n",
       "33750      21.4286     1243      333     26.79000  1.039962e+09  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../0_Preexercise/data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df['GEOID'] = df['Census_Tract_Number']*100 + 10**6 * df['County_Code'] \\\n",
    "    + 10**9 * df['State_Code']   \n",
    "    \n",
    "df = df.sort_values('State_Code')  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and filter in Pandas\n",
    "\n",
    "\n",
    "Pandas supports functional transformations (`map`, `filter`) too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23999    2.0\n",
       "55215    2.0\n",
       "65492    2.0\n",
       "45193    2.0\n",
       "33750    2.0\n",
       "Name: State_Code2, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"State_Code2\"] = df[\"State_Code\"].apply(lambda x: x+1)\n",
    "\n",
    "df[\"State_Code2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>State_Code2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>311.00</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1495</td>\n",
       "      <td>263</td>\n",
       "      <td>17.5920</td>\n",
       "      <td>1.095031e+09</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23024</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>5.01</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>33.3333</td>\n",
       "      <td>615</td>\n",
       "      <td>232</td>\n",
       "      <td>37.7236</td>\n",
       "      <td>1.089001e+09</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65507</th>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>756.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.099076e+09</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39229</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>54.04</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>23.3333</td>\n",
       "      <td>9263</td>\n",
       "      <td>2051</td>\n",
       "      <td>22.1419</td>\n",
       "      <td>1.103005e+09</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65472</th>\n",
       "      <td>1.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>435</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "45193         1.0         95.0               311.00       20        3   \n",
       "23024         1.0         89.0                 5.01        9        3   \n",
       "65507         1.0         99.0               756.00        1        0   \n",
       "39229         1.0        103.0                54.04       60       14   \n",
       "65472         1.0        113.0                  NaN        2        0   \n",
       "\n",
       "       PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA         GEOID  State_Code2  \n",
       "45193      15.0000     1495      263      17.5920  1.095031e+09          2.0  \n",
       "23024      33.3333      615      232      37.7236  1.089001e+09          2.0  \n",
       "65507       0.0000      116        0       0.0000  1.099076e+09          2.0  \n",
       "39229      23.3333     9263     2051      22.1419  1.103005e+09          2.0  \n",
       "65472       0.0000      435        0       0.0000           NaN          2.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['County_Code'] > 75]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: from Python to PySpark\n",
    "\n",
    "Say I want to map and filter a list at the same time. In other words, I'd like to see the square of each element in the list where said element is under 4. Once more, the Python neophyte way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = []\n",
    "for number in numbers:\n",
    "    if number < 4:\n",
    "        squares.append(number*number)\n",
    "print (squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before re-writing it in PySpark, re-write it using map and filter expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 9\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "squares = map(lambda x: x*x, filter(lambda x: x < 4, numbers))\n",
    "print (*squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "try:\n",
    "    sc\n",
    "except NameError:    \n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "#We do not need to create the SparkSession/SparkContext in the notebook, but we would do in a standalone application...\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "squares_rdd = numbers_rdd.filter(lambda x: x < 4).map(lambda x: x*x)\n",
    "print (squares_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Spark jobs via Slurm (this part is done on Adroit cluster)\n",
    "\n",
    "Open a new terminal window, or use an ssh client to login to Adroit cluster:\n",
    "\n",
    "```bash\n",
    "ssh -XC your_username@adroit3.princeton.edu\n",
    "```\n",
    "\n",
    "Checkout the course exercise on Adroit as well:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ASvyatkovskiy/BigDataCourse && cd BigDataCourse\n",
    "```\n",
    "\n",
    "Change into the working directory\n",
    "```bash\n",
    "cd BigDataCourse/1_TransformationsActions\n",
    "```\n",
    "\n",
    "Before starting with the exercise.py, you need to make sure the scratch is set up.\n",
    "Look for your scratch folder:\n",
    "\n",
    "```bash\n",
    "ls -l /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "create it if necessary:\n",
    "```bash\n",
    "mkdir /scratch/network/<your_username>\n",
    "```\n",
    "\n",
    "Define an environmental variable to store its location:\n",
    "\n",
    "```bash\n",
    "export SCRATCH_PATH=\"/scratch/network/<your_username>\"\n",
    "``` \n",
    "\n",
    "The Slurm submission file for Spark job will look like:\n",
    "\n",
    "```bash\n",
    "#SBATCH -N 1\n",
    "#SBATCH -t 00:05:00\n",
    "#SBATCH --ntasks-per-node 2\n",
    "#SBATCH --cpus-per-task 3\n",
    "\n",
    "module load spark/hadoop2.7/2.2.0\n",
    "spark-start\n",
    "echo $MASTER\n",
    "\n",
    "spark-submit --total-executor-cores 6 exercise.py\n",
    "```\n",
    "\n",
    "Monitor the progress of your Spark application:\n",
    "\n",
    "```bash\n",
    "squeue -u alexeys\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "            219838       all slurm_fo  alexeys  R       0:04      1 adroit-06\n",
    "```       \n",
    "\n",
    "Read the comments in the exercise.py file inline to understand the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
