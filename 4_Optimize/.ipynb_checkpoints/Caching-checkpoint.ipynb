{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "This notebook is a quick guide to explain different memory and disk persistence options in Spark.\n",
    "Caching RDDs\n",
    "To simply cache an RDD in memory in deserialized form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "rdd = sc.parallelize([x for x in range(100000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time square  0.122296094894\n",
      "Elapsed time sqrt  0.127455949783\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "start = time.time()\n",
    "rdd1 = rdd.map(lambda x: x*x)\n",
    "rdd1.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time square \", end-start\n",
    "start = time.time()\n",
    "rdd2 = rdd.map(lambda x: sqrt(x))\n",
    "rdd2.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time sqrt \", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time square  0.128567934036\n",
      "Elapsed time sqrt  0.0969128608704\n"
     ]
    }
   ],
   "source": [
    "rdd.cache()\n",
    "\n",
    "start = time.time()\n",
    "rdd1 = rdd.map(lambda x: x*x)\n",
    "rdd1.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time square \", end-start\n",
    "start = time.time()\n",
    "rdd2 = rdd.map(lambda x: sqrt(x))\n",
    "rdd2.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time sqrt \", end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs can also be persisted in memory backed by disk in serialized form with replication across Workers. You can read more on RDD persistence here. \n",
    "\n",
    "## The primary factors to consider while choosing a storage level\n",
    "\n",
    "### Persistence Level: \n",
    "\n",
    "Storing in memory is the easiest option without too much overhead. If the entire RDD doesn't fit in memory, then in case of missing partitions, the partitions will be recomputed from the lineage automatically. But if an RDD which is formed from a wide transformation is going to be used heavily in an iterative or interactive fashion, then it is better to store it in memory backed by disk to ensure that the partitions are not recomputed.\n",
    "\n",
    "### Serialization: \n",
    "\n",
    "The default serialization in Spark is Java serialization. However for better peformance, we recommend Kryo serialization, which you can learn more about here\n",
    "\n",
    "### Replication: \n",
    "\n",
    "Spark, by default, provides fault tolerance by recomputing any missing partitions in the fly. To optimize for performance, you can optionally provide a replication factor. But note that this will increase the initial cache time and storage usage significantly.\n",
    "\n",
    "To store an RDD in memory serialized backed by disk (deserialized) and a replication factor of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-eaa54f93728e>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-eaa54f93728e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    val rdd2 = sc.parallelize(1 to 1000)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "val rdd2 = sc.parallelize(1 to 1000)\n",
    "rdd2.persist(StorageLevel.MEMORY_AND_DISK_SER_2)\n",
    "\n",
    "import org.apache.spark.storage._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Storage\" tab in the Spark UI shows the list of all RDDs cached and also where each partition of a RDD resides in the cluster.\n",
    "\n",
    "To uncache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching DataFrames and tables in memory\n",
    "\n",
    "Spark's SQL tables and DataFrames can be cached too. The tables and DataFrames are cached in the JVM memory and compressed using a simple algorithm in a columnar format. Typically however, the compression ratio will not be as good as something like parquet. To cache a DataFrame in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val df = sqlContext.range(100)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cache/uncache a table in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val tableName = \"YOUR TABLE NAME HERE\"\n",
    "#// Load the table and cache it in memory as a best effort\n",
    "sqlContext.cacheTable(tableName)                     \n",
    "# // Your queries on df here..\n",
    "val df = sqlContext.table(tableName)  \n",
    "#// Uncache the table from memory\n",
    "sqlContext.uncacheTable(tableName)                   \n",
    "tableName: String = diamonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To uncache all tables from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
