{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "This notebook is a quick guide to explain different memory and disk persistence options in Spark.\n",
    "Caching RDDs\n",
    "To simply cache an RDD in memory in deserialized form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "N = 10**5\n",
    "rdd = sc.parallelize([x for x in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time square  1.19509005547\n",
      "Elapsed time sqrt  0.145571947098\n"
     ]
    }
   ],
   "source": [
    "#we will only get slow results when running the first time, because DAG will force it to cache!\n",
    "\n",
    "from math import sqrt\n",
    "start = time.time()\n",
    "rdd1 = rdd.map(lambda x: x*x)\n",
    "rdd1.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time square \", end-start\n",
    "start = time.time()\n",
    "rdd2 = rdd.map(lambda x: sqrt(x))\n",
    "rdd2.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time sqrt \", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time square  0.194844961166\n",
      "Elapsed time sqrt  0.142702817917\n"
     ]
    }
   ],
   "source": [
    "#in case we running same cell multiple times\n",
    "rdd.unpersist()\n",
    "rdd.cache()\n",
    "\n",
    "start = time.time()\n",
    "rdd1 = rdd.map(lambda x: x*x)\n",
    "rdd1.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time square \", end-start\n",
    "start = time.time()\n",
    "rdd2 = rdd.map(lambda x: sqrt(x))\n",
    "rdd2.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time sqrt \", end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs can also be persisted in memory backed by disk in serialized form with replication across Workers. You can read more on RDD persistence: http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence. \n",
    "\n",
    "## The primary factors to consider while choosing a storage level\n",
    "\n",
    "### Persistence Level: \n",
    "\n",
    "Storing in memory is the easiest option without too much overhead. If the entire RDD doesn't fit in memory, then in case of missing partitions, the partitions will be recomputed from the lineage automatically. But if an RDD which is formed from a wide transformation is going to be used heavily in an iterative or interactive fashion, then it is better to store it in memory backed by disk to ensure that the partitions are not recomputed.\n",
    "\n",
    "### Serialization: \n",
    "\n",
    "The default serialization in Spark is Java serialization. However for better peformance, we recommend Kryo serialization, which you can learn more about here\n",
    "\n",
    "### Replication: \n",
    "\n",
    "Spark, by default, provides fault tolerance by recomputing any missing partitions in the fly. To optimize for performance, you can optionally provide a replication factor. But note that this will increase the initial cache time and storage usage significantly.\n",
    "\n",
    "To store an RDD in memory serialized backed by disk (deserialized) and a replication factor of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time square  0.15674495697\n",
      "Elapsed time sqrt  0.101278066635\n"
     ]
    }
   ],
   "source": [
    "rdd.unpersist()\n",
    "\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "start = time.time()\n",
    "rdd1 = rdd.map(lambda x: x*x)\n",
    "rdd1.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time square \", end-start\n",
    "start = time.time()\n",
    "rdd2 = rdd.map(lambda x: sqrt(x))\n",
    "rdd2.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time sqrt \", end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Storage\" tab in the Spark UI shows the list of all RDDs cached and also where each partition of a RDD resides in the cluster.\n",
    "\n",
    "To uncache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching DataFrames and tables in memory\n",
    "\n",
    "Spark's SQL tables and DataFrames can be cached too. The tables and DataFrames are cached in the JVM memory and compressed using a simple algorithm in a columnar format. \n",
    "\n",
    "Typically however, the compression ratio will not be as good as something like parquet. To cache a DataFrame in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.range(N)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "rdd = sc.parallelize([x for x in range(N)],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time square  0.200296163559\n",
      "Elapsed time sqrt  0.122011184692\n"
     ]
    }
   ],
   "source": [
    "#we will only get slow results when running the first time, because DAG will force it to cache!\n",
    "rdd.cache()\n",
    "\n",
    "from math import sqrt\n",
    "start = time.time()\n",
    "rdd1 = rdd.map(lambda x: x*x)\n",
    "rdd1.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time square \", end-start\n",
    "start = time.time()\n",
    "rdd2 = rdd.map(lambda x: sqrt(x))\n",
    "rdd2.collect()\n",
    "end = time.time()\n",
    "print \"Elapsed time sqrt \", end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Large scale caching and partitioning tests\n",
    "\n",
    "Let us return to the Adroit cluster accounts, and rerun these tests in a distributed environment.\n",
    "I have prepared a relatively large (order of a GB) file here:\n",
    "\n",
    "```bash\n",
    "[alexeys@adroit3 4_Optimize] ls -l /scratch/network/alexeys/BigDataCourse/large/test.json \n",
    "-rw-r--r-- 1 alexeys cses 1090671230 Apr  4 23:00 /scratch/network/alexeys/BigDataCourse/large/test.json\n",
    "```\n",
    "\n",
    "Change into the exercise folder:\n",
    "\n",
    "```bash\n",
    "cd BigDataCourse/4_Optimize\n",
    "```\n",
    "\n",
    "inspect the cache_partition.py source file, and submit it without changes to the cluster by running:\n",
    "\n",
    "```bash\n",
    "sbatch slurm.cmd\n",
    "```\n",
    "\n",
    "As the slurm\\*out file appears in your submission area, connect to the Spark web UI:\n",
    "\n",
    "```bash\n",
    "firefox --no-remote http://<your master URL>:8080\n",
    "```\n",
    "\n",
    "where master URL will become available in the slurm output file.\n",
    "\n",
    "<img src=\"/Users/alexeys/Desktop/BigDataCourse_2016/4_Optimize/debug/SparkUI_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast variables\n",
    "\n",
    "Broadcast variables allow to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n",
    "\n",
    "Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\n",
    "\n",
    "## Creating broadcast variables\n",
    "\n",
    "Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The code below shows this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators\n",
    "\n",
    "Accumulators are variables that are only “added” to through an associative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types. If accumulators are created with a name, they will be displayed in Spark’s UI. This can be useful for understanding the progress of running stages (NOTE: this is not yet supported in Python).\n",
    "\n",
    "An accumulator is created from an initial value v by calling SparkContext.accumulator(v). Tasks running on the cluster can then add to it using the add method or the += operator (in Scala and Python). However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.\n",
    "\n",
    "The code below shows an accumulator being used to add up the elements of an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
