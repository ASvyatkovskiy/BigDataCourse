{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing RDDs: when, how, why\n",
    "\n",
    "Spark offers several options for RDD re-use including `persisting`, `caching`, and `checkpointing`,\n",
    "although Spark does not perform any of these automatically. \n",
    "\n",
    "Spark does not do so by default since storing RDD for re-use breaks some pipelining and can be\n",
    "a waste if the RDD is only used one or the data is inexpensive to recompute. Persisting/caching requires a lot of memory or disk and is unlikely to improve performance for operations that are preformed only once. \n",
    "\n",
    "See H. Karau and R. Warren, \"High-performance Spark\" O'Reilly book for more details.\n",
    "\n",
    "## Iterative Computations\n",
    "\n",
    "For transformations that use the same parent RDD multiple times, re-using an RDD\n",
    "forces evaluation of that RDD and so can help avoid repeated computations. For\n",
    "example, if you were performing a loop of joins to the same dataset, persisting that\n",
    "dataset could lead to huge performance improvements since it ensures that the partitions\n",
    "of that RDD will be available in-memory to do each join.\n",
    "\n",
    "In the following example we are computing the root mean squared error (RMSE) on\n",
    "a number of different RDDs representing predictions from different models. To do\n",
    "this we have to join each RDD of predictions to an RDD of the data in the validation\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a sample dataset of key-value pairs\n",
    "import random\n",
    "keyValuePairs = [(i,random.randint(1,100)) for i in range(100000)]\n",
    "\n",
    "#Define root-mean square function\n",
    "def RMSE(rdd):\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    n = rdd.count()\n",
    "    return sqrt(rdd.map(lambda x: (x[0] - x[1]) ** 2).reduce(add) / float(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors is:  1.0\n",
      "Elapsed time, step1:  1.09853816032\n",
      "Error is:  2.0\n",
      "Elapsed time, step2  1.10933494568\n"
     ]
    }
   ],
   "source": [
    "start = time.time() \n",
    "validationSet = sc.parallelize(keyValuePairs) #.cache()\n",
    "#validationSet.unpersist()\n",
    "testSet = validationSet.mapValues(lambda x: x+1)\n",
    "print \"Errors is: \", RMSE(testSet.join(validationSet).values())\n",
    "end = time.time()\n",
    "print \"Elapsed time, step1: \", end-start    \n",
    "\n",
    "testSet = validationSet.mapValues(lambda x: x+2)\n",
    "print \"Error is: \", RMSE(testSet.join(validationSet).values())\n",
    "end2 = time.time()\n",
    "print \"Elapsed time, step2 \", end2-end    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs can also be persisted in memory backed by disk in serialized form with replication across Workers. You can read more on RDD persistence: http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence. \n",
    "\n",
    "## The primary factors to consider while choosing a storage level\n",
    "\n",
    "### Persistence Level: \n",
    "\n",
    "Storing in memory is the easiest option without too much overhead. If the entire RDD doesn't fit in memory, then in case of missing partitions, the partitions will be recomputed from the lineage automatically. But if an RDD which is formed from a wide transformation is going to be used heavily in an iterative or interactive fashion, then it is better to store it in memory backed by disk to ensure that the partitions are not recomputed.\n",
    "\n",
    "### Serialization: \n",
    "\n",
    "The default serialization in Spark is Java serialization. However for better peformance, we recommend Kryo serialization, which you can learn more about here\n",
    "\n",
    "### Replication: \n",
    "\n",
    "Spark, by default, provides fault tolerance by recomputing any missing partitions in the fly. To optimize for performance, you can optionally provide a replication factor. But note that this will increase the initial cache time and storage usage significantly.\n",
    "\n",
    "To store an RDD in memory serialized backed by disk (deserialized) and a replication factor of 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching DataFrames and tables in memory\n",
    "\n",
    "Spark's SQL tables and DataFrames can be cached too. The tables and DataFrames are cached in the JVM memory and compressed using a simple algorithm in a columnar format. \n",
    "\n",
    "Typically however, the compression ratio will not be as good as something like parquet. To cache a DataFrame in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=100000\n",
    "df = sqlContext.range(N)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Partitioning\n",
    "\n",
    "Partition in Spark represents a unit of parallel execution that corresponds to one task. The number of tasks that can be executed concurrently is limited by the total number of executor cores in the Spark cluster. \n",
    "\n",
    "The partitioner object defines a function from an element of a pair RDD to a partition via a mapping from each record to partition number. By assigning a partitioner to an RDD we can guarantee something about the data that lives in each partition, for example that it falls within a given range (range partitioner), or includes only elements whose keys\n",
    "have the same hash code (hash partitioner). \n",
    "\n",
    "## Changing the RDD partitioning\n",
    "\n",
    "There are three methods that exists exclusively to change the way an RDD is partitioned. In the generic RDD class `repartition` and `coalesce` can be used simply change the number of partitions that the RDD uses, irrespective of the value of the records in the RDD. \n",
    "\n",
    "`repartition` transformation causes a shuffle, while `coalesce` is an optimized version of repartition that avoids a full shuffle if the desired number of partitions is less than the current number of partitions. When coalesce reduces the number of partitions, it does so by merely combining partitions and thus is not a wide transformation since the\n",
    "partition can be determined at design time. \n",
    "\n",
    "For RDDs of key/value pairs, we can use a function called `partitionBy`, which takes a partition object rather than a\n",
    "number of partitions and shuffles the RDD with the new partitioner. \n",
    "\n",
    "PartitionBy allows for much more control in the way that the records are partitioned since the\n",
    "partitioner supports defining a function that assigns a partition to a record based on\n",
    "the value of that key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "N = 100000\n",
    "rdd = sc.parallelize([x for x in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print rdd.coalesce(2).getNumPartitions()\n",
    "print rdd.repartition(2).getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Partitioning of key-value RDDs\n",
    "\n",
    "Following plots show a schematic representation of how word count is performed in a distributed way using `groupByKey` and `reduceByKey` approach. Aggregation is going to be discussed later in the course in more details.\n",
    "\n",
    "<img src=\"./debug/groupbykey.png\">\n",
    "\n",
    "<img src=\"./debug/reducebykey.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Large scale caching and partitioning tests\n",
    "\n",
    "Let us return to the Adroit cluster accounts, and rerun these tests in a distributed environment.\n",
    "I have prepared a relatively large (order of a GB) file here:\n",
    "\n",
    "```bash\n",
    "[alexeys@adroit3 4_Optimize] ls -l /scratch/network/alexeys/BigDataCourse/large/test.json \n",
    "-rw-r--r-- 1 alexeys cses 1090671230 Apr  4 23:00 /scratch/network/alexeys/BigDataCourse/large/test.json\n",
    "```\n",
    "\n",
    "Change into the exercise folder:\n",
    "\n",
    "```bash\n",
    "cd BigDataCourse/4_Optimize\n",
    "```\n",
    "\n",
    "inspect the cache_partition.py source file, and submit it without changes to the cluster by running:\n",
    "\n",
    "```bash\n",
    "sbatch slurm.cmd\n",
    "```\n",
    "\n",
    "As the slurm\\*out file appears in your submission area, connect to the Spark web UI:\n",
    "\n",
    "```bash\n",
    "firefox --no-remote http://<your master URL>:8080\n",
    "```\n",
    "\n",
    "where master URL will become available in the slurm output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast variables\n",
    "\n",
    "Broadcast variables allow to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n",
    "\n",
    "Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\n",
    "\n",
    "## Creating broadcast variables\n",
    "\n",
    "Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The code below shows this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators\n",
    "\n",
    "Accumulators are variables that are only “added” to through an associative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types. If accumulators are created with a name, they will be displayed in Spark’s UI. This can be useful for understanding the progress of running stages (NOTE: this is not yet supported in Python).\n",
    "\n",
    "An accumulator is created from an initial value v by calling SparkContext.accumulator(v). Tasks running on the cluster can then add to it using the add method or the += operator (in Scala and Python). However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.\n",
    "\n",
    "The code below shows an accumulator being used to add up the elements of an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
