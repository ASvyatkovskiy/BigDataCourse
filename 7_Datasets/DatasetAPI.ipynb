{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Datasets API, added in Spark 1.6, provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL's optimized execution engine. \n",
    "\n",
    "A Dataset can be constructed using Scala objects/case classes and then manipulated using functional transformations (`map`, `flatMap`, `filter`, etc.) similar to RDD. The benefits is that, unlike RDD, these transformations are now applied on a structured and strongly typed distributed collection that allows Spark to leverage several optimizations (Catalyst).\n",
    "\n",
    "Typically, Jupyter will create SparkSession (spark) and SparkContext (sc) for you, but in this case we are going to need to recreate the SparkSession to be albe to import implicits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"MakeLabeledCartesian\").getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = Seq(1, 2, 3).toDS()\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a sequence of case classes, simply calling `.toDS()` will provide a dataset with all the necessary fields in the dataset.\n",
    "\n",
    "Similarly to the above, we can define a dataset from the sequence of objects (case classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|   Max| 33|\n",
      "|  Adam| 32|\n",
      "|Muller| 62|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class Person(name: String, age: Int)\n",
    "val personDS = Seq(Person(\"Max\", 33), Person(\"Adam\", 32), Person(\"Muller\", 62)).toDS()\n",
    "personDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating datasets from RDD and DataFrames\n",
    "\n",
    "Datasets can be easily converted to/from DataFrames and RDDs. \n",
    "Calling `.toDS()` in an RDD converts it into a Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| _1|        _2|\n",
      "+---+----------+\n",
      "|  1|     Spark|\n",
      "|  2|Databricks|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val rdd = sc.parallelize(Seq((1, \"Spark\"), (2, \"Databricks\")))\n",
    "val integerDS = rdd.toDS()\n",
    "integerDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|uid|      name|\n",
      "+---+----------+\n",
      "|  1|     Spark|\n",
      "|  2|Databricks|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val integerDF = rdd.toDF(\"Id\",\"Name\")\n",
    "integerDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call df.as[SomeCaseClass] to convert the DataFrame to a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|uid|      name|\n",
      "+---+----------+\n",
      "|  1|     Spark|\n",
      "|  2|Databricks|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class Entity(Id: Int, Name: String)\n",
    "val integerDS = integerDF.as[Entity]\n",
    "integerDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also deal with tuples while converting a DataFrame to Dataset without using a case class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|uid|      name|\n",
      "+---+----------+\n",
      "|  1|     Spark|\n",
      "|  2|Databricks|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dataset = integerDF.as[(Int, String)]\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Datasets\n",
    "\n",
    "Let us consider a simple word count example. First, we prepare a Dataset from a sequence of strings. Next, we apply normalization and tokenization transformations - note here, we have to use `flatMap` transformation instead of `map` to create a single list from all the sentences (the result of the tokenization of each sentence is a list of tokens, so `map` would operate on a list of lists).\n",
    "\n",
    "Finally, we filter empty tokens, and groupBy value column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val wordsDataset = sc.parallelize(Seq(\"Spark I am your father\", \"May the spark be with you\", \"Spark I am your father\")).toDS()\n",
    "val groupedDataset = wordsDataset.flatMap(_.toLowerCase.split(\" \")).filter(_ != \"\").groupBy(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply various sorts of aggrewgation functions on grouped data (more about this in the next section), in particular, applying `count` will produce a column with counts of elements in each group (with the same name - count):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| value|count|\n",
      "+------+-----+\n",
      "|father|    2|\n",
      "|   you|    1|\n",
      "|  with|    1|\n",
      "|    be|    1|\n",
      "|  your|    2|\n",
      "|   may|    1|\n",
      "| spark|    3|\n",
      "|   the|    1|\n",
      "|     i|    2|\n",
      "|    am|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val countsDataset = groupedDataset.count()\n",
    "countsDataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union, group by, join of datasets\n",
    "\n",
    "The following example demonstrates:\n",
    "\n",
    " 1. Union multiple datasets\n",
    " 1. Doing an inner join on a condition\n",
    " 1. Grouping by a specific column\n",
    " \n",
    "The examples use only Datasets API to demonstrate all the operations available. In reality, using dataframes for doing aggregation would be simpler and faster than doing custom aggregation with `mapGroups`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------------+--------+\n",
      "|  name|age|departmentId|  salary|\n",
      "+------+---+------------+--------+\n",
      "|   Max| 22|           1|100000.0|\n",
      "|  Adam| 33|           2| 93000.0|\n",
      "|   Eve| 35|           2| 89999.0|\n",
      "|Muller| 39|           3|120000.0|\n",
      "|  John| 26|           1|990000.0|\n",
      "|   Joe| 38|           3|115000.0|\n",
      "+------+---+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class Employee(name: String, age: Int, departmentId: Int, salary: Double)\n",
    "\n",
    "val employeeDataset1 = Seq((\"Max\", 22, 1, 100000.0), (\"Adam\", 33, 2, 93000.0), (\"Eve\", 35, 2, 89999.0), (\"Muller\", 39, 3, 120000.0)).toDF(\"name\",\"age\",\"departmentId\",\"salary\").as[Employee]\n",
    "val employeeDataset2 = Seq((\"John\", 26, 1, 990000.0), (\"Joe\", 38, 3, 115000.0)).toDF(\"name\",\"age\",\"departmentId\",\"salary\").as[Employee]\n",
    "\n",
    "val employeeDataset = employeeDataset1.union(employeeDataset2)\n",
    "employeeDataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|       name|\n",
      "+---+-----------+\n",
      "|  1|Engineering|\n",
      "|  2|  Marketing|\n",
      "|  3|      Sales|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class Department(id: Int, name: String)\n",
    "case class Record(name: String, age: Int, salary: Double, departmentId: Int, departmentName: String)\n",
    "case class ResultSet(departmentId: Int, departmentName: String, avgSalary: Double)\n",
    "\n",
    "val departmentDataSet = Seq((1, \"Engineering\"), (2, \"Marketing\"), (3, \"Sales\")).toDF(\"id\",\"name\").as[Department]\n",
    "departmentDataSet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val exampleJoin = employeeDataset.joinWith(departmentDataSet, $\"departmentId\" === $\"id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|                  _1|             _2|\n",
      "+--------------------+---------------+\n",
      "| [Max,22,1,100000.0]|[1,Engineering]|\n",
      "| [Adam,33,2,93000.0]|  [2,Marketing]|\n",
      "|  [Eve,35,2,89999.0]|  [2,Marketing]|\n",
      "|[Muller,39,3,1200...|      [3,Sales]|\n",
      "|[John,26,1,990000.0]|[1,Engineering]|\n",
      "| [Joe,38,3,115000.0]|      [3,Sales]|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exampleJoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
