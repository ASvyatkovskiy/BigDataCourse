{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP example case study: Apple/apple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook documents an NLP case study. \n",
    "\n",
    "Useful tools (all - Python libraries):\n",
    " - http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    " - http://www.nltk.org/\n",
    " - http://www.nltk.org/howto/wordnet.html\n",
    " \n",
    "The main Python package used for test mining is **nltk** \n",
    "\n",
    "##Overview\n",
    " - Bag of words\n",
    " - TF/IDF\n",
    " - n-grams\n",
    " - Stemming / part of speech tagging / etc.\n",
    " - Feature hashing\n",
    " - Other topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem: Word sense disambiguation.  \n",
    "\n",
    "In a given block of text, we need to be able to distinguish between the meaning of Apple (the company) vs. apple (the fruit).  Ideally one would also be able to tell apart Windows vs windows, etc. via similar examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The type of learner**: going to choose to look at this as a _supervised classification_ problem. This means we need some labeled data.\n",
    "\n",
    "**The training dataset**: going to try to use Wikipedia's articles on the given topics as a chosen \"corpus\" of text.\n",
    "\n",
    "**The test dataset**: Possible option would be to mark up a small corpus by hand, or to use sentences taken from Wikipedia with the disambiguation coming from looking at the target of outgoing links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorming:\n",
    "\n",
    "For \"Apple\"/\"apple\" case simple capitalization, presence of a possessive, could be the main discriminants. To do better one could look at nearby words (e.g., \"software\" and \"computer\" hint one way, while \"flavor\" hints the other).  Figuring out how to turn this idea into an actual implementation takes us to our first two topics: **bag of words** and **TF/IDF**.\n",
    "\n",
    "How might we do this?\n",
    "  1. We need to clean and tokenize the text data: __Tokenization__ refers to splitting the text into pieces, in this case into sentences and into words.  Cleaning can also include things like __stemming__ or __lemmatizing__ (identifying similar words like \"computer\" and \"computers\" to their stem).\n",
    "  2. We need to extract __features__.  We're going to think of our input as a sentence, and try to develop features of that sentence.  In this example application, I try to use:\n",
    "   - Capitalized of the word apple? (_a_pple vs _A_pple)    \n",
    "   - Pluralization of the word apple? (apples)\n",
    "   - Possessive form of the word apple? (Apple's)\n",
    "   - Presence (or frequency) of certain well-chosen words : Does (e.g.,) the word \"computer\" or \"fruit\" occur in the sentence?  (This feature regards the sentence as a simple __bag of words__ without regard to trying to parse its structure.)\n",
    "   - In addition to single words, I also try looking for __n-grams__: Strings of n consecutive words.\n",
    "   - There are common techniques for determining which words / n-grams to look for.  One of them is called __tf-idf__.\n",
    "  3. Finally, run some sort of classifier on the features.\n",
    "  \n",
    "I mostly focused on general NLP techniques in 1 and 2, rather than diving deeply into techniques for word disambiguation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Cleaning and tokenizing the data\n",
    "\n",
    "The goal of the tokenization step is to clean the text and load it one string per sentence. I am going to use the nltk for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Splitting into words/sentences:\n",
    "NLTK has convenient presets for sentence and word tokenization (i.e., splitting a document into sentences, resp. splitting a sentence into words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.'], ['Please', 'buy', 'me', 'two', 'of', 'them', '.'], ['Thanks', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "my_long_string = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "my_list_of_sentences = nltk.tokenize.sent_tokenize(my_long_string) \n",
    "words = [ nltk.tokenize.word_tokenize(sent) for sent in my_list_of_sentences]\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk.tokenize\n",
    "\n",
    "# Spit out (slightly cleaned up) sentences from a Wikipedia article.\n",
    "def wikipedia_to_sents(url):\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url)).find(attrs={'id':'mw-content-text'})\n",
    "\n",
    "    # The text is litered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join( re.split('\\[\\d+\\]', s) )\n",
    "    \n",
    "    paragraphs = [drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    \n",
    "    raw_sents = reduce(lambda x, y: x + y, [nltk.tokenize.sent_tokenize(p.strip()) for p in paragraphs if p.strip()!=''])    \n",
    "    return filter(lambda s: len(s.split(\" \"))>2, raw_sents)\n",
    "\n",
    "fruit_sents = wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple\")\n",
    "company_sents = wikipedia_to_sents(\"http://en.wikipedia.org/wiki/Apple_Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, online services, and personal computers.',\n",
       " u'Its best-known hardware products are the Mac line of computers, the iPod media player, the iPhone smartphone, the iPad tablet computer, and the Apple Watch smartwatch.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#company_sents[-110:-100]\n",
    "company_sents[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Malus pumila auct.', u'Pyrus malus L.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit_sents[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 1.1.  Features: Bag-of-words\n",
    "\n",
    "Using text for learning algorithms is not good, need to use vectors of numbers instead. \n",
    "The simplest way to turn a text into a vector of number is to treat the text as a \"bag of words.\"  Namely\n",
    "\n",
    "  - Split the text into words\n",
    "  - Count how many times each word (/each word in some fixed vocabulary) occurs\n",
    "  - _(Variant)_ Just do a binary \"yes / no\" for whether each word (/.. in some vocabulary) is contained in the material\n",
    "  \n",
    "The output is a very large, but usually sparse, vector: The number of coordinates is the number of words in our dictionary, and the $i$-th coordinate entry is the number of occurances of the $i$-th word.\n",
    "\n",
    "There's a reasonable implementation of this in the CountVectorizer class in sklearn.feature_extraction.text.  See http://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref for more detail on the options.\n",
    "\n",
    "For instance, here we'll apply this to each sentence (as a separate bag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 26)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# *temporarily* use a small hand built vocabulary of words.\n",
    "vocabulary = \"fruit eat tasty pie leaf cook tree computer computers laptop tech technology ceo jobs ipad iphone announce announced mac company companies employee employees user software released\".split(\" \")\n",
    "\n",
    "bag_of_words_vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "counts = bag_of_words_vectorizer.fit_transform( fruit_sents[1:3] + company_sents[1:3])\n",
    "print counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0]]\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 24)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "# Note that this is a **sparse** matrix.\n",
    "print counts.toarray() #This is what it actually looks like..\n",
    "print counts           # .. this is just describing the non-zero entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit',\n",
       " 'eat',\n",
       " 'tasty',\n",
       " 'pie',\n",
       " 'leaf',\n",
       " 'cook',\n",
       " 'tree',\n",
       " 'computer',\n",
       " 'computers',\n",
       " 'laptop',\n",
       " 'tech',\n",
       " 'technology',\n",
       " 'ceo',\n",
       " 'jobs',\n",
       " 'ipad',\n",
       " 'iphone',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'mac',\n",
       " 'company',\n",
       " 'companies',\n",
       " 'employee',\n",
       " 'employees',\n",
       " 'user',\n",
       " 'software',\n",
       " 'released']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "It's common to want to __omit__ certain common words when doing these counts -- \"a\", \"an\", and \"the\" are common enough so that their counts do not tend to give us any hints as to the meaning of documents.  Such words that we want to omit are called __stop words__ (they don't stop anything, though).\n",
    "\n",
    "NLTK contains a standard list of such stop words for English in `nltk.corpus.stopwords.words('english')`.  In our application, we'd also want to include \"apple\" -- it is certainly not going to help us distinguish our two meanings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.stopwords.words('english')[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10', u'100', u'16', u'19', u'1997', u'1999', u'20', u'2001', u'2006']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<210x300 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 751 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=CountVectorizer(max_features=300,\n",
    "                        stop_words=nltk.corpus.stopwords.words('english') + ['apple'])\n",
    "counter=counter.fit( fruit_sents + company_sents )\n",
    "print counter.get_feature_names()[1:10]\n",
    "\n",
    "# Now we can use it with that vectorizer, like so...\n",
    "counter.transform(company_sents)\n",
    "counter.transform(fruit_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 162)\t1\n",
      "  (1, 162)\t1\n",
      "  (2, 162)\t1\n",
      "  (3, 44)\t1\n",
      "  (3, 104)\t1\n",
      "  (3, 139)\t1\n",
      "  (3, 162)\t1\n",
      "  (3, 269)\t2\n",
      "  (4, 104)\t1\n",
      "  (4, 111)\t1\n",
      "  (4, 162)\t1\n",
      "  (4, 269)\t1\n",
      "  (4, 295)\t1\n",
      "  (5, 37)\t1\n",
      "  (5, 52)\t1\n",
      "  (5, 100)\t1\n",
      "  (5, 162)\t1\n",
      "  (5, 250)\t1\n",
      "  (5, 269)\t1\n",
      "  (5, 288)\t1\n",
      "  (6, 32)\t1\n",
      "  (6, 37)\t1\n",
      "  (6, 48)\t1\n",
      "  (6, 92)\t1\n",
      "  (6, 111)\t1\n",
      "  :\t:\n",
      "  (203, 234)\t1\n",
      "  (203, 244)\t1\n",
      "  (204, 141)\t1\n",
      "  (204, 234)\t1\n",
      "  (204, 244)\t1\n",
      "  (205, 139)\t1\n",
      "  (205, 185)\t2\n",
      "  (205, 234)\t2\n",
      "  (206, 167)\t1\n",
      "  (206, 222)\t1\n",
      "  (206, 239)\t1\n",
      "  (206, 257)\t1\n",
      "  (207, 25)\t1\n",
      "  (207, 53)\t2\n",
      "  (207, 75)\t3\n",
      "  (207, 76)\t1\n",
      "  (207, 88)\t1\n",
      "  (207, 97)\t1\n",
      "  (207, 104)\t1\n",
      "  (207, 113)\t1\n",
      "  (207, 190)\t1\n",
      "  (207, 193)\t1\n",
      "  (207, 250)\t1\n",
      "  (207, 278)\t1\n",
      "  (207, 291)\t1\n"
     ]
    }
   ],
   "source": [
    "#print counter.transform(company_sents)[1:2]\n",
    "print counter.transform(fruit_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##n-grams\n",
    "\n",
    "Instead of looking at just single words, it is also useful to look at **n-grams**: These are n-word long sequences of words (i.e., each of \"farmer's market\", \"market share\", and \"farm share\" is a 2-gram).\n",
    "\n",
    "The exact same sort of counting techniques apply.  The `CountVectorizer` function has built in support for this, too:\n",
    "\n",
    "If you pass it the `ngram_range=(m, M)` then it will count $n$-grams with  $m \\leq n \\leq M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2011 jobs', u'2012 update', u'2014 update', u'24 2012', u'27 2010', u'30 000', u'30 2012', u'33182 122', u'37 33182']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<210x300 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 191 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_counter=CountVectorizer(max_features=300, \n",
    "                           ngram_range=(2,2), \n",
    "                           stop_words=nltk.corpus.stopwords.words('english') + ['apple', 'Apple'])\n",
    "ng_counter=ng_counter.fit( fruit_sents + company_sents  )\n",
    "print ng_counter.get_feature_names()[1:10]\n",
    "\n",
    "# Now we can use it with that vectorizer, like so...\n",
    "ng_counter.transform(company_sents)\n",
    "ng_counter.transform(fruit_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (3, 165)\t1\n"
     ]
    }
   ],
   "source": [
    "#print counter.transform(company_sents)[1:2]\n",
    "print ng_counter.transform(fruit_sents)[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TF-IDF: term frequencyâ€“inverse document frequency\n",
    "\n",
    "With single word vocabularies, we can probably do an okay job of coming up with a reasonable list of words that distinguish between the two documents.  With n-grams, even for $n=2$, it is better to let a computer help us.  \n",
    "\n",
    "Just using frequencies, as above, is clearly not great.  Both apples the fruit and Apple the company are enjoyed around the world (one of the 2-grams that came up above!).  We would like to find words that are common in one document, but not common in all of them.  This is the goal of the __td-idf weighting__.  A precise definition is:\n",
    "\n",
    "\n",
    "  1. If $d$ denotes a document and $t$ denotes a term, then the _raw term frequency_ $\\mathrm{tf}^{raw}(t,d)$ is\n",
    "  $$ \\mathrm{tf}^{raw}(t,d) = \\text{the number of times the term $t$ occurs in the document $d$} $$\n",
    "  The vector of all term frequencies can optionally be _normalized_ either by dividing by the maximum of ny single word's occurance count ($L^1$) or by the Euclidean length of the vector of word occurance counts ($L^2$).  Scikit-learn by defaults does this second one:\n",
    "  $$ \\mathrm{tf}(t,d) = \\mathrm{tf}^{L^2}(t,d) = \\frac{\\mathrm{tf}^{raw}(t,d)}{\\sqrt{\\sum_t \\mathrm{tf}^{raw}(t,d)^2}} $$\n",
    "  2. If $$ D = \\left\\{ d : d \\in D \\right\\} $$ is the set of possible documents, then  the _inverse document frequency_ is\n",
    "  $$ \\mathrm{idf}^{naive}(t,D) = \\log \\frac{\\# D}{\\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "  = \\log \\frac{\\text{count of all documents}}{\\text{count of those documents containing the term $t$}} $$\n",
    "  with a common variant being\n",
    "  $$ \\mathrm{idf}(t, D) = \\log \\frac{\\# D}{1 + \\# \\{d \\in D : t \\in d\\}} \\\\\n",
    "   = \\log \\frac{\\text{count of all documents}}{1 + \\text{count of those documents containing the term $t$}} $$\n",
    "  (This second one is the default in scikit-learn. Without this tweak we would omit the $1+$ in the denominator and have to worry about dividing by zero if $t$ is not found in any documents.)\n",
    "  3. Finally, the weight that we assign to the term $t$ appearing in document $d$ and depending on the corpus of all documents $D$ is\n",
    "  $$ \\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\mathrm{idf}(t,D) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'10', u'100', u'16', u'19', u'20', u'2001', u'2006', u'2007', u'2008', u'2009', u'2010', u'2011', u'2012', u'2013', u'2014', u'2015', u'24', u'30', u'500', u'800', u'access', u'according', u'added', u'almost', u'also', u'america', u'american', u'announced', u'app']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "ng_tfidf=TfidfVectorizer(max_features=300, \n",
    "                         ngram_range=(1,2), \n",
    "                         stop_words=nltk.corpus.stopwords.words('english') + [\"apple\", \"apples\"],\n",
    "                         )#token_pattern=u'\\w') #u'[^0-9]+')\n",
    "ng_tfidf=ng_tfidf.fit( fruit_sents + company_sents )\n",
    "print ng_tfidf.get_feature_names()[1:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In addition: Document similarity\n",
    "\n",
    "A common problem is looking up a document similar to a given snippet, or relatedly comparing two documents for similarity.  The above provides a simple method for this called __cosine similarity__:\n",
    "  - To each of the two douments $d_1, d_2$ in a corpus of documents $D$, assign its tf or tf-idf vector $$ (v_i)_{j} = \\mathrm{tfidf}( t_{j}, d_i, D ) $$\n",
    "  where $i$ ranges over indices for documents, and $j$ ranges over indices for terms in the vocabulary.\n",
    "  - To compare two documents, simply find the cosine of the angle between the vectors:\n",
    "  $$ \\frac{v_i \\cdot v_{i'}}{|v_i| |v_{i'}|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "In our original hand-built vocabulary, we had to include both \"computer\" and \"computers\".  It would have been useful to identify them as one word.\n",
    "\n",
    "This is not limited to just trailing \"s\" characters: e.g., the words \"carry\", \"carries\", \"carrying\", and \"carried\" all carry -- roughly -- the same meaning.  The process of replacing them by a common root, or **stem**, is called stemming -- the stem will not, in general, be a full word itself.\n",
    "\n",
    "There's a related process called **lemmatization**: The analog of the \"stem\" here _is_ an actual word.  We can choose to first stem our words before counting them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming carry [u'carri', u'carri', u'carri', u'carri']\n",
      "stemming eat [u'eat', u'eat', u'eaten', u'ate']\n",
      "the quick brown fox jumped over the lazy dog.  i can't believe it's not butter.  i tried to ford the river and my unfortunate oxen died.\n",
      "the quick brown fox jump over the lazi dog.  i can't believ it not butter.  i tri to ford the river and my unfortun oxen died.\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "print \"stemming carry\", [stemmer.stem(s) for s in [\"carry\", \"carries\", \"carrying\", \"carried\"]]\n",
    "print \"stemming eat\", [stemmer.stem(s) for s in [\"eat\", \"eating\", \"eaten\", \"ate\"]]\n",
    "                                \n",
    "# More examples \n",
    "print stemmer.stem(\"The quick brown fox jumped over the lazy dog.  I can't believe it's not butter.  I tried to ford the river and my unfortunate oxen died.\")\n",
    "print \" \".join(map(stemmer.stem, \"The quick brown fox jumped over the lazy dog.  I can't believe it's not butter.  I tried to ford the river and my unfortunate oxen died.\".split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma carry ['carry', u'carry', 'carrying', 'carried']\n",
      "lemma eat ['eat', 'eating', 'eaten', 'ate']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "print \"lemma carry\", [lemmatizer.lemmatize(s) for s in [\"carry\", \"carries\", \"carrying\", \"carried\"]]\n",
    "print \"lemma eat\", [lemmatizer.lemmatize(s) for s in [\"eat\", \"eating\", \"eaten\", \"ate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell our bag-of-words counters (/tf-idf) to first run its input through the stemmer.  This way it won't have to include both e.g., 'computer' and 'computers':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'000', u'10', u'100', u'16', u'19', u'20', u'2001', u'2006', u'2007', u'2008', u'2009', u'2010', u'2011', u'2012', u'2013', u'2014', u'2015', u'30', u'800', u'access', u'accord', u'ad', u'addit', u'aim', u'allow', u'also', u'american', u'announc', u'app', u'app store', u'applic', u'april', u'around', u'august', u'avail', u'averag', u'back', u'base', u'becam', u'began', u'best', u'billion', u'brand', u'brought', u'build', u'call', u'camera', u'campus', u'centuri', u'ceo', u'chang', u'china', u'climat', u'color', u'commerci', u'common', u'compani', u'comput', u'condit', u'consum', u'continu', u'control', u'cook', u'corpor', u'countri', u'creat', u'cultiv', u'cultivar', u'davidson', u'day', u'decemb', u'design', u'develop', u'devic', u'differ', u'digit', u'direct', u'diseas', u'display', u'download', u'due', u'dwarf', u'earli', u'effect', u'electron', u'employe', u'end', u'europ', u'even', u'event', u'facil', u'factori', u'featur', u'first', u'focus', u'follow', u'form', u'found', u'foxconn', u'free', u'fresh', u'fruit', u'gb', u'general', u'generat', u'golden', u'grown', u'high', u'hour', u'howev', u'ii', u'imac', u'import', u'improv', u'inc', u'includ', u'increas', u'india', u'individu', u'industri', u'inform', u'intern', u'introduc', u'io', u'ipad', u'iphon', u'ipod', u'itun', u'itun store', u'januari', u'job', u'juli', u'june', u'known', u'labor', u'larg', u'largest', u'later', u'launch', u'led', u'like', u'line', u'locat', u'logo', u'long', u'low', u'mac', u'mac os', u'macintosh', u'made', u'maintain', u'major', u'make', u'malus', u'manag', u'mani', u'manufactur', u'march', u'market', u'may', u'media', u'microsoft', u'million', u'mobil', u'model', u'much', u'music', u'name', u'new', u'north', u'novemb', u'number', u'octob', u'offer', u'often', u'one', u'onlin', u'open', u'oper', u'oper system', u'organ', u'origin', u'os', u'part', u'pay', u'peopl', u'per', u'period', u'person', u'person comput', u'pest', u'phone', u'plan', u'plant', u'platform', u'player', u'point', u'pollin', u'popular', u'power', u'practic', u'presid', u'price', u'pro', u'processor', u'produc', u'product', u'profit', u'program', u'project', u'provid', u'public', u'publish', u'purchas', u'quarter', u'rang', u'rate', u'raw', u'reaction', u'record', u'reduc', u'releas', u'replac', u'report', u'research', u'result', u'retail', u'retail store', u'revenu', u'rootstock', u'sale', u'samsung', u'sculley', u'second', u'seed', u'select', u'sell', u'septemb', u'seri', u'servic', u'set', u'sever', u'share', u'show', u'signific', u'similar', u'sinc', u'size', u'softwar', u'sold', u'sourc', u'standard', u'state', u'steve', u'still', u'stock', u'storag', u'store', u'success', u'suicid', u'suit', u'supplier', u'symbol', u'system', u'take', u'tax', u'team', u'technolog', u'third', u'though', u'three', u'time', u'total', u'touch', u'tree', u'tv', u'two', u'uk', u'unit', u'unit state', u'updat', u'us', u'use', u'user', u'varieti', u'various', u'version', u'video', u'well', u'wide', u'wild', u'within', u'without', u'work', u'worker', u'world', u'worldwid', u'would', u'year', u'yield']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "def tokenize_stem(text):\n",
    "    \"\"\"\n",
    "    We will use the default tokenizer from TfidfVectorizer, combined with the nltk SnowballStemmer.\n",
    "    \"\"\"\n",
    "    tokens = default_tokenizer(text)\n",
    "    stemmed = map(stemmer.stem, tokens)\n",
    "    return stemmed\n",
    "\n",
    "ng_stem_tfidf=TfidfVectorizer(max_features=300, \n",
    "                         ngram_range=(1,2), \n",
    "                         stop_words=map(stemmer.stem, nltk.corpus.stopwords.words('english') + [\"apple\"]),\n",
    "                         tokenizer = tokenize_stem)\n",
    "ng_stem_tfidf=ng_stem_tfidf.fit( fruit_sents + company_sents )\n",
    "\n",
    "ng_stem_vocab = ng_stem_tfidf.get_feature_names()\n",
    "print ng_stem_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Variation: Feature hashing\n",
    "\n",
    "When doing \"bag of words\" type techniques on a *large* corpus and without an existing vocabulary, there is a simple trick that can often be useful.  The issue (and solution) is as follows: \n",
    "\n",
    " - The output is a feature vector, so that whenever we encounter a word we must look up which coordinate slot it is in.  A naive way would be to keep a list of all the words encoutered so far, and look up each word when it is encountered.  Whenever we encounter a new word, we see if we've already seen it before and if not -- assign it a new number.  This requires storing all the words that we have seen in memory, cannot be done in parallel (because we'd have to share the hash-table of seen words), etc.\n",
    " - A **hash function** takes as input something complicated (like a string) and spits out a number, with the desired property being that different inputs *usually* produce different outputs.  (This is how hash tables are implemented, as the name suggests.)\n",
    " - So -- rather than exactly looking up the coordinate of a given word, we can just use its hash value (modulo a big size that we choose).  This is fast and parallelizes easily.  (There are some downsides: You cannot tell, after the fact, what word each of your feature actually corresponds to!)\n",
    " \n",
    "Scikit-learn includes `sklearn.feature_extraction.text.HashingVectorizer` to do this.  It behaves as almost a drop-in replacement for `CountVectorizer`.  It can be used with tf-idf by combining it with the `TfidfTransformer` (the `TfidfVectorizer` is the `CountVectorizer` together with the `TfidfTransformer`). For our application (where the training and test data is small), we may as well just use `TfidfVectorizer` -- but it is good to know that `HashingVectorizer` is there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 1.2: Some other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part of speech tagging.\n",
    "\n",
    "Consider the \"Ford\" vs \"ford\" example.  As a human being, the easiest way to tell these apart is that Ford is a __noun__ while ford is a __verb__.\n",
    "\n",
    "Fortunately, NLTK also has a part-of-speech tagger: You give it a sentence, and it tries to tag the parts of speech (e.g., noun, verb, adjective, etc.).  The command is `nltk.pos_tag` and for documentation on the tags either search around online, or use `nltk.help.upenn_tagset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = \"I tried to ford the river, and my unfortunate oxen died\"\n",
    "s2 = \"Henry Ford built factories to facilitate the construction of the Ford automobile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('tried', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('ford', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('river', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('my', 'PRP$'),\n",
       " ('unfortunate', 'NN'),\n",
       " ('oxen', 'NN'),\n",
       " ('died', 'VBD')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Henry', 'NNP'),\n",
       " ('Ford', 'NNP'),\n",
       " ('built', 'VBD'),\n",
       " ('factories', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('facilitate', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('construction', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Ford', 'NNP'),\n",
       " ('automobile', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.tokenize.word_tokenize(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Capitalization, punctuation, etc.\n",
    "There are the obvious features that we had in mind... lorum ipsum est."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def feature_verbs(words, positions):\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    return len( [ i for i in positions if pos_tag[i][1] == 'VB'] )\n",
    "\n",
    "def is_cap(word):\n",
    "    return word[0] == word[0].capitalize()\n",
    "def feature_caps(words, positions):\n",
    "    return len ( [ i for i in positions if is_cap(words[i]) ] )\n",
    "\n",
    "def feature_plural(words, positions):\n",
    "    def is_plural(word):\n",
    "        return re.match( \".*s$\", word )\n",
    "    return len ( [ i for i in positions if is_plural(words[i]) ] )\n",
    "\n",
    "## N.B. The nltk word tokenizer will tokenize \"Apple's\" as [\"Apple\", \"'s\"]\n",
    "def feature_posessive(words, positions):\n",
    "    l = len(words)\n",
    "    return len ( [ i for i in positions if i+1 < l and words[i+1]==\"'s\" ] )\n",
    "\n",
    "def ad_hoc_features(keyword, strs):\n",
    "    \"\"\"\n",
    "    Given a keyword (e.g., \"apple\") and a list of strings;\n",
    "    Returns a numpy ndarray encoding several ad hoc features of the string that are local\n",
    "    near occurances of the keyword:\n",
    "        - If the keyword is capitalized\n",
    "        - If it is plural (in the stupid sense of ending in s.. good enough for 'apple')\n",
    "        - If it is possessive in the stupid sense of being followed by 's)\n",
    "        - If the keyword is a verb (e.g., for Ford vs ford)\n",
    "    \"\"\"\n",
    "    stemmed_word = stemmer.stem(keyword)\n",
    "    def feature_one(s):\n",
    "        words = nltk.tokenize.word_tokenize(s)\n",
    "        hits = [ i for i in range(len(words)) if stemmer.stem(words[i]) == stemmed_word ]\n",
    "        return np.array([ feature_caps(words, hits), \n",
    "                          feature_plural(words, hits), \n",
    "                          feature_posessive(words, hits), \n",
    "                          feature_verbs(words, hits) ])\n",
    "    return np.asarray(map(feature_one, strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 1, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_hoc_features(\"ford\", [\"I drive a Ford.\", \"I tried to ford the river.\", \"That's not Ford's.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [1, 1, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_hoc_features(\"apple\", [\"Have you eaten your apple?\", \"How is Apple's stock doing?\", \"Apples are tasty.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual application\n",
    "Diclaimer: This version is actually pretty bad -- it uses many of the right ideas, but puts them together pretty poorly (and with fairly little available data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "import nltk.tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def wikipedia_to_sents(url):\n",
    "    \"\"\"\n",
    "    Retrieves a URL from wikipedia, and returns a list of sentences (of at least 3 words) in the body text.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url)).find(attrs={'id':'mw-content-text'})\n",
    "    \n",
    "    # The text is litered by references like [n].  Drop them.\n",
    "    def drop_refs(s):\n",
    "        return ''.join( re.split('\\[\\d+\\]', s) )\n",
    "    \n",
    "    paragraphs = [drop_refs(p.text) for p in soup.find_all('p')]\n",
    "    \n",
    "    raw_sents = reduce(lambda x, y: x + y, [nltk.tokenize.sent_tokenize(p.strip()) for p in paragraphs if p.strip()!=''])\n",
    "    return filter(lambda s: len(s.split(\" \"))>2, raw_sents)\n",
    "\n",
    "\n",
    "#### Bag-of-words features, using tf-idf\n",
    "def make_ng_stem_vectorizer(texts, extra_stop_words, max_features):\n",
    "    \"\"\"\n",
    "    Given \n",
    "        - a list of texts (\"documents\");\n",
    "        - a list of extra stop words (in addition to the standard NLTK English ones); and,\n",
    "        - a number of features to remember\n",
    "    Returns the tf-idf feature extractor with this number of features, based on these documents.\n",
    "    \"\"\"\n",
    "    default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    def tokenize_stem(text):\n",
    "        return map(stemmer.stem, default_tokenizer(text))\n",
    "    ng_stem_tfidf=TfidfVectorizer(#max_features=max_features, \n",
    "                             ngram_range=(1,2),\n",
    "                             stop_words=map(stemmer.stem, nltk.corpus.stopwords.words('english') + extra_stop_words),\n",
    "                             tokenizer = tokenize_stem)\n",
    "    return ng_stem_tfidf.fit( texts )\n",
    "\n",
    "\n",
    "#### Ad hoc features\n",
    "def feature_verbs(words, positions):\n",
    "    pos_tag = nltk.pos_tag(words)\n",
    "    return len( [ i for i in positions if pos_tag[i][1] == 'VB'] )\n",
    "\n",
    "def is_cap(word):\n",
    "    return word[0] == word[0].capitalize()\n",
    "def feature_caps(words, positions):\n",
    "    return len ( [ i for i in positions if is_cap(words[i]) ] )\n",
    "\n",
    "def feature_plural(words, positions):\n",
    "    def is_plural(word):\n",
    "        return re.match( \".*s$\", word )\n",
    "    return len ( [ i for i in positions if is_plural(words[i]) ] )\n",
    "\n",
    "## N.B. The nltk word tokenizer will tokenize \"Apple's\" as [\"Apple\", \"'s\"]\n",
    "def feature_posessive(words, positions):\n",
    "    l = len(words)\n",
    "    return len ( [ i for i in positions if i+1 < l and words[i+1]==\"'s\" ] )\n",
    "\n",
    "def ad_hoc_features(keyword, strs, use_verbs=False):\n",
    "    \"\"\"\n",
    "    Given a keyword (e.g., \"apple\") and a list of strings;\n",
    "    Returns a numpy ndarray encoding several ad hoc features of the string that are local\n",
    "    near occurances of the keyword:\n",
    "        - If the keyword is capitalized\n",
    "        - If it is plural (in the stupid sense of ending in s.. good enough for 'apple')\n",
    "        - If it is possessive in the stupid sense of being followed by 's)\n",
    "        - If the keyword is a verb (e.g., for Ford vs ford)\n",
    "    \"\"\"\n",
    "    stemmed_word = stemmer.stem(keyword)\n",
    "    def feature_one(s):\n",
    "        words = nltk.tokenize.word_tokenize(s)\n",
    "        hits = [ i for i in range(len(words)) if stemmer.stem(words[i]) == stemmed_word ]\n",
    "        ret_list = [ feature_caps(words, hits), \n",
    "                     feature_plural(words, hits), \n",
    "                     feature_posessive(words, hits) ]\n",
    "        if use_verbs:  # This is slow, so only use it sometimes\n",
    "            ret_list.append( feature_verbs(words, hits)  )\n",
    "        return np.array(ret_list)\n",
    "    return np.asarray(map(feature_one, strs))\n",
    "\n",
    "####\n",
    "def make_classifier(base_word, meaning1, meaning2, use_verbs=False):\n",
    "    \"\"\"\n",
    "    Given\n",
    "        - a base word (e.g., \"apple\", \"ford\") that can have ambiguous meaning\n",
    "        - a pair meaning1 = (name1, url1) of a label for the first meaning, and a Wikipedia URL for it\n",
    "        - a pair meaning2 = ... for the other meaning\n",
    "    Returns a tuple (make_features, classifier) where\n",
    "        - make_features is a function taking in a string text, and returns a feature vector\n",
    "        - classifier takes in a feature vector (output by make_features) and predicts the meaning\n",
    "    \"\"\"\n",
    "    name1, url1 = meaning1\n",
    "    name2, url2 = meaning2\n",
    "    sents1 = wikipedia_to_sents(url1)\n",
    "    sents2 = wikipedia_to_sents(url2)\n",
    "    tfidf_vect = make_ng_stem_vectorizer(sents1 + sents2,\n",
    "                                        [base_word],\n",
    "                                        100000)\n",
    "    def make_features(sents, use_verbs=False):\n",
    "        a = ad_hoc_features(base_word, sents, use_verbs)\n",
    "        t = tfidf_vect.transform(sents).toarray()\n",
    "        return np.hstack((a,t))\n",
    "\n",
    "    # Build the training data\n",
    "    train_feat = make_features(sents1 + sents2, use_verbs)\n",
    "    train_res  = np.array( [0] * len(sents1) + [1] * len(sents2) )\n",
    "    \n",
    "    classifier = MultinomialNB()\n",
    "    classifier = classifier.fit(train_feat, train_res)\n",
    "    return (lambda x: make_features(x, use_verbs), classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fruit', 'company', 'company', 'company', 'fruit', 'fruit', 'company', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "#### Now we actually run our code for Apple\n",
    "base_word=\"apple\"\n",
    "options = [ (\"fruit\", \"http://en.wikipedia.org/wiki/Apple\"),\n",
    "            (\"company\", \"http://en.wikipedia.org/wiki/Apple_Inc.\") ]\n",
    "(make_features, classifier) = make_classifier(\"apple\", *options)\n",
    "print map(lambda x: options[x][0], classifier.predict(make_features([\n",
    "    \"I'm baking a pie with my granny smith apples.\",\n",
    "    \"I looked up the recipe on my Apple iPhone.\",\n",
    "    \"The apple pie recipe is on my desk.\",\n",
    "    \"How is Apple's stock doing?\",\n",
    "    \"I'm drinking apple juice.\",\n",
    "    \"I have three apples.\",\n",
    "    \"Steve Jobs is the CEO of apple.\",\n",
    "    \"Steve Jobs likes to eat apples.\"\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['software', 'building', 'software', 'building', 'software', 'software']\n"
     ]
    }
   ],
   "source": [
    "#### Now we actually run our code for Apple\n",
    "base_word=\"windows\"\n",
    "options = [ (\"building\", \"http://en.wikipedia.org/wiki/Window\"),\n",
    "            (\"software\", \"http://en.wikipedia.org/wiki/Microsoft_Windows\") ]\n",
    "(make_features, classifier) = make_classifier(\"apple\", use_verbs=True, *options)\n",
    "print map(lambda x: options[x][0], classifier.predict(make_features([\n",
    "    \"Bill Gates was involved with Windows.\",\n",
    "    \"Could you open the window?\",\n",
    "    \"The 'broken window' theory related broken windows to increases in crime rate.\",\n",
    "    \"The windows are all made of shatter-proof glass.\",\n",
    "    \"Could you install windows on your computer?\",\n",
    "    \"Could you install windows on your house?\"\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company', 'company', 'company', 'company']\n"
     ]
    }
   ],
   "source": [
    "#### Now we actually run our code for Ford\n",
    "base_word=\"ford\"\n",
    "options = [ (\"crossing\", \"http://en.wikipedia.org/wiki/Ford_(crossing)\"),\n",
    "            (\"company\", \"http://en.wikipedia.org/wiki/Ford\") ]\n",
    "(make_features, classifier) = make_classifier(\"apple\", *options)\n",
    "print map(lambda x: options[x][0], classifier.predict(make_features([\n",
    "    \"I tried to ford the river and my unfortunate oxen died.\",\n",
    "    \"Ford makes cars, though their quality is sometimes in dispute.\",\n",
    "    \"The Ford Mustang is an iconic automobile.\",\n",
    "    \"The river crossing was shallow, but we could not ford it.\"\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
