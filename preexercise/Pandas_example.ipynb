{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e., when everything fits into memory.\n",
    "\n",
    "The basic new \"noun\" in pandas is the **data frame**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What is a data frame\n",
    "\n",
    "It's like a table, with rows and columns (e.g., as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. (Like in SQL, columns are homogeneous.)\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "We'll go through a little on all of these in the context of an example.  To go through it, you must have the (output) data files from the HMDA \"Project structure\" example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to explore a dataset of mortgage insurance issued by the Federal Housing Authority (FHA).  The data is broken down by census tract and tells us how big of a player the FHA is in each tract (how many homes etc ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading data (and basic statistics / visualization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../small_data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df['GEOID'] = df['Census_Tract_Number']*100 + 10**6 * df['County_Code'] \\\n",
    "    + 10**9 * df['State_Code']   ## A computed field!\n",
    "    \n",
    "df = df.sort('State_Code')  # sorting data to make it easier to read\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic statistics and a histogram of the percentage of mortages \n",
    "# in each census tract insured by FHA\n",
    "print df['PCT_AMT_FHA'].describe() \n",
    "df['PCT_AMT_FHA'].hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The above distribution looks a little skewed, let's look at it's log\n",
    "\n",
    "# We can save off the data into a new column\n",
    "df['LOG_AMT_ALL'] = np.log(df['AMT_ALL'])\n",
    "print df['LOG_AMT_ALL'].describe()\n",
    "\n",
    "# We can use the apply function to transform data\n",
    "df['AMT_ALL'].apply(np.log).hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selecting off a column\n",
    "print df['State_Code'].head()\n",
    "\n",
    "# Selecting off multiple columns\n",
    "print df[['State_Code', 'County_Code']].head()\n",
    "\n",
    "# programatically access column names\n",
    "print [col for col in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Filtering\n",
    "\n",
    "This is a slightly fancied up version of Python's index notation.  When you write something like \n",
    "\n",
    "        df['column_name']==5\n",
    "\n",
    "what's actually happening is that pandas creates a new, __binary__, data series indexed by the same indexing set as `df`.  You can combine such binary series using the term-wise `&` (`and`) and term-wise `|` (`or`) operations.  For instance:\n",
    "\n",
    "        df['column_name2']==MD & ( df['column_name1']==5 | df['column_name2']==6 )\n",
    "\n",
    "Now the `df[...]` notation is very flexible:\n",
    "  - It accepts column names;\n",
    "  - It accepts column numbers (so long as there is no ambiguity with column names..);\n",
    "  - It accepsts _binary data series!_\n",
    "  \n",
    "This means that you can write\n",
    "\n",
    "        df[ df['column_name2']==MD & ( df['column_name1']==5 | df['column_name1']==6 ) ]\n",
    "   \n",
    "for what you would write in SQL as\n",
    "\n",
    ">         SELECT * FROM df WHERE\n",
    "            column_name2='MD\" AND (column_name1=5 OR column_name1=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selection returns a boolean array ..\n",
    "print (df['State_Code'] == 1).head()\n",
    "\n",
    "# ... we can apply the usual boolean operators to it\n",
    "print ((df['State_Code'] == 1) & (df['Census_Tract_Number'] == 9613)).head()\n",
    "\n",
    "# pandas indices take boolean lists of the appropriate length\n",
    "print ((df['State_Code'] == 1) & (df['Census_Tract_Number'] == 9613)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Join-ing\n",
    "\n",
    "The analogue of a\n",
    "\n",
    ">             \n",
    "    SELECT * \n",
    "        FROM df1\n",
    "        INNER JOIN df2 \n",
    "        ON df1.field_name=df2.field_name;\n",
    "\n",
    "is\n",
    "\n",
    "    df_joined = df1.merge(df2, on='field_name')\n",
    "\n",
    "You can also do left / right / outer joins, mix-and-match column names, etc.  For that consult the Pandas documentation. (The example below will do a left join..)\n",
    "\n",
    "Of course, just looking at the distribution of insurance by census tract isn't interesting unless we know more about the census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading information about census tracts\n",
    "df_geo = pd.read_csv('../small_data/2013_Gaz_tracts_national.tsv', sep='\\t')\n",
    "\n",
    "# And join it in\n",
    "df_joined = df.merge(df_geo, on='GEOID', how='left')\n",
    "df_joined.sort('AMT_ALL', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "The analog of SQL's `GROUP BY` is\n",
    "\n",
    "    grouped = df.groupby(['field_name1', ...])\n",
    "\n",
    "The above is analogous to\n",
    ">             \n",
    "    SELECT * \n",
    "        FROM df\n",
    "        GROUP BY df.field_name1, ...\n",
    "\n",
    "Pandas is somewhat more flexible in how you can use grouping, not requiring you to specify an aggregation function up front.  A few examples are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This isn't a SQL-style 'GROUP BY'.\n",
    "df_joined.groupby('USPS').first().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the analog of\n",
    "# SELECT USPS, SUM(AMT_FHA), SUM(AMT_ALL), ... FROM df GROUP BY USPS;\n",
    "df_by_state = df_joined[['USPS', 'AMT_FHA', 'AMT_ALL', 'NUM_FHA', 'NUM_ALL']].groupby('USPS').sum()\n",
    "\n",
    "df_by_state['PCT_AMT_FHA'] = 100.0 * df_by_state['AMT_FHA']  / df_by_state['AMT_ALL']\n",
    "df_by_state['PCT_NUM_FHA'] = 100.0 * df_by_state['NUM_FHA']  / df_by_state['NUM_ALL']\n",
    "\n",
    "# This sure looks different than the census-tract level histogram!\n",
    "df_by_state['PCT_AMT_FHA'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA handling\n",
    "\n",
    "\n",
    "When you read in a CSV file / SQL data base there are often \"NA\" (or \"null\", \"None\", etc.) values.  The CSV reader has a special field for specifying how this is denoted, and SQL has the built-in notion of NULL.  Pandas provides some tools for working with these -- they are generally similar too (and a little bit worse than) `R`\n",
    "\n",
    "- `isnull` / `notnull`: Testing for null-ness e.g., \n",
    ">       \n",
    "        df['column_name'].isnull()\n",
    "        \n",
    "   returns a Boolean series\n",
    "- `fillna`: Replacing null values by something else, e.g.,\n",
    ">         \n",
    "        df['column_name'].fillna(0)             # Fills constant value, here 0\n",
    "        df['column_name'].fillna(method='ffill')  # Fill forwards\n",
    "        df['column_name'].fillna(method='bfill', limit=5)  # Fill backwards, at most 5\n",
    "        \n",
    "    At least by default, this is *not in place* -- that is, it creates a new series and does not change the original one.\n",
    "\n",
    "- `interpolate`: Replacing null values by (linear, or quadratic, or...) interpolation.  There is support for indexing by times (not necessarily equally spaced), etc. in the documentation.  The most basic usage is\n",
    ">        \n",
    "        df['column_name'].interpolate()\n",
    "    \n",
    "    As above, this is not in place.\n",
    "\n",
    "\n",
    "For more details: http://pandas.pydata.org/pandas-docs/stable/missing_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas HTML data import example\n",
    "================================\n",
    "Pandas takes a \"batteries included approach\" and throws in a whole lot of convenience functions.  For instance it has import functions for a variety of formats.  One of the pleasant surprises is a command `read_html` that's meant to automate the process of extacting tabular data from HTML.  In particular, it works pretty well with tables on Wikipedia.  \n",
    "\n",
    "Let's do an example: We'll try to extract the list of the world's tallest buildings from\n",
    "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world\n",
    "\n",
    "\n",
    "(This example will, likely, not get lecture time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = pd.read_html('http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world')\n",
    "\n",
    "# There are several tables on the page.  By inspection we can figure out which one we want\n",
    "tallest = dfs[2]  \n",
    "\n",
    "# The header appeared as the first row.  We can get at it via tallest[0:1].values, copy-paste-fix and get the next line:\n",
    "tallest.columns=['Category', 'Structure', 'Country', 'City', 'Height (metres)', 'Height (ft)', 'Year built', 'Coordinates']\n",
    "tallest = tallest[1:]\n",
    "\n",
    "# The coordinates column needs to be fixed up.  This is a bit of string parsing:\n",
    "def clean_lat_long(s):\n",
    "    try:\n",
    "        parts = s.split(\"/\")\n",
    "    except AttributeError:\n",
    "        return (None, None)\n",
    "    if len(parts)<3:\n",
    "        return None\n",
    "    m=re.search(r\"(\\d+[.]\\d+);[^\\d]*(\\d+[.]\\d+)[^\\d]\", parts[2])\n",
    "    if not m:\n",
    "        return (None, None)\n",
    "    return (m.group(1), m.group(2))\n",
    "\n",
    "tallest['Coordinates'] = tallest['Coordinates'].apply(clean_lat_long)\n",
    "tallest['Latitude'] = tallest['Coordinates'].apply(lambda x:x[0])\n",
    "tallest['Longitude'] = tallest['Coordinates'].apply(lambda x:x[1])\n",
    "\n",
    "tallest.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
