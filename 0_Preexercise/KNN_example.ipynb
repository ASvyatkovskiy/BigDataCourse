{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "\n",
    "In this notebook we'll explore the K Nearest Neighbors algorithm.  As the name implies, the prediction for a new point is based on the closest $k$ points in the training set (where closest is usually defined in terms of Euclidiean distance on the $p$-dimensional feature space).  In math terms, we're taking the Euclidean distance between the $p$-vectors $X_{j\\cdot}$ as our metric.  As a classificaiton algorithm, it takes a majority vote of the nearest $k$ neighbors.  As a regression algorithm, it takes the average label of the nearest $k$ neighbors.\n",
    "\n",
    "## Some resources\n",
    "\n",
    "1. [Wikipedia](http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "1. The Scikit Learn package implements [K Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html).  It has both a [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [`KNeighborsRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and display a sample of the iris dataset\n",
    "\n",
    "from sklearn import neighbors, datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "# The iris data set labels 3 flower types (y) by 4 different attributes of the flower (X).\n",
    "# For more about the attributes, see http://mldata.org/repository/data/viewslug/datasets-uci-iris/\n",
    "data = datasets.load_iris()\n",
    "X = pd.DataFrame(data.data, columns=[\"sepal_length\", \"sepal_width\", \"petal_legnth\", \"petal_width\"])\n",
    "y = pd.Series(data.target)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process the nearest neighbors\n",
    "nearest_neighbors = neighbors.NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
    "nearest_neighbors.fit(X)\n",
    "distances, indices = nearest_neighbors.kneighbors(X)\n",
    "\n",
    "# Column $k$ gives the distances between each point and its $k$-th nearest neighbor from 0 to 4\n",
    "# Column 0 is always itself.\n",
    "# Each row is a separate point (note the first column)\n",
    "print \"Distance\"\n",
    "print pd.DataFrame(distances).head()\n",
    "print \"Indices\"\n",
    "print pd.DataFrame(indices).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We illustrate the effect of the number of neigbhors used on accruacy.\n",
    "\n",
    "from sklearn import neighbors, cross_validation, grid_search\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(y), n_iter=20, test_size=0.2, random_state=42)\n",
    "param_grid = { \"n_neighbors\": range(4, 24, 4) }\n",
    "nearest_neighbors_cv = grid_search.GridSearchCV(neighbors.KNeighborsClassifier(), \n",
    "                                                param_grid=param_grid, cv=cv, \n",
    "                                                scoring='accuracy')\n",
    "\n",
    "nearest_neighbors_cv.fit(X, y)\n",
    "cv_accuracy = pd.DataFrame.from_records(\n",
    "    [(score.parameters['n_neighbors'],\n",
    "      score.mean_validation_score)\n",
    "     for score in nearest_neighbors_cv.grid_scores_],\n",
    "columns=['n_neighbors', 'accuracy'])\n",
    "\n",
    "plt.plot(cv_accuracy.n_neighbors, cv_accuracy.accuracy)\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Data.\n",
    "\n",
    "Based on your answer to the last question, we will want to scale the individual features so that they have similar variance.  For each feature, we are going to subtract the mean and divide by the standard deviation:\n",
    "\n",
    "$$ X_{ji}' = \\frac{X_{ji} - \\mu_i}{\\sigma_i} $$\n",
    "\n",
    "where $\\mu_i$ is the mean of the $i$-th column (or feature) and $\\sigma_i$ is the standard deviation.\n",
    "\n",
    "**Question**: For $k$-Nearest-Neighbors, was subtracting the mean necessary?  For what other algorithms might subtracting the mean help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# first, let's see what the standard deviations are.\n",
    "scaler = preprocessing.StandardScaler(copy=True).fit(X)\n",
    "pd.DataFrame({\n",
    "    \"Mean\": scaler.mean_,\n",
    "    \"Std\": scaler.std_\n",
    "}, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_copy = X.copy()  # Feature (bug?): transform modifies the original \n",
    "X_scaled = scaler.transform(X_copy)\n",
    "pd.DataFrame({\n",
    "    \"Mean\": X_scaled.mean(axis=0), \n",
    "    \"Std\": X_scaled.std(axis=0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are going to first scale and then apply KNeighbors.\n",
    "# Pipeline allows us to chain together multiple transformers and then a regressor or classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaled_nearest_neighbors = Pipeline([('scaling', preprocessing.StandardScaler(copy=True)), \n",
    "                                     ('neighbors', neighbors.KNeighborsClassifier())])\n",
    "\n",
    "param_grid = { \"neighbors__n_neighbors\": range(4, 24, 4) }    # parameters to Pipeline take the form [label]__[estimator_param]\n",
    "scaled_nearest_neighbors_cv = grid_search.GridSearchCV(scaled_nearest_neighbors, \n",
    "                                                       param_grid=param_grid, cv=cv, \n",
    "                                                       scoring='accuracy')\n",
    "\n",
    "scaled_nearest_neighbors_cv.fit(X, y)\n",
    "scaled_cv_accuracy = pd.DataFrame.from_records(\n",
    "    [(score.parameters['neighbors__n_neighbors'],\n",
    "      score.mean_validation_score)\n",
    "     for score in scaled_nearest_neighbors_cv.grid_scores_],\n",
    "columns=['n_neighbors', 'accuracy'])\n",
    "\n",
    "plt.plot(scaled_cv_accuracy.n_neighbors, scaled_cv_accuracy.accuracy, label=\"scaled_cv_accuracy\")\n",
    "plt.plot(cv_accuracy.n_neighbors, cv_accuracy.accuracy, label=\"cv_accuracy\")\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower center')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
