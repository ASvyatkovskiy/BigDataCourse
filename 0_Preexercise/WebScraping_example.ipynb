{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another topicthat will be touched during the Big Data course is \"scraping\": getting unstructured data and turning it into something usable.  The tools available through Python are mature and easy to use.  We'll focus primarily on _web scraping_, where the source data comes in HTML form.\n",
    "\n",
    "The basic workflow is:\n",
    "\n",
    "1.  Find the data you want on the web.\n",
    "2.  Inspect the page you're dealing with, to figure out how to zoom-in towards the content you want.  This will involve some combiation of\n",
    "    - Looking at the source code of the page (especially if it is simple), and\n",
    "    - Figuring out the structure of the HTML parse tree.  This step is much easier with a something like __Chrome Developer Tools__.\n",
    "3.  Write code to get out what you want:\n",
    "    - If the page is very simple, treat it as a bunch of text => __string manipulation / regular expressions__ in Python.\n",
    "    - If the page is more complicated (and/or written in good style), we want to use the HTML parse tree => __BeautifulSoup__ in Python.\n",
    "4.  Make sure it worked!\n",
    "5.  If your crawling problem is at all non-trivial, you will now have to go back to Step 2 to zoom in further -- or you'll have parsed the URL of a link you want to follow, in which case you'll go back to Step 1 to figure out how to parse what you want from the new target page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "As an example, suppose we want to crawl the list of \"Available Technologies\" being licensed by MIT at http://technology.mit.edu and store their basic info; their associated patents; and the reference counts on their associated patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Let's go to that URL.\n",
    "\n",
    "- _First try_:  A list of links on the right.  Let's click on a few -- what do we see?  Many are empty, the categories are not obviously mutually exclusive, okay.  Maybe there's a better way.\n",
    "- _Second try_: Let's just search for all technologies at http://technology.mit.edu/technologies.  Okay, better but it only gives us 50 at a time.  We could just combine the four pages, that's fine.  Let's just click on page 2 to see what happens\n",
    "- _Third try_: The URL for page 2 is http://technology.mit.edu/technologies?limit=50&offset=50&query=.  That looks like we can just specify a higher limit and offset 0 and get the whole thing.\n",
    "- _Final answer_: Indeed, http://technology.mit.edu/technologies?limit=1000 has a giant list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "\n",
    "url = \"http://technology.mit.edu/technologies?limit=1000\"\n",
    "raw_page = urllib2.urlopen(url).read()\n",
    "print(raw_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A quick introduction to HTML and the DOM**\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Pull up http://technology.mit.edu/technologies?limit=1000 in Chrome.  \n",
    "- Open __View->Developer->Developer Tools__.  \n",
    "- Right click on one of the technology titles, and choose __\"Inspect Element\"__.\n",
    "\n",
    "What are we looking at?  Well.. it's this is the structure of the webpage.  Nested _tags_ of different _types_ and having a variety of _attributes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: What we learned above:\n",
    "\n",
    "  - All of the technologies are underneath (\"_descendents of_\")   `<div class=\"search\" id=\"nouvant-portfolio-content\">`\n",
    "  - In fact, each of them is in its own `<div class=\"technology\" data-images=\"true\" id=\"technology_XXXX\">`\n",
    "  \n",
    "Now we're ready to move on to **Step 3**: We'll use BeautifulSoup to leverage the above to zoom in on the individual technologies and to get links to the pages with detailed info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(raw_page)\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent_div = soup.find('div', attrs={'id': 'nouvant-portfolio-content'}) #Find (at most) *one*\n",
    "tech_divs = parent_div.find_all('div', attrs={'class':'technology'})  #Find *all*\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Introduction to CSS selectors**\n",
    "\n",
    "This pattern -- where you have nested finds, each given by conditions on tag type, id, and class -- is very common.  It's so common that there is a special convenience language for such traversals: [CSS selectors](http://www.w3schools.com/cssref/css_selectors.asp).\n",
    "\n",
    "BeautifulSoup supports a form of CSS selectors, and this will let us write the above in a more concise and expressive way:\n",
    "    >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "\n",
    "All selectors work like a 'find_all'.  Some basic building examples of selectors are:\n",
    "\n",
    " - _'mytag'_ picks out all tags of type _mytag_.\n",
    " - _'#myid'_ picks out all tags whose _id_ is equal to _myid_\n",
    " - _'.myclass'_ picks out all tags whose _class_ is equal to _myclass_\n",
    " - _'mytag#myid'_ will pick all tags of type _mytag_ **and** _id_ equal to _myid_ (analgously for _'mytag.myclass'_)\n",
    " - If _'selector1'_ and _'selector2'_ are two selectors, then there is another selector '_selector1 selector2'_.  It picks out all tags satisfying _selector2_ that are __descendents__(*) of something satisfying _selector1_, i.e., it's like our nested find.\n",
    " \n",
    " (*) It doesn't have to be a _direct_ descedent.  I.e., it can be a grand-grand-..-grand-child of something satisfying _selector1_.  For direct descendents we'd instead write _'selector1 > selector2'_\n",
    " \n",
    "Let's just explain how this applies to our example:\n",
    "\n",
    "1.  Let's start with the first half\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "This picks out all 'div' tags with id 'nouvant-portfolio-content'.\n",
    "2.  Then the second half\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                                                            ^^^^^^^^^^^^^^\n",
    "This picks out all 'div' tags with class 'technology'.\n",
    "3.  Finally the whole thing\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "does exactly the same as our nested find above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tech_divs = soup.select('div#nouvant-portfolio-content div.technology')\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what we've zoomed in to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tech_divs[0].prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to pull out some key pieces of info:\n",
    "\n",
    "- The technology's \"title\" (the text in the `<a>` element)\n",
    "- The link to follow for more info on the technology (the _href_ attribute of the `<a>`)\n",
    "- And a short blurb about the text (in the `<span>`)\n",
    "\n",
    "Let's write some code to extract this.  But before we do, let's discuss what _form_ the output should take: It is often convenient to store data in _key-value_ form (e.g., as a hashtable), in other words to name the bits of data you are collecting.  One big advantage is that this makes it easier to add in extra fields progrssively.\n",
    "\n",
    "Let's see what the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firsta = tech_divs[0].find('a')\n",
    "print firsta.text\n",
    "print firsta['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "# We're going to use a \"named tuple\" to store our key-value data.\n",
    "# We could also have used a dictionary, with strings as keys.\n",
    "# Named tuples have some advantages\n",
    "#  - Better notation, x.field_name instead of x['field_name']\n",
    "#  - If you change your object structure later and fail to update your\n",
    "#    code to include the new fields, this will make it easier to find.\n",
    "#  - They are immutable, preventing certain sorts of bugs.\n",
    "# .. and some disadvantages:\n",
    "#  - If you want to augment object structure you need a new type\n",
    "#    (or to go back and fill your code )\n",
    "#  - They are immutable.\n",
    "##\n",
    "from collections import namedtuple\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "\n",
    "def td_info(td):\n",
    "    la = td.select('h2 > a')\n",
    "    ls = td.select('span')\n",
    "    if len(la)!=1 or len(ls)!=1:\n",
    "        print \"Uh oh! We did something wrong\"\n",
    "        return None\n",
    "    return TechBasic (\n",
    "            title = la[0].text,\n",
    "            url   = la[0]['href'],\n",
    "            short = ls[0].text\n",
    "            )\n",
    "tech_links=[td_info(td) for td in tech_divs]\n",
    "\n",
    "print tech_links[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Patent = namedtuple('Patent', 'name url')\n",
    "TechDetailed = namedtuple('TechDetailed', 'tech_basic, patents')\n",
    "def get_tech_details(tech_basic):\n",
    "    url_base=\"http://technology.mit.edu/\"\n",
    "    soup = BeautifulSoup( urllib2.urlopen(url_base + tech_basic.url) )\n",
    "    def patent_info(a):\n",
    "       return Patent ( \n",
    "                name = a.text, \n",
    "                url = a['href'] \n",
    "                )\n",
    "    patents = [patent_info(a) for a in soup.select('dd.us_patent_issued a')]\n",
    "    return TechDetailed ( \n",
    "            tech_basic = tech_basic, \n",
    "            patents = patents \n",
    "            )\n",
    "\n",
    "tech_basics = map(get_tech_details, tech_links[0:2])  #This takes a list\n",
    "print tech_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "In the last code segment, we only did the first one.  If we try to get them all this way, it'll take a while.  Run the next cell for as long (or not) as you wish, and when you get bored use _Kernel->Interrupt_ to stop it.\n",
    "\n",
    "The problem is of course that it takes a while to connect to the remote server and fetch the page.  Fortunately, thought it takes a long time it is not actually _computationally expensive_: your computer would be perfectly happy doing this for 20 pages at a time.  The **multiprocessing** package in Python makes it easy to do this kind of (easy) parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slow version -- takes about 2 minutes to complete\n",
    "# Uncomment and run it to see\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# tech_details = map(get_tech_details, tech_links)  #This takes a list\n",
    "# end_time = time.time()\n",
    "\n",
    "# print \"Done!\", end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multi-processor version -- takes a few seconds to complete\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "workers = Pool(30)  # 30 worker processes\n",
    "\n",
    "start_time = time.time()\n",
    "tech_details = workers.map(get_tech_details, tech_links)\n",
    "end_time = time.time()\n",
    "\n",
    "print \"Done!\", end_time-start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
