{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different ML classifiers: an overview\n",
    "\n",
    "Following is an overview of some of the popular machine learnign methods (classical methods only here, no deep neural networks).\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "A decision tree is a binary tree.  At each of the internal nodes, it chooses a feature $i$ and a threshold $t$.  Each leaf has a value.  Evaluation of the model is just traversal of the tree from the root.  At each node, for example $j$, we go down the left branch if $X_{ji} \\le t$ and the right branch otherwise.  The value of the model $f(X_{ji})$ is the value at the value at the terminating leaf of this traveral.  Below, we show a picture of this on small decision tree trained on the iris data set.  Notice that each internal node has a decision criterion and each leaf has the breakdown of label classes left at this leaf of the tree.  \n",
    "\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "A random forest is just an ensemble of decision trees.  The predicted value is just the average of the trees (for both regression and classification problems - for classification problems, it is the probabilities that are averaged).  You can adjust `n_estimators` to change the number of trees in the forest.  If each tree is trained on the same subset of data, why aren't they identical?  Two reasons:\n",
    "1. **Subsampling**: each tree is actually trained on a random selected (with replacement) subset (i.e. bootstrap)\n",
    "1. **Maximum Features**: the optimal split comes from a randomly selected subset of the features.  In scikit-learn, this feature is controlled by `max_features`.\n",
    "\n",
    "### Random Forest Training Algorithm and Tuning Parameters\n",
    "\n",
    "A Random Forest is pretty straightforward to train once you know how a Decision Tree works.  In fact, their construction can even be parallelized.  \n",
    "\n",
    "Below, various parameters that affect decision tree and random forest training are discussed. \n",
    "\n",
    "The first two parameters we mention are the most important, and tuning them can often improve performance:\n",
    "\n",
    "**numTrees**: Number of trees in the forest.\n",
    "\n",
    "Increasing the number of trees will decrease the variance in predictions, improving the modelâ€™s test-time accuracy.\n",
    "Training time increases roughly linearly in the number of trees.\n",
    "\n",
    "\n",
    "**maxDepth**: Maximum depth of each tree in the forest.\n",
    "Increasing the depth makes the model more expressive and powerful. However, deep trees take longer to train and are also more prone to overfitting.\n",
    "In general, it is acceptable to train deeper trees when using random forests than when using a single decision tree. One tree is more likely to overfit than a random forest (because of the variance reduction from averaging multiple trees in the forest).\n",
    "\n",
    "The next two parameters generally do not require tuning. However, they can be tuned to speed up training.\n",
    "\n",
    "**subsamplingRate**: This parameter specifies the size of the dataset used for training each tree in the forest, as a fraction of the size of the original dataset. The default (1.0) is recommended, but decreasing this fraction can speed up training.\n",
    "\n",
    "**featureSubsetStrategy**: Number of features to use as candidates for splitting at each tree node. The number is specified as a fraction or function of the total number of features. Decreasing this number will speed up training, but can sometimes impact performance if too low.\n",
    "\n",
    "## Linear SVM\n",
    "\n",
    "The canonical Support Vector Machine is the linear one.  Assume we have two groups labeled by $y = \\pm 1$.  Then we are trying to find the line $\\beta$ such that $X \\beta + \\beta_0$ maximially separates the points in our two classes:\n",
    "\n",
    "If the two classes can be separated by a linear hyperplane (picture on the left), we want to maximize the **margin** $M$ of the **boundary region**.  A little bit of math can show us that finding the largest separation is actually solved by the minimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge 1 \\quad \\mbox{for } j = 1,\\ldots,N\n",
    "$$\n",
    "\n",
    "The picture and the equation are equivalent: in the picture we are setting the margin to be $M$ and finding the largest margin possible.  In the equation, we are setting the margin to be $1$ and finding the smallest $\\beta$ that will make that true.  So $\\beta$ and $M$ are related through $\\| \\beta \\| = \\frac{1}{M}$.  If the two classes cannot be separated (picture on the right), we will have to add a forgiveness terms $\\xi$,\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j) & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\sum_j \\xi_j \\le C\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "for some constant $C$.  The constant $C$ is an important tradeoff.  It corresponds to the total \"forgiveness budget\" (see the last constraint).  The larger $C$, the forgiveness we have and the wider the margin $M$ can be.  We can rewrite the constrained optimization problem as the primal Lagrangian function with Lagrange multipliers $\\alpha_j \\ge 0$, $\\mu_j \\ge 0$, and $\\gamma \\ge 0$,  for each of our three constraints:\n",
    "\n",
    "$$ L_P(\\gamma) = \\min_{\\beta, \\beta_0, \\xi} \\max_{\\alpha, \\mu} \\frac{1}{2} \\| \\beta \\|^2 - \\sum_j \\alpha_j \\left[y_j (X_{j \\cdot} \\cdot \\beta + \\beta_0 - (1-\\xi_j)\\right] - \\sum_j \\mu_j \\xi_j  + \\gamma \\sum_j \\xi_j$$\n",
    "\n",
    "There is a one-to-one correspondence between $\\gamma$ and $C$.  By taking first order conditions, first-order conditions, the dual Lagrangian problem can be formulated as\n",
    "\n",
    "$$\n",
    "L_D(\\gamma) = \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_{j, j'} \\alpha_j \\alpha_{j'} y_j y_{j'} X_{j \\cdot} \\cdot X_{j' \\cdot} \\,. \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    "0 = \\sum_j \\alpha_j y_j \\\\\n",
    "0 \\le \\alpha_j \\le \\gamma & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "This is now a reasonably straightforward quadratic programming problem.  It is solved via [Sequential Minimization Optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization).  Once we have solved this problem for $\\alpha$, we can easily work out the coefficients from\n",
    "\n",
    "$$ \\beta = \\sum_j \\alpha_j y_j X_{j \\cdot} $$\n",
    "\n",
    "**Key takeaways**:\n",
    "1. Critically, only points inside the margin or on the wrong side of the margin ($j$ for which $\\xi_j > 0$) affect the SVM (see the picture).  This is intuitively clear from the picture.  In the dual form, this is because $\\alpha_j$ is the Lagrangian constraint corresponding to $y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j)$ and Complementary Slackness shows tells us that $\\alpha_j > 0$ is non-zero only when the constraint is binding ($y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) = (1-\\xi_j)$), i.e. we're in the boundary region.  This is meaning the **Support Vector** in \"SVM\": only the vectors in the boundary-the **Support Vectors**-contribute to the solution.\n",
    "1. $C$ or $\\gamma$ give a trade-off between the amount of forgiveness and the size of the margin or boundary region.  Hence, it controls how many points affect the SVM (based on the distance from the boundary).\n",
    "\n",
    "Below, we plot out a simple two-class linear SVM on some synthetic data\n",
    "\n",
    "\n",
    "## Non-linear SVM\n",
    "\n",
    "What if we don't believe that our data can be cleanly split by a linear hyperplane?  The common way to incorporate non-linear features is to have a non-linear function $h(X_{j\\cdot})$ (possibly to a higher-dimensional feature space with dimension $p'$ where $p' \\ge p$) and to train on that space.  One intuition is that there's a higher-dimensional space in which the data is has a linear separation and $h$ gives a non-linear mapping into that space.\n",
    "\n",
    "### Kernel Trick\n",
    "\n",
    "The **Kernel Trick** in SVM tells us that rather than directly computing the (potentially very large) vectors $h(X_{j \\cdot})$, we can just modify the Kernel.  If we use the transformed data $h(X_{j \\cdot})$, the dual Lagrangian would be\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot}) $$\n",
    "\n",
    "We can rewrite\n",
    "\n",
    "$$h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot})  = K(X_{j \\cdot}, X_{j' \\cdot})$$ \n",
    "\n",
    "for some non-linear Kernel $K$.  Our problem then becomes,\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} K(X_{j \\cdot}, X_{j' \\cdot}) $$\n",
    "\n",
    "There's a one-to-one correspondence between Kernel functions and functions $h$ (although $h$'s range may be infinite dimensional).  Some common Kernels include\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Kernel</th>\n",
    "<th>$K(x,x')$</th>\n",
    "<th>Scikit `kernel` parameter</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Linear Kernel</td>\n",
    "<td>$x \\cdot x'$</td>\n",
    "<td>`kernel='linear'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$d$-th Degree Polynomial</td>\n",
    "<td>$(r + c x \\cdot x')^d$</td>\n",
    "<td>`kernel='poly'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Radial Kernel</td>\n",
    "<td>$ \\exp(- c \\|x - x' \\|^2) $</td>\n",
    "<td>`kernel='rbf'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Neural Network Kernel</td>\n",
    "<td>$\\tanh(c x \\cdot x' + r)$</td>\n",
    "<td>`kernel='sigmoid'`</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The benefit of using a Kernel is that we don't have to compute a very high-dimensional (possibly infinite-dimensional) $h$.  All that complexity is just wrapped into the kernel $K$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
