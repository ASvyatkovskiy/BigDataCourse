{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational primitives \n",
    "\n",
    "Relational primitives include transformations of the type we use in SQL\n",
    "\n",
    "Structured Query Language (pronounced like \"sequel\")\n",
    "\n",
    "**Key nouns**:\n",
    "    - \"Relational\" database\n",
    "    - Table\n",
    "    - Row\n",
    "    - Column\n",
    "    - (Primary key)\n",
    "\n",
    "**Key verbs**:\n",
    "    - `CREATE TABLE` / `DROP TABLE` to modify the structure of the database\n",
    "    - `SELECT` to read out some rows\n",
    "    - `INSERT` / `UPDATE` / `DELETE` to modify (or delete) some rows of tables\n",
    "\n",
    "**Key modifiers**:\n",
    "    - `WHERE`: impose condition\n",
    "    - `ORDER BY`: sort\n",
    "    - `GROUP BY`: aggregate (e.g., pivot tables in Excel)\n",
    "    - `JOIN`: combining tables in one query\n",
    "  \n",
    "\n",
    "## Pandas data structures and functionality\n",
    "\n",
    "Pandas is Python's answer to R and SQL.  It's a good tool for small(ish) data analysis -- i.e., when everything fits into memory. The basic new \"noun\" in pandas is the **data frame**. As a part of pre-exercises, you have received an iPython notebook with some Pandas case study. \n",
    "  \n",
    "It's like a table, with rows and columns (e.g., as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. (Like in SQL, columns are homogeneous.)\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)  \n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **Grouping** `groupby`\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.  \n",
    " \n",
    " \n",
    "Below, we're going to explore a dataset of mortgage insurance issued by the *Federal Housing Authority (FHA)*. The data is broken down by census tract and tells us how big of a player the FHA is in each tract (how many homes etc ...). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_Code</th>\n",
       "      <th>County_Code</th>\n",
       "      <th>Census_Tract_Number</th>\n",
       "      <th>NUM_ALL</th>\n",
       "      <th>NUM_FHA</th>\n",
       "      <th>PCT_NUM_FHA</th>\n",
       "      <th>AMT_ALL</th>\n",
       "      <th>AMT_FHA</th>\n",
       "      <th>PCT_AMT_FHA</th>\n",
       "      <th>GEOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>9613.0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>2184</td>\n",
       "      <td>799</td>\n",
       "      <td>36.58420</td>\n",
       "      <td>1.049961e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55215</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5000</td>\n",
       "      <td>774</td>\n",
       "      <td>76</td>\n",
       "      <td>9.81912</td>\n",
       "      <td>1.003010e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65492</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>1495</td>\n",
       "      <td>263</td>\n",
       "      <td>17.59200</td>\n",
       "      <td>1.095031e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33750</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9618.0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>21.4286</td>\n",
       "      <td>1243</td>\n",
       "      <td>333</td>\n",
       "      <td>26.79000</td>\n",
       "      <td>1.039962e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       State_Code  County_Code  Census_Tract_Number  NUM_ALL  NUM_FHA  \\\n",
       "23999         1.0         49.0               9613.0       16        4   \n",
       "55215         1.0          3.0                102.0        8        1   \n",
       "65492         1.0         27.0                  NaN        1        0   \n",
       "45193         1.0         95.0                311.0       20        3   \n",
       "33750         1.0         39.0               9618.0       14        3   \n",
       "\n",
       "       PCT_NUM_FHA  AMT_ALL  AMT_FHA  PCT_AMT_FHA         GEOID  \n",
       "23999      25.0000     2184      799     36.58420  1.049961e+09  \n",
       "55215      12.5000      774       76      9.81912  1.003010e+09  \n",
       "65492       0.0000       82        0      0.00000           NaN  \n",
       "45193      15.0000     1495      263     17.59200  1.095031e+09  \n",
       "33750      21.4286     1243      333     26.79000  1.039962e+09  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../0_Preexercise/data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "#create a new column as a function of  a group of columns\n",
    "df['GEOID'] = df['Census_Tract_Number']*100 + 10**6 * df['County_Code'] \\\n",
    "    + 10**9 * df['State_Code']   \n",
    "    \n",
    "#sort by value in column    \n",
    "df = df.sort_values('State_Code')  \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM_ALL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>35833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>7414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>75730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>25531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>307482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NUM_ALL\n",
       "State_Code         \n",
       "1.0           35833\n",
       "2.0            7414\n",
       "4.0           75730\n",
       "5.0           25531\n",
       "6.0          307482"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#groupby aggregate\n",
    "df_by_state = df[['State_Code', 'NUM_ALL']].groupby('State_Code').sum()\n",
    "df_by_state.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM_FHA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>9269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>22533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>88745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NUM_FHA\n",
       "State_Code         \n",
       "1.0            9269\n",
       "2.0            1850\n",
       "4.0           22533\n",
       "5.0            5665\n",
       "6.0           88745"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#groupby aggregate\n",
    "df_by_state = df[['State_Code', 'NUM_FHA']].groupby('State_Code').sum()\n",
    "df_by_state.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "Spark SQL brings in dataframes and SQL functionality to Spark and in many senses a lot like Pandas' dataframes. \n",
    "\n",
    "Spark SQL allows one to query structured data in Spark applications. It supports 2 query parsers: SQL and HiveQL.\n",
    "We will focus on SQL in the following. Spark SQL and dataframes are often used as synonyms.\n",
    "\n",
    "It also brings in a DataFrame data structure which is best for analysis of tabular data.\n",
    "\n",
    "Despite all suggested similarities the implementation of dataframes in Spark and Pandas are rather different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames with Python\n",
    "\n",
    "This notebook demonstrates examples using Spark DataFrames in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "try:\n",
    "    sc\n",
    "except NameError:    \n",
    "    spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").appName(\"BD course\").getOrCreate()\n",
    "    sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, repeat FHA example as we did in Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State_Code: integer (nullable = true)\n",
      " |-- County_Code: integer (nullable = true)\n",
      " |-- Census_Tract_Number: double (nullable = true)\n",
      " |-- NUM_ALL: integer (nullable = true)\n",
      " |-- NUM_FHA: integer (nullable = true)\n",
      " |-- PCT_NUM_FHA: double (nullable = true)\n",
      " |-- AMT_ALL: integer (nullable = true)\n",
      " |-- AMT_FHA: integer (nullable = true)\n",
      " |-- PCT_AMT_FHA: double (nullable = true)\n",
      "\n",
      "+----------+-----------+-------------------+-------+-------+-----------+-------+-------+-----------+\n",
      "|State_Code|County_Code|Census_Tract_Number|NUM_ALL|NUM_FHA|PCT_NUM_FHA|AMT_ALL|AMT_FHA|PCT_AMT_FHA|\n",
      "+----------+-----------+-------------------+-------+-------+-----------+-------+-------+-----------+\n",
      "|        28|         49|             103.01|      1|      1|      100.0|     71|     71|      100.0|\n",
      "|        40|          3|               null|      1|      1|      100.0|    215|    215|      100.0|\n",
      "|        39|        113|              603.0|      3|      3|      100.0|    206|    206|      100.0|\n",
      "|        12|        105|             124.04|      2|      2|      100.0|    303|    303|      100.0|\n",
      "|        12|         86|             9808.0|      1|      1|      100.0|    188|    188|      100.0|\n",
      "|        39|         35|             1202.0|      1|      1|      100.0|     19|     19|      100.0|\n",
      "|        12|        103|              207.0|      2|      2|      100.0|    100|    100|      100.0|\n",
      "|        36|        119|               30.0|      1|      1|      100.0|    354|    354|      100.0|\n",
      "|        39|        153|               null|      1|      1|      100.0|    213|    213|      100.0|\n",
      "|        48|        113|              87.04|      1|      1|      100.0|     64|     64|      100.0|\n",
      "|        12|         86|              18.01|      1|      1|      100.0|     45|     45|      100.0|\n",
      "|        13|        121|               64.0|      1|      1|      100.0|     61|     61|      100.0|\n",
      "|        12|         81|               7.03|      2|      2|      100.0|    241|    241|      100.0|\n",
      "|        48|         61|             138.02|      1|      1|      100.0|     66|     66|      100.0|\n",
      "|        17|         31|            2522.01|      2|      2|      100.0|    202|    202|      100.0|\n",
      "|        34|         31|               null|      1|      1|      100.0|    164|    164|      100.0|\n",
      "|        24|         33|            8056.02|      2|      2|      100.0|    315|    315|      100.0|\n",
      "|        39|         35|            1015.01|      2|      2|      100.0|    145|    145|      100.0|\n",
      "|        72|         83|             9597.0|      1|      1|      100.0|     85|     85|      100.0|\n",
      "|        13|        121|              81.01|      1|      1|      100.0|     68|     68|      100.0|\n",
      "+----------+-----------+-------------------+-------+-------+-----------+-------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delimiter = \",\"\n",
    "\n",
    "#csv into spark dataframe   \n",
    "input_df = spark.read.options(header='true', inferschema='true',delimiter=delimiter).csv('../0_Preexercise/data/fha_by_tract.csv')\n",
    "\n",
    "#rename columns\n",
    "oldColumns = input_df.schema.names\n",
    "newColumns = [\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "\n",
    "from functools import reduce\n",
    "fha_df = reduce(lambda input_df, idx: input_df.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), input_df)\n",
    "fha_df.printSchema()\n",
    "fha_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframes\n",
    "\n",
    "In the previous example we created a dataframe by loading data from CSV file.\n",
    "\n",
    "In the following, we create dataframe from a list of `Row` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print (department1)\n",
    "print (employee2)\n",
    "print (departmentWithEmployees1.employees[0].email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the first DataFrame from a list of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2nd DataFrame from a list of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with DataFrames\n",
    "\n",
    "Union to DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unionDF = df1.unionAll(df2)\n",
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the Unioned DataFrame to a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will define this utility function once again inline here\n",
    "def output_cleaner():\n",
    "    import os\n",
    "    os.system(\"rm -rf ./output*\")\n",
    "    print (\"Output folders removed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folders removed!\n"
     ]
    }
   ],
   "source": [
    "#clean tmp directories if necessary\n",
    "output_cleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF.write.parquet(\"./output_parquet/dfexample_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a DataFrame from the Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|[345678,Theater a...|[[michael,armbrus...|\n",
      "|[123456,Computer ...|[[michael,armbrus...|\n",
      "|[789012,Mechanica...|[[matei,null,no-r...|\n",
      "|[901234,Indoor Re...|[[xiangrui,meng,n...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF = spark.read.parquet(\"./output_parquet/dfexample_parquet\")\n",
    "parquetDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Struct type columns: explode function\n",
    "\n",
    "When working with composite columns -- for instance, where each element is an object or struct type, or an array -- we often times need to apply a transformation to a particular class/struct member. Explode function helps to achieve that:\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html#explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = parquetDF.select(explode(\"employees\").alias(\"e\"))\n",
    "explodedDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "explodedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstName: string, lastName: string, email: string, salary: bigint]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and sorting dataframes\n",
    "\n",
    "We can now apply filtering and sorting by an individual data member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterDF = explodedDF.filter(explodedDF.firstName == \"xiangrui\").sort(explodedDF.lastName)\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to apply a composite predicate for filtering. Also, by default sorting is in descending order, one can request to do in ascending:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "# Use `|` instead of `or` \n",
    "filterDF = explodedDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `where()` clause is equivalent to `filter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whereDF = explodedDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "whereDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with NAs\n",
    "\n",
    "Replace null values with -- using DataFrame Na functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "|  michael|armbrust|no-reply@berkeley...|100000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "|       --| wendell|no-reply@berkeley...|160000|\n",
      "| xiangrui|    meng|no-reply@stanford...|120000|\n",
      "|    matei|      --|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonNullDF = explodedDF.fillna(\"--\")\n",
    "nonNullDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve only rows with missing firstName or lastName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+\n",
      "|firstName|lastName|               email|salary|\n",
      "+---------+--------+--------------------+------+\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|     null| wendell|no-reply@berkeley...|160000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "|    matei|    null|no-reply@waterloo...|140000|\n",
      "+---------+--------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterNonNullDF = explodedDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\n",
    "filterNonNullDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and aggregating the data\n",
    "\n",
    "Example aggregations using `agg()` and `countDistinct()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------------+\n",
      "|firstName|lastName|count(DISTINCT firstName)|\n",
      "+---------+--------+-------------------------+\n",
      "|     null| wendell|                        0|\n",
      "|    matei|    null|                        1|\n",
      "| xiangrui|    meng|                        1|\n",
      "|  michael|armbrust|                        1|\n",
      "+---------+--------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "countDistinctDF = explodedDF.select(\"firstName\", \"lastName\")\\\n",
    "  .groupBy(\"firstName\", \"lastName\").agg(countDistinct(\"firstName\"))\n",
    "\n",
    "countDistinctDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the DataFrame and SQL Query Physical Plans (Hint: They should be the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[firstName#1775, lastName#1776], functions=[count(distinct firstName#1775)])\n",
      "+- *HashAggregate(keys=[firstName#1775, lastName#1776], functions=[partial_count(distinct firstName#1775)])\n",
      "   +- *HashAggregate(keys=[firstName#1775, lastName#1776, firstName#1775], functions=[])\n",
      "      +- Exchange hashpartitioning(firstName#1775, lastName#1776, firstName#1775, 200)\n",
      "         +- *HashAggregate(keys=[firstName#1775, lastName#1776, firstName#1775], functions=[])\n",
      "            +- *Project [e#1772.firstName AS firstName#1775, e#1772.lastName AS lastName#1776]\n",
      "               +- Generate explode(employees#1755), false, false, [e#1772]\n",
      "                  +- *FileScan parquet [employees#1755] Batched: false, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/3_DataFramesSQL/output_parquet/dfexample_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<employees:array<struct<firstName:string,lastName:string,email:string,salary:bigint>>>\n"
     ]
    }
   ],
   "source": [
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are coming from SQL rather than Pandas, you might prefer the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[firstName#1775, lastName#1776], functions=[count(distinct firstName#1775)])\n",
      "+- *HashAggregate(keys=[firstName#1775, lastName#1776], functions=[partial_count(distinct firstName#1775)])\n",
      "   +- *HashAggregate(keys=[firstName#1775, lastName#1776, firstName#1775], functions=[])\n",
      "      +- Exchange hashpartitioning(firstName#1775, lastName#1776, firstName#1775, 200)\n",
      "         +- *HashAggregate(keys=[firstName#1775, lastName#1776, firstName#1775], functions=[])\n",
      "            +- *Project [e#1772.firstName AS firstName#1775, e#1772.lastName AS lastName#1776]\n",
      "               +- Generate explode(employees#1755), false, false, [e#1772]\n",
      "                  +- *FileScan parquet [employees#1755] Batched: false, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/3_DataFramesSQL/output_parquet/dfexample_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<employees:array<struct<firstName:string,lastName:string,email:string,salary:bigint>>>\n"
     ]
    }
   ],
   "source": [
    "# register the DataFrame as a temp table so that we can query it using SQL\n",
    "explodedDF.registerTempTable(\"df_example\")\n",
    "\n",
    "# Perform the same query as the DataFrame above and return ``explain``\n",
    "countDistinctDF_sql = spark.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM df_example GROUP BY firstName, lastName\")\n",
    "\n",
    "countDistinctDF_sql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum up all the salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|    1040000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salarySumDF = explodedDF.agg({\"salary\" : \"sum\"}) \n",
    "salarySumDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(explodedDF.salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the summary statistics for the salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            salary|\n",
      "+-------+------------------+\n",
      "|  count|                 8|\n",
      "|   mean|          130000.0|\n",
      "| stddev|23904.572186687874|\n",
      "|    min|            100000|\n",
      "|    max|            160000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodedDF.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example using Pandas & Matplotlib Integration\n",
    "\n",
    "There is a connector allowing to convert Spark dataframes into pandas dataframes. Once you have the data in Pandas dataframes, several plotting front-ends are available. Including Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7faf291a93c8>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf5ada2e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEkCAYAAADTtG33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+8VVWd//HXW0BRU0G8NsYPLyX+CsdSVGb8ViYO4o/C\nSr9ClmgYM4rOZFZiOVkW39GmicZSJ0sUnUY0xpJvYkTqWM3XH6CmgNZwJUdvOoqCP1JR0c/3j7Vu\n7S7ncjfn3HvPOdz38/E4D/b+7LX3WWevw/ncvfbaeysiMDMzK2OrelfAzMyah5OGmZmV5qRhZmal\nOWmYmVlpThpmZlaak4aZmZXmpGFmZqU5aZiZWWlOGmZmVtrAelegp+2yyy7R2tpa72qYmTWVe++9\n95mIaOmu3BaXNFpbW1m2bFm9q2Fm1lQk/XeZcu6eMjOz0pw0zMysNCcNMzMrbYs7p2FmVsnrr79O\ne3s769evr3dV6mrw4MGMGDGCQYMGVbW+k4aZ9Qvt7e3ssMMOtLa2Iqne1amLiODZZ5+lvb2d0aNH\nV7WNbrunJM2V9LSkFZ3iZ0n6jaSVkr5WiJ8nqS0vO7IQn5RjbZJmFeKjJd0taZWk6yVtnePb5Pm2\nvLy1qk9oZgasX7+eYcOG9duEASCJYcOG1XS0VeacxtXApE5v/H5gMvDnEfFO4Os5vi8wBXhnXucy\nSQMkDQAuBY4C9gWm5rIAFwNzImIMsA6YnuPTgXURsQcwJ5czM6taf04YHWrdB90mjYj4ObC2U/h0\n4KKIeDWXeTrHJwPzI+LViPgt0AYcnF9tEbE6Il4D5gOTlWp/OLAgrz8POK6wrXl5egEwQW5xM7O6\nqvacxp7AeyTNBtYDn4mIpcBw4K5CufYcA3i8U/wQYBjwXERsqFB+eMc6EbFB0vO5/DNV1tnM7A9a\nZ93co9t79KJjenR7p5xyCsceeyzHH398j263VtUmjYHAUGA8cBBwg6S3A5WOBILKRzSxifJ0s+xP\nSJoBzAAYNWrUJivebHr6iw09/+W2/svfz8axYcMGBg7s/bFN1V6n0Q7cGMk9wJvALjk+slBuBPDE\nJuLPAEMkDewUp7hOXr4TG3eTARARV0TEuIgY19LS7a1TzMzq4qWXXuKYY45h//33Z+zYsVx//fVc\neOGFHHTQQYwdO5YZM2YQsfHfxl2VOeyww/j85z/P+973PmbPns3o0aN5/fXXAXjhhRdobW39w3xP\nqTZp/Ih0LgJJewJbkxLAQmBKHvk0GhgD3AMsBcbkkVJbk06WL4z0yW8HOo6/pgE35emFeZ68/Lao\ntDfNzJrET37yE972trfxwAMPsGLFCiZNmsSZZ57J0qVLWbFiBa+88go//vGPN1pvU2Wee+457rjj\nDi644AIOO+wwbr45Hf3Nnz+fj3zkI1Vfj9GVMkNurwPuBPaS1C5pOjAXeHsehjsfmJaPOlYCNwAP\nAT8BZkbEG/mcxZnAYuBh4IZcFuBc4NOS2kjnLK7M8SuBYTn+aeAPw3TNzJrRfvvtx89+9jPOPfdc\nfvGLX7DTTjtx++23c8ghh7Dffvtx2223sXLlyo3W21SZE0888Q/Tp512GldddRUAV111FaeeemqP\nf4ZuO8AiYmoXiz7WRfnZwOwK8UXAogrx1aTRVZ3j64ETuqufmVmz2HPPPbn33ntZtGgR5513HhMn\nTuTSSy9l2bJljBw5ki996UsbXUOxfv16zjjjjC7LbL/99n+YPvTQQ3n00Ue54447eOONNxg7dmyP\nfwbfe8rMrI888cQTbLfddnzsYx/jM5/5DPfddx8Au+yyC7///e9ZsGDBRut0JIhNlSk6+eSTmTp1\naq8cZYBvI2Jm/VQ9RmktX76cz372s2y11VYMGjSIyy+/nB/96Efst99+tLa2ctBBB220zpAhQ/jk\nJz+5yTJFJ510Eueffz5Tp3bVSVQbbWnnlseNGxdb0kOYPKTRGlkzfT8ffvhh9tlnn17ZdiNZsGAB\nN910E9dee22XZSrtC0n3RsS47rbvIw0zsy3EWWedxS233MKiRRudPu4xThpmZluIb33rW73+Hj4R\nbmb9xpbWHV+NWveBk4aZ9QuDBw/m2Wef7deJo+N5GoMHD656G+6eMrN+YcSIEbS3t7NmzZp6V6Wu\nOp7cVy0nDTPrFwYNGlT10+rsj9w9ZWZmpTlpmJlZaU4aZmZWmpOGmZmV5qRhZmalOWmYmVlpThpm\nZlaak4aZmZVW5nGvcyU9nR/t2nnZZySFpF3yvCRdIqlN0oOSDiiUnSZpVX5NK8QPlLQ8r3OJJOX4\nzpKW5PJLJA3tmY9sZmbVKnOkcTUwqXNQ0kjgr4DHCuGjgDH5NQO4PJfdGbgAOIT0aNcLCkng8ly2\nY72O95oF3BoRY4Bb8TPCzczqrtukERE/B9ZWWDQH+BxQvPvXZOCaSO4ChkjaDTgSWBIRayNiHbAE\nmJSX7RgRd0a6i9g1wHGFbc3L0/MKcTMzq5OqzmlI+iDwu4h4oNOi4cDjhfn2HNtUvL1CHOCtEfEk\nQP5312rqamZmPWezb1goaTvgC8DESosrxKKK+ObWaQapi4tRo0Zt7upmDaeZHqNqPaNZ2ryaI413\nAKOBByQ9CowA7pP0Z6QjhZGFsiOAJ7qJj6gQB3gqd1+R/326qwpFxBURMS4ixrW0tFTxkczMrIzN\nThoRsTwido2I1ohoJf3wHxAR/wMsBE7Oo6jGA8/nrqXFwERJQ/MJ8InA4rzsRUnj86ipk4Gb8lst\nBDpGWU0rxM3MrE7KDLm9DrgT2EtSu6Tpmyi+CFgNtAHfBc4AiIi1wFeApfl1YY4BnA58L6/zCHBL\njl8E/JWkVaRRWhdt3kczM7Oe1u05jYiY2s3y1sJ0ADO7KDcXmFshvgwYWyH+LDChu/qZmVnf8RXh\nZmZWmpOGmZmV5qRhZmalOWmYmVlpThpmZlaak4aZmZXmpGFmZqU5aZiZWWlOGmZmVpqThpmZleak\nYWZmpTlpmJlZaU4aZmZWmpOGmZmV5qRhZmalOWmYmVlpThpmZlZamce9zpX0tKQVhdg/Svq1pAcl\n/VDSkMKy8yS1SfqNpCML8Uk51iZpViE+WtLdklZJul7S1jm+TZ5vy8tbe+pDm5lZdcocaVwNTOoU\nWwKMjYg/B/4LOA9A0r7AFOCdeZ3LJA2QNAC4FDgK2BeYmssCXAzMiYgxwDqg4xnk04F1EbEHMCeX\nMzOzOuo2aUTEz4G1nWI/jYgNefYuYESengzMj4hXI+K3QBtwcH61RcTqiHgNmA9MliTgcGBBXn8e\ncFxhW/Py9AJgQi5vZmZ1MrAHtvEJ4Po8PZyURDq05xjA453ihwDDgOcKCahYfnjHOhGxQdLzufwz\nnSsgaQYwA2DUqFE1fhzbkrXOurnHt/noRcf0+DatZ/V0u/fnNq/pRLikLwAbgO93hCoUiyrim9rW\nxsGIKyJiXESMa2lp2XSlzcysalUfaUiaBhwLTIiIjh/zdmBkodgI4Ik8XSn+DDBE0sB8tFEs37Gt\ndkkDgZ3o1E1mZmZ9q6ojDUmTgHOBD0bEy4VFC4EpeeTTaGAMcA+wFBiTR0ptTTpZvjAnm9uB4/P6\n04CbCtualqePB24rJCczM6uDbo80JF0HHAbsIqkduIA0WmobYEk+N31XRPxNRKyUdAPwEKnbamZE\nvJG3cyawGBgAzI2IlfktzgXmS/oqcD9wZY5fCVwrqY10hDGlBz6vmZnVoNukERFTK4SvrBDrKD8b\nmF0hvghYVCG+mjS6qnN8PXBCd/UzM7O+4yvCzcysNCcNMzMrzUnDzMxKc9IwM7PSnDTMzKw0Jw0z\nMyvNScPMzEpz0jAzs9KcNMzMrDQnDTMzK81Jw8zMSnPSMDOz0pw0zMysNCcNMzMrzUnDzMxKc9Iw\nM7PSnDTMzKy0bpOGpLmSnpa0ohDbWdISSavyv0NzXJIukdQm6UFJBxTWmZbLr5I0rRA/UNLyvM4l\nys+P7eo9zMysfsocaVwNTOoUmwXcGhFjgFvzPMBRwJj8mgFcDikBkJ4tfgjp0a4XFJLA5blsx3qT\nunkPMzOrk26TRkT8HFjbKTwZmJen5wHHFeLXRHIXMETSbsCRwJKIWBsR64AlwKS8bMeIuDMiArim\n07YqvYeZmdXJwCrXe2tEPAkQEU9K2jXHhwOPF8q159im4u0V4pt6j41ImkE6WmHUqFGlPkDrrJtL\nldscj150TI9vs1l4f5r1Dz19IlwVYlFFfLNExBURMS4ixrW0tGzu6mZmVlK1SeOp3LVE/vfpHG8H\nRhbKjQCe6CY+okJ8U+9hZmZ1Um3SWAh0jICaBtxUiJ+cR1GNB57PXUyLgYmShuYT4BOBxXnZi5LG\n51FTJ3faVqX3MDOzOun2nIak64DDgF0ktZNGQV0E3CBpOvAYcEIuvgg4GmgDXgZOBYiItZK+AizN\n5S6MiI6T66eTRmhtC9ySX2ziPczMrE66TRoRMbWLRRMqlA1gZhfbmQvMrRBfBoytEH+20nuYmVn9\n+IpwMzMrzUnDzMxKc9IwM7PSnDTMzKw0Jw0zMyvNScPMzEpz0jAzs9KcNMzMrDQnDTMzK81Jw8zM\nSnPSMDOz0pw0zMysNCcNMzMrzUnDzMxKc9IwM7PSnDTMzKw0Jw0zMyutpqQh6WxJKyWtkHSdpMGS\nRku6W9IqSddL2jqX3SbPt+XlrYXtnJfjv5F0ZCE+KcfaJM2qpa5mZla7qpOGpOHA3wLjImIsMACY\nAlwMzImIMcA6YHpeZTqwLiL2AObkckjaN6/3TmAScJmkAZIGAJcCRwH7AlNzWTMzq5Nau6cGAttK\nGghsBzwJHA4syMvnAcfl6cl5nrx8giTl+PyIeDUifgu0AQfnV1tErI6I14D5uayZmdVJ1UkjIn4H\nfB14jJQsngfuBZ6LiA25WDswPE8PBx7P627I5YcV453W6Sq+EUkzJC2TtGzNmjXVfiQzM+tGLd1T\nQ0l/+Y8G3gZsT+pK6iw6Vuli2ebGNw5GXBER4yJiXEtLS3dVNzOzKtXSPXUE8NuIWBMRrwM3An8J\nDMndVQAjgCfydDswEiAv3wlYW4x3WqeruJmZ1UktSeMxYLyk7fK5iQnAQ8DtwPG5zDTgpjy9MM+T\nl98WEZHjU/LoqtHAGOAeYCkwJo/G2pp0snxhDfU1M7MaDey+SGURcbekBcB9wAbgfuAK4GZgvqSv\n5tiVeZUrgWsltZGOMKbk7ayUdAMp4WwAZkbEGwCSzgQWk0ZmzY2IldXW18zMald10gCIiAuACzqF\nV5NGPnUuux44oYvtzAZmV4gvAhbVUkczM+s5viLczMxKc9IwM7PSnDTMzKw0Jw0zMyvNScPMzEpz\n0jAzs9KcNMzMrDQnDTMzK81Jw8zMSnPSMDOz0pw0zMysNCcNMzMrzUnDzMxKc9IwM7PSnDTMzKw0\nJw0zMyutpqQhaYikBZJ+LelhSX8haWdJSyStyv8OzWUl6RJJbZIelHRAYTvTcvlVkqYV4gdKWp7X\nuSQ/VtbMzOqk1iONfwZ+EhF7A/sDDwOzgFsjYgxwa54HOIr0/O8xwAzgcgBJO5Oe/ncI6Yl/F3Qk\nmlxmRmG9STXW18zMalB10pC0I/Be8jPAI+K1iHgOmAzMy8XmAcfl6cnANZHcBQyRtBtwJLAkItZG\nxDpgCTApL9sxIu6MiACuKWzLzMzqoJYjjbcDa4CrJN0v6XuStgfeGhFPAuR/d83lhwOPF9Zvz7FN\nxdsrxM3MrE5qSRoDgQOAyyPi3cBL/LErqpJK5yOiivjGG5ZmSFomadmaNWs2XWszM6taLUmjHWiP\niLvz/AJSEnkqdy2R/326UH5kYf0RwBPdxEdUiG8kIq6IiHERMa6lpaWGj2RmZptSddKIiP8BHpe0\nVw5NAB4CFgIdI6CmATfl6YXAyXkU1Xjg+dx9tRiYKGloPgE+EVicl70oaXweNXVyYVtmZlYHA2tc\n/yzg+5K2BlYDp5IS0Q2SpgOPASfksouAo4E24OVclohYK+krwNJc7sKIWJunTweuBrYFbskvMzOr\nk5qSRkT8ChhXYdGECmUDmNnFduYCcyvElwFja6mjmZn1HF8RbmZmpTlpmJlZaU4aZmZWmpOGmZmV\n5qRhZmalOWmYmVlpThpmZlaak4aZmZXmpGFmZqU5aZiZWWlOGmZmVpqThpmZleakYWZmpTlpmJlZ\naU4aZmZWmpOGmZmV5qRhZmal1Zw0JA2QdL+kH+f50ZLulrRK0vX5UbBI2ibPt+XlrYVtnJfjv5F0\nZCE+KcfaJM2qta5mZlabnjjS+Dvg4cL8xcCciBgDrAOm5/h0YF1E7AHMyeWQtC8wBXgnMAm4LCei\nAcClwFHAvsDUXNbMzOqkpqQhaQRwDPC9PC/gcGBBLjIPOC5PT87z5OUTcvnJwPyIeDUifgu0AQfn\nV1tErI6I14D5uayZmdVJrUca3wQ+B7yZ54cBz0XEhjzfDgzP08OBxwHy8udz+T/EO63TVdzMzOqk\n6qQh6Vjg6Yi4txiuUDS6Wba58Up1mSFpmaRla9as2UStzcysFrUcaRwKfFDSo6Suo8NJRx5DJA3M\nZUYAT+TpdmAkQF6+E7C2GO+0TlfxjUTEFRExLiLGtbS01PCRzMxsU6pOGhFxXkSMiIhW0ons2yLi\nJOB24PhcbBpwU55emOfJy2+LiMjxKXl01WhgDHAPsBQYk0djbZ3fY2G19TUzs9oN7L7IZjsXmC/p\nq8D9wJU5fiVwraQ20hHGFICIWCnpBuAhYAMwMyLeAJB0JrAYGADMjYiVvVBfMzMrqUeSRkT8B/Af\neXo1aeRT5zLrgRO6WH82MLtCfBGwqCfqaGZmtfMV4WZmVpqThpmZleakYWZmpTlpmJlZaU4aZmZW\nmpOGmZmV5qRhZmalOWmYmVlpThpmZlaak4aZmZXmpGFmZqU5aZiZWWlOGmZmVpqThpmZleakYWZm\npTlpmJlZaU4aZmZWWtVJQ9JISbdLeljSSkl/l+M7S1oiaVX+d2iOS9IlktokPSjpgMK2puXyqyRN\nK8QPlLQ8r3OJJNXyYc3MrDa1HGlsAM6JiH2A8cBMSfsCs4BbI2IMcGueBzgKGJNfM4DLISUZ4ALg\nENJjYi/oSDS5zIzCepNqqK+ZmdWo6qQREU9GxH15+kXgYWA4MBmYl4vNA47L05OBayK5CxgiaTfg\nSGBJRKyNiHXAEmBSXrZjRNwZEQFcU9iWmZnVQY+c05DUCrwbuBt4a0Q8CSmxALvmYsOBxwurtefY\npuLtFeJmZlYnNScNSW8B/h34VES8sKmiFWJRRbxSHWZIWiZp2Zo1a7qrspmZVammpCFpEClhfD8i\nbszhp3LXEvnfp3O8HRhZWH0E8EQ38REV4huJiCsiYlxEjGtpaanlI5mZ2SbUMnpKwJXAwxHxjcKi\nhUDHCKhpwE2F+Ml5FNV44PncfbUYmChpaD4BPhFYnJe9KGl8fq+TC9syM7M6GFjDuocCHweWS/pV\njn0euAi4QdJ04DHghLxsEXA00Aa8DJwKEBFrJX0FWJrLXRgRa/P06cDVwLbALfllZmZ1UnXSiIhf\nUvm8A8CECuUDmNnFtuYCcyvElwFjq62jmZn1LF8RbmZmpTlpmJlZaU4aZmZWmpOGmZmV5qRhZmal\nOWmYmVlpThpmZlaak4aZmZXmpGFmZqU5aZiZWWlOGmZmVpqThpmZleakYWZmpTlpmJlZaU4aZmZW\nmpOGmZmV5qRhZmalNXzSkDRJ0m8ktUmaVe/6mJn1Zw2dNCQNAC4FjgL2BaZK2re+tTIz678aOmkA\nBwNtEbE6Il4D5gOT61wnM7N+q9GTxnDg8cJ8e46ZmVkdKCLqXYcuSToBODIiTsvzHwcOjoizOpWb\nAczIs3sBv+nhquwCPNPD2+wNrmfPaYY6guvZ0/pzPXePiJbuCg3s4Tftae3AyML8COCJzoUi4grg\nit6qhKRlETGut7bfU1zPntMMdQTXs6e5nt1r9O6ppcAYSaMlbQ1MARbWuU5mZv1WQx9pRMQGSWcC\ni4EBwNyIWFnnapmZ9VsNnTQAImIRsKjO1ei1rq8e5nr2nGaoI7iePc317EZDnwg3M7PG0ujnNMzM\nrIE4aZiZWWlOGj1E0vb1rsOWpBn2p6Th+VY3DUXSHvWuw5aqUdsc+q7dnTR6gKS3A/8oab9612VL\n0ET787PAPvWuRJGkIcAnJA2td122UA3X5tC37e6k0TPeAqwFTpP0znpXRlKzt2tD7c+uRMSngBcl\n/VBSo4xEfBH4MrCHpIvqXZlKmvn72aBtDn3Y7k3beI1AkgAi4kHgetIP3en1+qGTtIukHSLizXq8\nf60abX92R9KgiPhvYEfg+/X8ESnsuzci4lVgMLCXpPPrVafOmv37CY3V5rk+fd7uThpVkqQojFeO\niOWksdPPUocfOkm7Az8EdujL9+0pjbY/uyPpz4GLJO0cERNI/1nn1+NHpLjvJLVKGhkRvwAuAvaV\n9MW+rlNnzf79hMZq81yfurS7k0aVCo11tqTLJN1E+hJdCzwFzMhfsl4naRRwGTAjIja6N1czaKT9\nWdIzwB7A5yQNjYjJpP9PC/v6R6Sw784BvgP8q6Q5wCrgW6Rb8dStq2pL+H5mDdPmUL92d9KogaTp\nwNHAucAY4LMR0QbcCKwHPpbvmdWbddgd+DFwTkQ83Jvv1dsaYX92R9LekvbOP35/A4wCviBp+4j4\nMPAm0OfJTdLRwMSIOBK4H9grItZGxJ2kI7ZdJe1Sh3o1/fezUds8163P291XhNdA0rmkH7QPAEcA\nxwFvkJLxbsDLEdFrt1nOQ/++CMxv1v+QRfXenyXq945cpz2Br0fEKkm7ATeT/ro7IyKe7aO6bBsR\nr+RpAQcCe5P+Ev4L4AMR8ZqkAyPiXkmDI2J9X9StUMem/342Upvn+tS93Rvp7H9D69znng0B5gGr\ngeNyY50DDIqIXu8OiIg3JH0lIjb09nv1tEbcn5si6XDSX5lXATsBZ0j6TkT8WtJlwHRS/Xv9B0Tp\nGpYJkp4G3g5sDawBziJ15U2KiJD018AUSR+MiBd7u16dNfP3ExqrzXN9GqLdnTRK6HTCaSLwNPA/\npH7DqcC/AwMlTQFOBY7vq7o143/IRt6flUgaQ/rxuDAiVkh6lvTc+tmSluT6nR0Rj/Rhtd4knSfY\nCdg/In4v6VDg3cA0ScOBE4Gp9UgYHZrx+wkN2+bQAO3upFFC8SQt6cvyS2B30iiF9wPfI/Vp7gac\nGBG/rlNVm0Iz7c98DmU6sB/pqZArIuIeSWtJf90dAfxTRNzVB3VRJC9Jeor09LY7gfHAzyLi85JO\nB1qAYcD/9ndx8zVSm+f6NFS7+5xGSflQ9dyIOFLSN4GDSU8W/FpELMv9iztGxPN1rWiTaIb9KekA\n4DXS0yLPzeEbI+LuQpltIuLVLrrberIuxaOzbSPiFUk7kgYOTARuiYgf5L+QfxcRL/dWXbZkjdTm\n+b0art2dNLogaauIeLOj0ZSGe74ATABOAk4G/g/wDuDiiPATBTehWfZnoX77A58GRgN/TRpu+WnS\nifmb8+iUrs7N9Gb9/hb4S+Al4Brg58AM0l+dA0lHZ1MjYk1f1anZNXqb5/dsmHb3kNsuxB+vWt0r\n/9W7IiIeJZ2A+vuIaAceAe4AltWnls2jWfZn/vE4FpgLrAQeB74OvA2YA2wHTJa0U0f5vqqbpJnA\nh4HzSN0Q84BjI+I7wPeBV4BPOWFsnkZuc2jAdo8IvwovUjY/JU+fCbSRMvtppIvNzgf+m3TjsgeA\nUfWucyO/mm1/AgK+ARyT54eTRqfcQhp2+TZg7z6qy1aF6W1IJ2aHAeeQhiafSBpp9qF6t3Mzvxqp\nzZuh3d091YmkI0gXxcwnnXCaDRwG7A88EhGXSvokMBK4ISJW1KuuzaAZ96ekK0ldt5/I8wcDF5NO\ngn4h+njEjKTJpL8mXwHuARYDJ0TEGkk/A95KSs6/D/+HrkqjtXmuQ0O2u7unstxlQkT8DDiFNLxu\n20g3J/tX4P8Be0r6LPCvEfHFRviBa1TNtj8lvUvSe/Ps+cB2kv4+z79GOkJ6kXQhVW/XRYXpKaRb\nRBxO+mv4o6QfkN0knULqyjsiIl50wtg8jdTmuT5N0e4ecstGIxT2IzXI54BrJZ0aEVcBCyRtA4wF\ntiVlf6ugWfZn4QToBNLY9+ck3UnqAvg68A1Jf0H60TiKdLJ+H9LVwL1apzy9OxDAoRHxiKSPArOA\nQcCrpGtajouIp3qrPluaRmzzYr3ydGO3ez36xBr1BZxNGpUwIs8fAfwKOLVQZod617NZXs2wP4ED\ngB8BraRzLBeTRnEdQDoS35vUx/0e4CFgz16siwrTM4G78nueBgzO8Q+STtR+iDQkue7t3GyvRmrz\nZmz3ft09JWlQYXoS6QTTRyKN5CFS18rZwJckfTzH6nZ1baNrtv0paTDp8H8CsGuke/R8k3RidDow\nPtJFUtsDnwCmRMR/9VZ9ouMXJPVlvxv4OOlmf/sB4yUNjDQU+RzggYh4obfqsqVqtDaH5mv3fps0\nJO1Nek5DRz9iAHdEOsn0lkLRX5KuI/jPvq5jM2mW/VnsN84/GN8i9R1/QdK+EfFkjr1MeggU+Ufj\n7yI9HKq36zc8vz8RsYp0w78XgI8A788/IDdExOrersuWotHbPNexadq9346ekvQu4DHSRTHPkG7F\nfSkwLiJez2VOAt6IiPl1q2iTaIb9WejPPhY4lHTDt/NJF2+dSxrR9eWIWC5p60g3TNwq+vhJc5I+\nDHybdDvx65Se1fA10n2Hvhi+2ru0ZmnzXNfmaPd69o3V48WfjoEeQjo0/TowgHTvo4dIF9KcA/wa\n2KfedW7kV7PtT+AY4F7SLaXvyq/tSUfds4GFHfMNUM8HSVf5Qhq00lLv9m7GV7O0ebO0e78bPRX5\nLwhJHyQ92OcRUsN8mXTF5SPAQaQx0B+KJn0OQF9p9P0paTTwroj4Ye7P/gAwjfT8gZeA50g/KAcA\nXwJ2j4iX+rKOlUTEzZLeBK6QtCEifkC6DbZ1o1nbHJqj3ftN91SnIW1TgH8Gvku+6Rep/3AI8I2I\nWCdpQERPA9rpAAAEeUlEQVS8UbcKN7hm2Z9KN6AbBKyKiLWShpKurv030gNrnpK0BvgdcEDUoVti\nUyT9FekiyLr3ZTeLZm9zaOx27xcnwrsYA/2XEXE+qSvlQ6TD01eAs5WeONZwX6RG0Sz7M9fzPtLt\nSZZIOisi1gG/Bx4G3iLpEOBy4MxG/PGIiCWN+MPRqLaENofGbvctvnuq0w/cTNJwth1JF/H8LiJu\nyIMr/ol0sc8cH2F0rZn2Z0SE0u0gjifd9+qbktZHxHclvQp8AZgEnBwRvyx+NmtObvPet8UnjcIP\nXHEM9Cf54xjoX+Yfug3Ar/JfJdaFZtifnX4IHiHd6vpB4POk5LYuImZI2gP4dv7LFP94NC+3ed/Z\n4pMG/MkY6J9GejD8F0l/cXwEGCTp9oi4sa6VbCKNvj/zX5vvJf1wrCfdKXS3iLhV0qnAdZJ2iogr\n61VH61lu877TL85pRMTvgE8BR0uaGukCny8DrwNHksZuW0mNvj/zxVy7k+4ddDCpO+LflR5kcxgp\n4fXqVb7Wt9zmfaffjJ4CkHQM8A/AP8QfL54ZGn5oTVWaZX9KugTYlTQe/3hgdkTc6/7sLZfbvPf0\ni+6pDs0wBrqZNPr+LFzZuxzYOSL+jTTsEnB/9pbIbd77+kX3VFFE3EK6Edm99a7LlqCR92dhOGUb\n8F5JO6lwU0Xb8rjNe1+/6p6y/ilf3PVn4av7+w23ee9x0jAzs9L6XfeUmZlVz0nDzMxKc9IwM7PS\nnDTMzKw0Jw2zAkl/K+lhSeskzdqM9VolfbQwf5ikkPSBQuzHkg7r4Sqb9SknDbM/dQZwdEQMjYiL\nOi/MV71X0gp8tFOsnXRPLrMthpOGWSbpX4C3AwslnS3p2zl+taRvSLoduFjS+yT9Kr/ul7QD6dG2\n78mxs/MmHwCezw/U6fxeX5S0VNIKSVfkeych6T8kzZH083zEc5CkGyWtkvTVwvofk3RPfr/vKD2z\nxKzXOWmYZRHxN8ATwPuBzrd03xM4IiLOAT4DzIyIdwHvIT1sahbwi4h4V0TMKaz3VeD8Cm/37Yg4\nKCLGAtsCxxaWvRYR7wX+BbgJmAmMBU6RNEzSPsCJwKG5Dm8AJ9Xy2c3K6lf3njKrwQ8KD5P6T9Iz\nGr4P3BgR7flAYSMR8QtJSHpPp0Xvl/Q5YDtgZ2Al8H/zsoX53+XAyoh4EkDSamAk8L+AA4Gl+X23\nBZ7ugc9o1i0nDbNyXuqYiIiLJN0MHA3cJemIbtadTTq3sQFA0mDSUw3HRcTjkr4EDC6UfzX/+2Zh\numN+ICBgXkScV/3HMauOu6fMNpOkd0TE8oi4GFgG7A28COxQqXxE/BQYCuyfQx0J4hlJbyHduntz\n3AocL2nXXJ+dlZ7VbtbrnDTMNt+n8gnsB0jnM24hPVp0g6QHCifCi2YDIwAi4jngu6Tupx8BSzfn\nzSPiIdJ5kp9KehBYAuxW7Ycx2xy+YaGZmZXmIw0zMyvNScPMzEpz0jAzs9KcNMzMrDQnDTMzK81J\nw8zMSnPSMDOz0pw0zMystP8PLWiQg+l53s4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf29b089b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.clf()\n",
    "pdDF = nonNullDF.toPandas()\n",
    "pdDF.plot(x='firstName', y='salary', kind='bar', rot=45)\n",
    "#display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions in Python\n",
    "\n",
    "This notebook contains some examples of creating a *UDF* in Python and registering it for use in Spark SQL.\n",
    "\n",
    "\n",
    "### Step 1: Register a function as a UDF.\n",
    "\n",
    "After the standard function is defined in Python, you need to register it as a UDF to be able to use on Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared(s):\n",
    "    return s * s\n",
    "\n",
    "spark.udf.register(\"squaredWithPython\", squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can also explicitly set the return type of your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "spark.udf.register(\"squaredWithPython\", squared, LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Call the UDF in Spark SQL\n",
    "\n",
    "Create a dataframe to test the newly created UDF on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1, 20).registerTempTable(\"test_dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use UDF with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|id_squared|\n",
      "+---+----------+\n",
      "|  1|         1|\n",
      "|  2|         4|\n",
      "|  3|         9|\n",
      "|  4|        16|\n",
      "|  5|        25|\n",
      "|  6|        36|\n",
      "|  7|        49|\n",
      "|  8|        64|\n",
      "|  9|        81|\n",
      "| 10|       100|\n",
      "| 11|       121|\n",
      "| 12|       144|\n",
      "| 13|       169|\n",
      "| 14|       196|\n",
      "| 15|       225|\n",
      "| 16|       256|\n",
      "| 17|       289|\n",
      "| 18|       324|\n",
      "| 19|       361|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "squared_udf = udf(squared, LongType())\n",
    "df = spark.table(\"test_dataframe\")\n",
    "df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our example with homes and Federal Housing Authority: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------------+-------+-------+-----------+-------+-------+-----------+-------------+\n",
      "|State_Code|County_Code|Census_Tract_Number|NUM_ALL|NUM_FHA|PCT_NUM_FHA|AMT_ALL|AMT_FHA|PCT_AMT_FHA|        GEOID|\n",
      "+----------+-----------+-------------------+-------+-------+-----------+-------+-------+-----------+-------------+\n",
      "|      null|       null|               null|   9477|   1932|    20.3862|1575871| 331515|    21.0369|         null|\n",
      "|         1|         87|             2319.0|      1|      1|      100.0|     28|     28|      100.0|  1.0872319E9|\n",
      "|         1|         25|            9580.01|      3|      3|      100.0|    459|    459|      100.0|1.025958001E9|\n",
      "|         1|         97|               50.0|      2|      2|      100.0|    128|    128|      100.0|   1.097005E9|\n",
      "|         1|        101|              56.03|      2|      2|      100.0|    126|    126|      100.0|1.101005603E9|\n",
      "|         1|         91|             9732.0|      2|      2|      100.0|    168|    168|      100.0|  1.0919732E9|\n",
      "|         1|         97|              13.02|      1|      1|      100.0|     29|     29|      100.0|1.097001302E9|\n",
      "|         1|         97|               49.0|      1|      1|      100.0|     70|     70|      100.0|  1.0970049E9|\n",
      "|         1|         73|              105.0|      1|      1|      100.0|     46|     46|      100.0|  1.0730105E9|\n",
      "|         1|         97|               40.0|      1|      1|      100.0|     25|     25|      100.0|   1.097004E9|\n",
      "|         1|         73|              51.04|      3|      3|      100.0|    361|    361|      100.0|1.073005104E9|\n",
      "|         1|         97|               27.0|      1|      1|      100.0|     60|     60|      100.0|  1.0970027E9|\n",
      "|         1|        101|              51.02|      1|      1|      100.0|     51|     51|      100.0|1.101005102E9|\n",
      "|         1|         97|              32.04|      3|      3|      100.0|    201|    201|      100.0|1.097003204E9|\n",
      "|         1|         33|              203.0|      1|      1|      100.0|    142|    142|      100.0|  1.0330203E9|\n",
      "|         1|         73|               35.0|      1|      1|      100.0|    117|    117|      100.0|  1.0730035E9|\n",
      "|         1|        101|               12.0|      1|      1|      100.0|     92|     92|      100.0|  1.1010012E9|\n",
      "|         1|         27|             9592.0|      2|      2|      100.0|    152|    152|      100.0|  1.0279592E9|\n",
      "|         1|         63|              600.0|      1|      1|      100.0|    141|    141|      100.0|    1.06306E9|\n",
      "|         1|         73|              38.03|      1|      1|      100.0|     59|     59|      100.0|1.073003803E9|\n",
      "+----------+-----------+-------------------+-------+-------+-----------+-------+-------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from math import pow\n",
    "\n",
    "def geoid(x,y,z):\n",
    "    if x is not None and y is not None and z is not None:\n",
    "        return 100*x + pow(10,6)*y + pow(10,9)*z\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "geoid_udf = udf(geoid)\n",
    "\n",
    "#geoid = lambda x,y,z: x*100 + 10**6*y + 10**9*z\n",
    "fha_df = fha_df.withColumn(\"GEOID\",geoid_udf(\"Census_Tract_Number\",\"County_Code\",\"State_Code\"))\n",
    "\n",
    "fha_df.sort('State_Code').show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+\n",
      "|State_Code|sum(State_Code)|sum(NUM_ALL)|\n",
      "+----------+---------------+------------+\n",
      "|        31|          16554|       18367|\n",
      "|        53|          76797|       68937|\n",
      "|        34|          66708|       58948|\n",
      "|        28|          17892|       16140|\n",
      "|        26|          64480|       73917|\n",
      "+----------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby aggregate\n",
    "fha_df_by_state = fha_df.select('State_Code', 'NUM_ALL').groupBy('State_Code').sum()\n",
    "fha_df_by_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+\n",
      "|State_Code|sum(State_Code)|sum(NUM_FHA)|\n",
      "+----------+---------------+------------+\n",
      "|        31|          16554|        4808|\n",
      "|        53|          76797|       15026|\n",
      "|        34|          66708|       14056|\n",
      "|        28|          17892|        3825|\n",
      "|        26|          64480|       23131|\n",
      "+----------+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby aggregate\n",
    "fha_df_by_state = fha_df.select('State_Code', 'NUM_FHA').groupBy('State_Code').sum()\n",
    "fha_df_by_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
