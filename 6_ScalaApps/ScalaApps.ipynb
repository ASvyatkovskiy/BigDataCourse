{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Spark applications with Scala API\n",
    "\n",
    "Apache Spark is written in Scala. Scala (along with Python and Java) is among three languages supported by Spark, and in fact Scala functionality is typically added the first to new Spark releases.\n",
    "\n",
    "\n",
    "## Preparing our first Scala Spark application\n",
    "\n",
    "Let us start with an PySpark application we have prepared on one of the previous steps. Here it is:\n",
    "\n",
    "```python\n",
    "#from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def main1(args):\n",
    "    start = time.time()\n",
    "    #sc = SparkContext(appName=\"LoadUnstructured\")\n",
    "\n",
    "    input_rdd = sc.textFile(\"../2_LoadingData/data/unstructured/\",10)\n",
    "    counts = input_rdd.flatMap(lambda line: line.split()) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    print \"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\"\n",
    "    print counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "    end = time.time()\n",
    "    print \"Elapsed time: \", (end-start)\n",
    "```\n",
    "\n",
    "The output of the function looks something like this:\n",
    "\n",
    "```bash\n",
    "\n",
    "Taking the 10 most frequent words in the text and corresponding frequencies:\n",
    "[(u'the', 22635), (u'of', 11167), (u'and', 11086), (u'to', 10707), (u'a', 10433), (u'I', 10183), (u'in', 7006), (u'that', 6911), (u'was', 6779), (u'his', 4955)]\n",
    "Elapsed time:  3.28145909309\n",
    "```\n",
    "\n",
    "The same application can be re-written in Scala as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "//import org.apache.spark.SparkContext._\n",
    "\n",
    "object WordCount {\n",
    "  def main() { \n",
    "\n",
    "    val t0 = System.nanoTime()\n",
    "    val conf = new SparkConf().setAppName(\"WordCount\")\n",
    "\n",
    "\n",
    "    val textFile = sc.textFile(\"../2_LoadingData/data/unstructured/\",10)\n",
    "    val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "                 .map(word => (word, 1))\n",
    "                 .reduceByKey(_ + _).map(x=>x.swap).sortByKey(false)\n",
    "                 \n",
    "    println(\"\\nTaking the 10 most frequent words in the text and corresponding frequencies:\")\n",
    "    for (d <- counts.take(10)) { \n",
    "        println(d)\n",
    "    }\n",
    "        \n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000000.0)\n",
    "    //Do not stop the SparkContext in the jupyter notebook.... sc.stop()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224428,)\n",
      "(22635,the)\n",
      "(11167,of)\n",
      "(11086,and)\n",
      "(10707,to)\n",
      "(10433,a)\n",
      "(10183,I)\n",
      "(7006,in)\n",
      "(6911,that)\n",
      "(6779,was)\n",
      "Elapsed time: 1.060742867\n"
     ]
    }
   ],
   "source": [
    "WordCount.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the course working area on Adroit and change to the folder for this exercise:\n",
    "\n",
    "```bash\n",
    "cd 6_ScalaApps\n",
    "```\n",
    "\n",
    "## Submitting Scala Spark application Q/A\n",
    "\n",
    "Q: So you've written some Spark code in Scala. How do you submit it to Spark and run it?  \n",
    "A: Use `sbt` or `maven` to package it into a Java jar, and submit it to Spark using `spark-submit`\n",
    "\n",
    "Q: What's a Java jar?  \n",
    "A: JAR (Java Archive) is a package file format typically used to aggregate many Java class files and associated metadata and resources (text, images, etc.) into one file to distribute application software or libraries on the Java platform.\n",
    "\n",
    "### Packaging with `sbt`\n",
    "\n",
    "**What is SBT?**  \n",
    "SBT is a modern build tool written in/for Scala, though it is also a general purpose build tool  \n",
    "\n",
    "**Why SBT?**\n",
    "- Good dependency management\n",
    "- Full Scala language support for creating tasks\n",
    "- Launch REPL in project context\n",
    "\n",
    "Create a root directory for your project and run:\n",
    "```bash\n",
    "mkdir -p src/{main,test}/{resources,scala}\n",
    "mkdir lib project\n",
    "```\n",
    "within it. \n",
    "\n",
    "This script will automatically create the proper `sbt` directory structure, which borrows from the Java `maven` directory structure. The script will also generate a template `build.sbt` file at the top of the directory that you should fill out with the appropriate versions and dependencies for your app.\n",
    "\n",
    "Then we can take our Scala code, and put it in the src folder (you should have it in the main folder, so just move it there):\n",
    "\n",
    "```bash\n",
    "mv WordCount.scala src/main/scala/\n",
    "```\n",
    "\n",
    "**Project Layout (Directory structure)**   \n",
    "\n",
    "`project` – project definition files  \n",
    "`project/build/` *yourproject* `.scala` – the main project definition file  \n",
    "`project/build.properties` – project, sbt and scala version definitions  \n",
    "`src/main` – your app code goes here, in a subdirectory indicating the code’s language (e.g. src/main/scala, src/main/java)  \n",
    "`src/main/resources` – static files you want added to your jar (e.g. logging config)  \n",
    "`src/test` – like src/main, but for tests  \n",
    "`lib_managed` – the jar files your project depends on. Populated by sbt update  \n",
    "`target` – the destination for generated stuff (e.g. generated thrift\n",
    "code, class files, jars)  \n",
    "\n",
    "#### `build.sbt`: Dependencies and versioning\n",
    "\n",
    "Example `simple.sbt` (located in the root directory of your project) \n",
    "\n",
    "```scala\n",
    "name := \"WordCount\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.7\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "    // Spark dependency\n",
    "    \"org.apache.spark\" % \"spark-core_2.11\" % \"2.0.0\" % \"provided\"\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "#### Assembly.sbt to build a fat Jar\n",
    "\n",
    "Example assembly.sbt located in the /project folder of your project:\n",
    "\n",
    "```scala\n",
    "resolvers += Resolver.url(\"artifactory\", url(\"http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases\"))(Resolver.ivyStylePatterns)\n",
    "\n",
    "addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.13.0\")\n",
    "```\n",
    "\n",
    "### Running (submitting a `jar` to Spark)\n",
    "\n",
    "1. Run `sbt assembly` in your project's home directory. The output to console will tell you the name and location of the resulting jar (under `./target`) \n",
    "\n",
    "You should now see the Jar file generated:\n",
    "```bash\n",
    "[alexeys@bd scala_spark] ll target/scala-2.11/\n",
    "total 6968\n",
    "drwxr-xr-x. 2 alexeys cses    4096 Dec  9 10:43 classes\n",
    "-rw-r--r--. 1 alexeys cses 7129172 Dec  9 10:43 WordCount-assembly-1.0.jar\n",
    "```\n",
    "\n",
    "2. In the Slurm batch script, use spark-submit as usual to submit the Spark app, but you would need to specify the --class and the path to jar from the current folder, for instance:\n",
    "\n",
    "```bash\n",
    "spark-submit --class \"WordCount\" --total-executor-cores target/scala-2.11/WordCount-assembly-1.0.jar\n",
    "```\n",
    "\n",
    "### Hands-on mini-exercise\n",
    "\n",
    "Run the above steps in your working area on Adroit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection via cross validation\n",
    "\n",
    "An important task in ML is model selection, or using data to find the best model or parameters for a given task. **Pipelines** facilitate model selection by making it easy to tune an entire **Pipeline** at once, rather than tuning each element in the **Pipelines** separately.\n",
    "\n",
    "Currently, **spark.ml** supports model selection using the **CrossValidator** class, which takes an Estimator, a set of ParamMaps, and an Evaluator. CrossValidator begins by splitting the dataset into a set of folds which are used as separate training and test datasets; e.g., with k=3 there is 3 folds, CrossValidator will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. CrossValidator iterates through the set of ParamMaps. For each ParamMap, it trains the given Estimator and evaluates it using the given Evaluator.\n",
    "\n",
    "The Evaluator can be a RegressionEvaluator for regression problems, a BinaryClassificationEvaluator for binary data, or a MultiClassClassificationEvaluator for multiclass problems. The default metric used to choose the best ParamMap can be overriden by the setMetric method in each of these evaluators.\n",
    "\n",
    "The ParamMap which produces the best evaluation metric (averaged over the k folds) is selected as the best model. CrossValidator finally fits the Estimator using the best ParamMap and the entire dataset.\n",
    "\n",
    "The following example demonstrates using CrossValidator to select from a grid of parameters. To help construct the parameter grid, we use the ParamGridBuilder utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.1256626071135714,0.8743373928864286], prediction=1.0\n",
      "(5, l m n) --> prob=[0.995215441016286,0.004784558983714055], prediction=0.0\n",
      "(6, mapreduce spark) --> prob=[0.3069689523262592,0.6930310476737409], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[0.8040279442401363,0.1959720557598636], prediction=0.0\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Prepare training data from a list of (id, text, label) tuples.\n",
    "val training = spark.createDataFrame(Seq(\n",
    "  (0L, \"a b c d e spark\", 1.0),\n",
    "  (1L, \"b d\", 0.0),\n",
    "  (2L, \"spark f g h\", 1.0),\n",
    "  (3L, \"hadoop mapreduce\", 0.0),\n",
    "  (4L, \"b spark who\", 1.0),\n",
    "  (5L, \"g d a y\", 0.0),\n",
    "  (6L, \"spark fly\", 1.0),\n",
    "  (7L, \"was mapreduce\", 0.0),\n",
    "  (8L, \"e spark program\", 1.0),\n",
    "  (9L, \"a e c l\", 0.0),\n",
    "  (10L, \"spark compile\", 1.0),\n",
    "  (11L, \"hadoop software\", 0.0)\n",
    ")).toDF(\"id\", \"text\", \"label\")\n",
    "\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "val lr = new LogisticRegression().setMaxIter(10)\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "// We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "val paramGrid = new ParamGridBuilder().addGrid(hashingTF.numFeatures, Array(10, 100, 1000)).addGrid(lr.regParam, Array(0.1, 0.01)).build()\n",
    "\n",
    "// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "// This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "// is areaUnderROC.\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(new BinaryClassificationEvaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2) // Use 3+ in practice\n",
    "\n",
    "// Run cross-validation, and choose the best set of parameters.\n",
    "val cvModel = cv.fit(training)\n",
    "\n",
    "// Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "val test = spark.createDataFrame(Seq(\n",
    "  (4L, \"spark i j k\"),\n",
    "  (5L, \"l m n\"),\n",
    "  (6L, \"mapreduce spark\"),\n",
    "  (7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "// Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "cvModel.transform(test).select(\"id\", \"text\", \"probability\", \"prediction\").collect().foreach { \n",
    "    case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n",
    "    println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on mini-exercise\n",
    "\n",
    "1) Move the ModelSelection_scala file into the folder designedtaed for Scala source files, and rename it to have a .scala extension:\n",
    "\n",
    "```bash\n",
    "mv ModelSelection\\_scala src/main/scala/ModelSelection.scala\n",
    "```\n",
    "\n",
    "The reason why the file name ends with \\_scala is because this way sbt is not going to try to include it in the build at the first place. A few additions are going to be required to make this build successful, which is a part of the exercise.\n",
    "\n",
    "\n",
    "2) Try building the new project:\n",
    "\n",
    "```bash\n",
    "sbt assembly\n",
    "```\n",
    "\n",
    "From the compiler error message, we can see that it does not recognize any of the Spark SQL or the ML packages. We did include those in the .scala source file, but we neeed to also include them in the SBT build file.\n",
    "\n",
    "Now open the simple.sbt and add the following lines to the dependencies:\n",
    "\n",
    "```bash\n",
    "  \"org.apache.spark\"  % \"spark-sql_2.11\" % \"2.0.0\" % \"provided\",\n",
    "  \"org.apache.spark\"  % \"spark-mllib_2.11\" % \"2.0.0\" % \"provided\" \n",
    "```  \n",
    "\n",
    "(watch out for the commas)\n",
    "\n",
    "Now issue the sbt assembly command again.\n",
    "\n",
    "We still see SBT complaining about SQLContext not being defined, let us look into the source file: we see that the SQLContext is not imported, nor is it defined in the main section of the application. You can accomplish both by uncommenting the lines which I have put in place for that purpose. And repeat the SBT assembly.\n",
    "\n",
    "We should be all set now!\n",
    "\n",
    "3) Make adjustments to the Slurm submission file if you find it necessary and submit the job to the cluster:\n",
    "\n",
    "```bash\n",
    "sbatch slurm_for_scala.cmd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
