{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine learning\n",
    "\n",
    "Let us consider a very simple machine learning example of logistic regression.\n",
    "Logistic regression is an iterative machine learning algorithm that seeks to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. It can be used to classify messages into spam vs non-spam, for example. Because the algorithm applies the same MapReduce operation repeatedly to the same dataset, it benefits greatly from caching the input in RAM across iterations.\n",
    "\n",
    "Spark MLlib is Spark’s scalable machine learning library consisting of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, and underlying optimization primitives.\n",
    "\n",
    "\n",
    "## Non-MLlib implementation\n",
    "\n",
    "First, let us consider the non-MLlib implementation and try to evaluate the effect of caching and partitioning on the perfromance. We're going to try to learn the rule that y(x) = 1 if x < fraction_positive, 0 otherwise\n",
    "\n",
    "Our training sample will be generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original random plane: [ 0.57319195  0.68650258]\n",
      "Final separating plane: [ 1612.27200241   387.93397398]\n",
      "Elapsed time:  5.05624508858\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#It's a non-MLlib implementation, but I still use LabelPoint format, ok...\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "import numpy as np\n",
    "N = 10**4\n",
    "fraction_positive = 0.5\n",
    "\n",
    "def y(x):\n",
    "    return 1 if x < fraction_positive else 0\n",
    "\n",
    "def generate_sample():\n",
    "    sample_X = np.arange(0, 1, 1.0/N)\n",
    "    np.random.shuffle( sample_X) # In-place shuffle!\n",
    "    sample_Y = map(y, sample_X)\n",
    "    return (sample_X, sample_Y)\n",
    "\n",
    "(sample_X, sample_Y) = generate_sample()\n",
    "\n",
    "\n",
    "## By hand.  This is the example code taken from the Spark Examples on the website.\n",
    "#  This is much slower than the above code, so I'm not going to even run it (or extract predictions, or test it..)\n",
    "start = time.time()\n",
    "def logistic_by_hand(ITERATIONS,nparts):\n",
    "    points = ( sc.parallelize( zip(sample_X, sample_Y), nparts)\n",
    "                 .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "                 .cache() )\n",
    "    w = np.random.ranf(size = 2) # current separating plane\n",
    "    print \"Original random plane: %s\" % w\n",
    "    for i in xrange(ITERATIONS):\n",
    "        gradient = points.map(\n",
    "            lambda pt: (1 / (1 + np.exp(-pt.label*(w.dot(pt.features)))) - 1) * pt.label * pt.features\n",
    "        ).reduce(lambda a, b: a + b)\n",
    "        w -= gradient\n",
    "    print \"Final separating plane: %s\" % w\n",
    "\n",
    "logistic_by_hand(20,3)\n",
    "end = time.time()\n",
    "print \"Elapsed time: \", (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## MLlib based implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexey/Desktop/BigDataCourse/local_spark/spark-2.1.0/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.8964\n",
      "Elapsed time:  3.60282087326\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "## Using MLLib and it's data structures.  This is fairly quick.\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "points = ( sc.parallelize( zip(sample_X, sample_Y), 3)\n",
    "             .map(lambda (x,y): LabeledPoint(y, [1, x]))\n",
    "             .cache() )\n",
    "model = LogisticRegressionWithSGD.train(points)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = points.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(points.count())\n",
    "print \"Accuracy on training set: %s\" % (1 - trainErr)\n",
    "end = time.time()\n",
    "print \"Elapsed time: \", (end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hands-on mini-exercise\n",
    "\n",
    "1. Play with the `fraction_positive` parameter: What happens to the accuracy measure as `fraction_positive` gets below 0.30 or above 0.70? (You should be somewhat disappointed with the results!)  What do you think is happening, and can you improve on it?\n",
    "1. Play with the \"by hand\" version (.. after lowering N to say 10**4 or so): Figure out what it's actually doing and how to use it to get results.  How much slower than the MLLib version does it seem to be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Different ML classifiers (skip for now, read as a homework)\n",
    "\n",
    "Finally, we will play with various ML classifiers available on the market.\n",
    "\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "A decision tree is a binary tree.  At each of the internal nodes, it chooses a feature $i$ and a threshold $t$.  Each leaf has a value.  Evaluation of the model is just traversal of the tree from the root.  At each node, for example $j$, we go down the left branch if $X_{ji} \\le t$ and the right branch otherwise.  The value of the model $f(X_{ji})$ is the value at the value at the terminating leaf of this traveral.  Below, we show a picture of this on small decision tree trained on the iris data set.  Notice that each internal node has a decision criterion and each leaf has the breakdown of label classes left at this leaf of the tree.  \n",
    "\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "A random forest is just an ensemble of decision trees.  The predicted value is just the average of the trees (for both regression and classification problems - for classification problems, it is the probabilities that are averaged).  You can adjust `n_estimators` to change the number of trees in the forest.  If each tree is trained on the same subset of data, why aren't they identical?  Two reasons:\n",
    "1. **Subsampling**: each tree is actually trained on a random selected (with replacement) subset (i.e. bootstrap)\n",
    "1. **Maximum Features**: the optimal split comes from a randomly selected subset of the features.  In scikit-learn, this feature is controlled by `max_features`.\n",
    "\n",
    "### Random Forest Training Algorithm and Tuning Parameters\n",
    "\n",
    "A Random Forest is pretty straightforward to train once you know how a Decision Tree works.  In fact, their construction can even be parallelized.  \n",
    "\n",
    "Below, various parameters that affect decision tree and random forest training are discussed. \n",
    "\n",
    "The first two parameters we mention are the most important, and tuning them can often improve performance:\n",
    "\n",
    "**numTrees**: Number of trees in the forest.\n",
    "\n",
    "Increasing the number of trees will decrease the variance in predictions, improving the model’s test-time accuracy.\n",
    "Training time increases roughly linearly in the number of trees.\n",
    "\n",
    "\n",
    "**maxDepth**: Maximum depth of each tree in the forest.\n",
    "Increasing the depth makes the model more expressive and powerful. However, deep trees take longer to train and are also more prone to overfitting.\n",
    "In general, it is acceptable to train deeper trees when using random forests than when using a single decision tree. One tree is more likely to overfit than a random forest (because of the variance reduction from averaging multiple trees in the forest).\n",
    "\n",
    "The next two parameters generally do not require tuning. However, they can be tuned to speed up training.\n",
    "\n",
    "**subsamplingRate**: This parameter specifies the size of the dataset used for training each tree in the forest, as a fraction of the size of the original dataset. The default (1.0) is recommended, but decreasing this fraction can speed up training.\n",
    "\n",
    "**featureSubsetStrategy**: Number of features to use as candidates for splitting at each tree node. The number is specified as a fraction or function of the total number of features. Decreasing this number will speed up training, but can sometimes impact performance if too low.\n",
    "\n",
    "### Linear SVM\n",
    "\n",
    "The canonical Support Vector Machine is the linear one.  Assume we have two groups labeled by $y = \\pm 1$.  Then we are trying to find the line $\\beta$ such that $X \\beta + \\beta_0$ maximially separates the points in our two classes:\n",
    "\n",
    "If the two classes can be separated by a linear hyperplane (picture on the left), we want to maximize the **margin** $M$ of the **boundary region**.  A little bit of math can show us that finding the largest separation is actually solved by the minimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge 1 \\quad \\mbox{for } j = 1,\\ldots,N\n",
    "$$\n",
    "\n",
    "The picture and the equation are equivalent: in the picture we are setting the margin to be $M$ and finding the largest margin possible.  In the equation, we are setting the margin to be $1$ and finding the smallest $\\beta$ that will make that true.  So $\\beta$ and $M$ are related through $\\| \\beta \\| = \\frac{1}{M}$.  If the two classes cannot be separated (picture on the right), we will have to add a forgiveness terms $\\xi$,\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\beta_0} \\|\\beta\\| \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    " y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j) & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\xi_j \\ge 0 & \\mbox{for } j = 1,\\ldots,N \\\\\n",
    " \\sum_j \\xi_j \\le C\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "for some constant $C$.  The constant $C$ is an important tradeoff.  It corresponds to the total \"forgiveness budget\" (see the last constraint).  The larger $C$, the forgiveness we have and the wider the margin $M$ can be.  We can rewrite the constrained optimization problem as the primal Lagrangian function with Lagrange multipliers $\\alpha_j \\ge 0$, $\\mu_j \\ge 0$, and $\\gamma \\ge 0$,  for each of our three constraints:\n",
    "\n",
    "$$ L_P(\\gamma) = \\min_{\\beta, \\beta_0, \\xi} \\max_{\\alpha, \\mu} \\frac{1}{2} \\| \\beta \\|^2 - \\sum_j \\alpha_j \\left[y_j (X_{j \\cdot} \\cdot \\beta + \\beta_0 - (1-\\xi_j)\\right] - \\sum_j \\mu_j \\xi_j  + \\gamma \\sum_j \\xi_j$$\n",
    "\n",
    "There is a one-to-one correspondence between $\\gamma$ and $C$.  By taking first order conditions, first-order conditions, the dual Lagrangian problem can be formulated as\n",
    "\n",
    "$$\n",
    "L_D(\\gamma) = \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_{j, j'} \\alpha_j \\alpha_{j'} y_j y_{j'} X_{j \\cdot} \\cdot X_{j' \\cdot} \\,. \\\\\n",
    "\\mbox{subject to } \\left\\{ \\begin{array} {cl} \n",
    "0 = \\sum_j \\alpha_j y_j \\\\\n",
    "0 \\le \\alpha_j \\le \\gamma & \\mbox{for } j = 1,\\ldots,N\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "This is now a reasonably straightforward quadratic programming problem.  It is solved via [Sequential Minimization Optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization).  Once we have solved this problem for $\\alpha$, we can easily work out the coefficients from\n",
    "\n",
    "$$ \\beta = \\sum_j \\alpha_j y_j X_{j \\cdot} $$\n",
    "\n",
    "**Key takeaways**:\n",
    "1. Critically, only points inside the margin or on the wrong side of the margin ($j$ for which $\\xi_j > 0$) affect the SVM (see the picture).  This is intuitively clear from the picture.  In the dual form, this is because $\\alpha_j$ is the Lagrangian constraint corresponding to $y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) \\ge (1-\\xi_j)$ and Complementary Slackness shows tells us that $\\alpha_j > 0$ is non-zero only when the constraint is binding ($y_j (X_{j\\cdot} \\cdot \\beta + \\beta_0) = (1-\\xi_j)$), i.e. we're in the boundary region.  This is meaning the **Support Vector** in \"SVM\": only the vectors in the boundary-the **Support Vectors**-contribute to the solution.\n",
    "1. $C$ or $\\gamma$ give a trade-off between the amount of forgiveness and the size of the margin or boundary region.  Hence, it controls how many points affect the SVM (based on the distance from the boundary).\n",
    "\n",
    "Below, we plot out a simple two-class linear SVM on some synthetic data\n",
    "\n",
    "\n",
    "### Non-linear SVM\n",
    "\n",
    "What if we don't believe that our data can be cleanly split by a linear hyperplane?  The common way to incorporate non-linear features is to have a non-linear function $h(X_{j\\cdot})$ (possibly to a higher-dimensional feature space with dimension $p'$ where $p' \\ge p$) and to train on that space.  One intuition is that there's a higher-dimensional space in which the data is has a linear separation and $h$ gives a non-linear mapping into that space.\n",
    "\n",
    "#### Kernel Trick\n",
    "\n",
    "The **Kernel Trick** in SVM tells us that rather than directly computing the (potentially very large) vectors $h(X_{j \\cdot})$, we can just modify the Kernel.  If we use the transformed data $h(X_{j \\cdot})$, the dual Lagrangian would be\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot}) $$\n",
    "\n",
    "We can rewrite\n",
    "\n",
    "$$h(X_{j \\cdot}) \\cdot h(X_{j' \\cdot})  = K(X_{j \\cdot}, X_{j' \\cdot})$$ \n",
    "\n",
    "for some non-linear Kernel $K$.  Our problem then becomes,\n",
    "\n",
    "$$ \\max_{\\alpha} \\sum_j \\alpha_j - \\frac{1}{2} \\sum_j \\sum_{j'} \\alpha_j \\alpha_{j'} y_j y_{j'} K(X_{j \\cdot}, X_{j' \\cdot}) $$\n",
    "\n",
    "There's a one-to-one correspondence between Kernel functions and functions $h$ (although $h$'s range may be infinite dimensional).  Some common Kernels include\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Kernel</th>\n",
    "<th>$K(x,x')$</th>\n",
    "<th>Scikit `kernel` parameter</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Linear Kernel</td>\n",
    "<td>$x \\cdot x'$</td>\n",
    "<td>`kernel='linear'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>$d$-th Degree Polynomial</td>\n",
    "<td>$(r + c x \\cdot x')^d$</td>\n",
    "<td>`kernel='poly'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Radial Kernel</td>\n",
    "<td>$ \\exp(- c \\|x - x' \\|^2) $</td>\n",
    "<td>`kernel='rbf'`</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>Neural Network Kernel</td>\n",
    "<td>$\\tanh(c x \\cdot x' + r)$</td>\n",
    "<td>`kernel='sigmoid'`</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The benefit of using a Kernel is that we don't have to compute a very high-dimensional (possibly infinite-dimensional) $h$.  All that complexity is just wrapped into the kernel $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic Regression Algorithm Guide\n",
    "\n",
    "This notebook provides an example of how you can perform Logistic Regression with the MLlib library.\n",
    "\n",
    "#### Algorithm Summary\n",
    "\n",
    "Task: Classification with binary or multiclass labels\n",
    "Input: Labels (binary or multiclass, 0-based indexed), Feature vectors (continuous, not categorical)\n",
    "For categorical features, use One-Hot Encoding to convert to binary features usable by Logistic Regression.\n",
    "Regularization: Logistic Regression, like other Generalized Linear Models (GLMs) in MLlib, support different types of regularization: None, L1, and L2.\n",
    "Elastic Net regularization, which mixes L1 and L2, is supported with the DataFrame-based ML Pipelines API\n",
    "\n",
    "#### Hints for Logistic Regression in Spark\n",
    "\n",
    "There are several APIs for Logistic Regression in Spark. Although this notebook demonstrates the RDD-based spark.mllib API, it is now recommended using the newer DataFrame-based spark.ml API. The DataFrame-based API includes faster, more robust algorithms and provides more information about the model learned.\n",
    "If you use the other APIs, the main items to be careful about are (a) which optimization algorithm is being run and (b) whether feature scaling is being used.\n",
    "Feature scaling refers to normalizing features (columns) to have unit variance. After training, the model weights are rescaled so that test data does not have to be normalized. This improves optimization (and often statistical) behavior, but it changes the effect of regularization since the same regularization parameter is used for all weights.\n",
    "\n",
    "#### APIs\n",
    "In *pyspark.mllib.classification* (original MLlib package with lower-level API)\n",
    "*LogisticRegressionWithSGD* does not use feature scaling.\n",
    "Since this does not use feature scaling, it is often important to tune (decrease) the step size to ensure convergence.\n",
    "If you decrease the step size, you may also need to increase the number of iterations.\n",
    "*LogisticRegressionWithLBFGS* uses feature scaling.\n",
    "In *pyspark.ml.classification* (newer MLlib package with higher-level API for Pipelines) (recommended)\n",
    "\n",
    "*LogisticRegression* uses feature scaling by default, but is adjustable.\n",
    "\n",
    "#### Load data\n",
    "\n",
    "You can load data from many sources in Spark. Here, we will load a hosted R dataset. However, you can also check out this guide for more info: Accessing Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read titanic data as DataFrame using spark-csv package, and cache it\n",
    "titanic = spark.read.options(header='true', inferSchema='true').csv('./data/titanic.csv').cache()\n",
    "titanic.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data is an observation-based version of the 1912 Titanic passenger survival log.\n",
    "\n",
    "#### Data Format\n",
    "\n",
    "A data frame with 1316 observations on the following 4 variables.\n",
    "\n",
    "**class**\n",
    "a factor with levels 1st class 2nd class 3rd class crew\n",
    "\n",
    "**age**\n",
    "a factor with levels child adults\n",
    "\n",
    "**sex**\n",
    "a factor with levels women man\n",
    "\n",
    "**survived**\n",
    "a factor with levels no yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Explore data\n",
    "This gives a quick idea of how to start exploring the data, and you can find more info here: Visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Class: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Freq: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+--------+----+\n",
      "|_c0|Class|   Sex|  Age|Survived|Freq|\n",
      "+---+-----+------+-----+--------+----+\n",
      "|  1|  1st|  Male|Child|      No|   0|\n",
      "|  2|  2nd|  Male|Child|      No|   0|\n",
      "|  3|  3rd|  Male|Child|      No|  35|\n",
      "|  4| Crew|  Male|Child|      No|   0|\n",
      "|  5|  1st|Female|Child|      No|   0|\n",
      "|  6|  2nd|Female|Child|      No|   0|\n",
      "|  7|  3rd|Female|Child|      No|  17|\n",
      "|  8| Crew|Female|Child|      No|   0|\n",
      "|  9|  1st|  Male|Adult|      No| 118|\n",
      "| 10|  2nd|  Male|Adult|      No| 154|\n",
      "| 11|  3rd|  Male|Adult|      No| 387|\n",
      "| 12| Crew|  Male|Adult|      No| 670|\n",
      "| 13|  1st|Female|Adult|      No|   4|\n",
      "| 14|  2nd|Female|Adult|      No|  13|\n",
      "| 15|  3rd|Female|Adult|      No|  89|\n",
      "| 16| Crew|Female|Adult|      No|   3|\n",
      "| 17|  1st|  Male|Child|     Yes|   5|\n",
      "| 18|  2nd|  Male|Child|     Yes|  11|\n",
      "| 19|  3rd|  Male|Child|     Yes|  13|\n",
      "| 20| Crew|  Male|Child|     Yes|   0|\n",
      "+---+-----+------+-----+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+--------+----+\n",
      "|_c0|Class|   Sex|  Age|Survived|Freq|\n",
      "+---+-----+------+-----+--------+----+\n",
      "|  1|  1st|  Male|Child|      No|   0|\n",
      "|  5|  1st|Female|Child|      No|   0|\n",
      "| 17|  1st|  Male|Child|     Yes|   5|\n",
      "| 21|  1st|Female|Child|     Yes|   1|\n",
      "+---+-----+------+-----+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic.where(\"Class like '1st' and Age like 'Child'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Alternatively, you can use the pure Spark SQL syntax as we have seen in the 3rd section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "titanic.registerTempTable(\"df_titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+-----+--------+----+\n",
      "|_c0|Class|   Sex|  Age|Survived|Freq|\n",
      "+---+-----+------+-----+--------+----+\n",
      "|  1|  1st|  Male|Child|      No|   0|\n",
      "|  5|  1st|Female|Child|      No|   0|\n",
      "| 17|  1st|  Male|Child|     Yes|   5|\n",
      "| 21|  1st|Female|Child|     Yes|   1|\n",
      "+---+-----+------+-----+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df_titanic WHERE Class like '1st' and Age like 'Child'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Preprocess data\n",
    "\n",
    "In this section, we convert string categorical columns into ordered indices usable by a linear model. Note that these categorical features are special: There is a natural ordering, so it makes sense to treat them as continuous. For general categorical features, you should probably use one-hot encoding instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 1st, 2nd, 3rd, Crew\n",
      "Age: Adult, Child\n",
      "Sex: Female, Male\n",
      "Survived: No, Yes\n"
     ]
    }
   ],
   "source": [
    "# Compute lists of string categories\n",
    "def getCategories(col):\n",
    "    vals = sorted(titanic.select(col).distinct().rdd.map(lambda x: x[0]).collect())\n",
    "    valDict = dict([(vals[i], i) for i in range(len(vals))])\n",
    "    print col + ': ' + ', '.join(vals)\n",
    "    return (vals, valDict)\n",
    "\n",
    "(classes, classDict) = getCategories(\"Class\")\n",
    "(ages, ageDict) = getCategories(\"Age\")\n",
    "(sexes, sexDict) = getCategories(\"Sex\")\n",
    "(survived, survivedDict) = getCategories(\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert the string categories into indices\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "classUDF = udf(lambda x: classDict[x], IntegerType())\n",
    "ageUDF = udf(lambda x: ageDict[x], IntegerType())\n",
    "sexUDF = udf(lambda x: sexDict[x], IntegerType())\n",
    "survivedUDF = udf(lambda x: survivedDict[x], IntegerType())\n",
    "\n",
    "titanicIndexed = titanic.select(classUDF(titanic[\"Class\"]).alias(\"class\"), ageUDF(titanic[\"Age\"]).alias(\"age\"), sexUDF(titanic[\"Sex\"]).alias(\"sex\"), survivedUDF(titanic[\"Survived\"]).alias(\"survived\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+--------+\n",
      "|class|age|sex|survived|\n",
      "+-----+---+---+--------+\n",
      "|    0|  1|  1|       0|\n",
      "|    1|  1|  1|       0|\n",
      "|    2|  1|  1|       0|\n",
      "|    3|  1|  1|       0|\n",
      "|    0|  1|  0|       0|\n",
      "|    1|  1|  0|       0|\n",
      "|    2|  1|  0|       0|\n",
      "|    3|  1|  0|       0|\n",
      "|    0|  0|  1|       0|\n",
      "|    1|  0|  1|       0|\n",
      "|    2|  0|  1|       0|\n",
      "|    3|  0|  1|       0|\n",
      "|    0|  0|  0|       0|\n",
      "|    1|  0|  0|       0|\n",
      "|    2|  0|  0|       0|\n",
      "|    3|  0|  0|       0|\n",
      "|    0|  1|  1|       1|\n",
      "|    1|  1|  1|       1|\n",
      "|    2|  1|  1|       1|\n",
      "|    3|  1|  1|       1|\n",
      "+-----+---+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanicIndexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Train a model\n",
    "We now train a Logistic Regression model using LogisticRegressionWithSGD. Since we will use the traditional MLlib API (not the Pipelines API), we first have to extract the label and features columns and create an RDD of LabeledPoints. (The Pipelines API takes DataFrames instead of RDDs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert data to RDD of LabeledPoint\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "featureCols = [\"age\", \"sex\", \"class\"]\n",
    "titanicLabels = titanicIndexed.select(\"survived\").rdd.map(lambda row: row[0])\n",
    "titanicFeatures = titanicIndexed.select(*featureCols).rdd.map(lambda x: list(x)) #[x[0], x[1], x[2]])\n",
    "titanicData = titanicLabels.zip(titanicFeatures).map(lambda l_p: LabeledPoint(l_p[0], l_p[1])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned LogisticRegressionModel:\n",
      "\t Intercept: 0\n",
      "\t Feature\tWeight\n",
      "\t age\t\t-0\n",
      "\t sex\t\t-0\n",
      "\t class\t\t-0\n"
     ]
    }
   ],
   "source": [
    "# Train the model, and print the intercept and weight vector\n",
    "# We use L1 (sparsifying) regularization, but you can also use None or \"l2\".\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "lr = LogisticRegressionWithSGD.train(titanicData, regParam=0.1, regType=\"l1\", intercept=True, iterations=100)\n",
    "print 'Learned LogisticRegressionModel:'\n",
    "print '\\t Intercept: %g' % lr.intercept\n",
    "print '\\t Feature\\tWeight'\n",
    "for i in range(len(featureCols)):\n",
    "    print '\\t %s\\t\\t%g' % (featureCols[i], lr.weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example prediction:\n",
      "  features: [0, 1, 0]\n",
      "  prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# We can make a single prediction:\n",
    "oneInstance = [0, 1, 0]\n",
    "prediction = lr.predict(oneInstance)\n",
    "print 'Example prediction:'\n",
    "print '  features: ' + str(oneInstance)\n",
    "print '  prediction: %d' % prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# We can also make predictions on the whole dataset and compute accuracy\n",
    "import numpy\n",
    "\n",
    "def accuracy(model, labelsRDD, featuresRDD):\n",
    "    predictionsRDD = featuresRDD.map(lambda x: model.predict(x))\n",
    "    return labelsRDD.zip(predictionsRDD).map(lambda labelAndPred: labelAndPred[0] == labelAndPred[1]).mean()\n",
    "\n",
    "print 'Training accuracy: %g' % accuracy(lr, titanicLabels, titanicFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability of label 1: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Previously, we were making 0/1 predictions.  We can clear the model threshold to make soft predictions.\n",
    "# Note: Soft prediction are currently only supported for binary classification.\n",
    "lr.clearThreshold()\n",
    "print 'Predicted probability of label 1: %g' % lr.predict(oneInstance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example: Estimator, Transformer, and Param\n",
    "    \n",
    "This example covers the concepts of Estimator, Transformer, and Param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print \"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30 # Specify 1 Param, overwriting the original maxIter.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55}) # Specify multiple Params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# You can combine paramMaps, which are python dictionaries.\n",
    "paramMap2 = {lr.probabilityCol: \"myProbability\"} # Change output column name\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)\n",
    "\n",
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "\n",
    "# Prepare test data\n",
    "test = sqlContext.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=DenseVector([-1.0, 1.5, 1.3]), label=1.0, myProbability=DenseVector([0.0571, 0.9429]), prediction=1.0)\n",
      "Row(features=DenseVector([3.0, 2.0, -0.1]), label=0.0, myProbability=DenseVector([0.9239, 0.0761]), prediction=0.0)\n",
      "Row(features=DenseVector([0.0, 2.2, -1.5]), label=1.0, myProbability=DenseVector([0.1097, 0.8903]), prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "prediction = model2.transform(test)\n",
    "selected = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example: Pipeline. Hands-on exercise\n",
    "\n",
    "This example follows the simple text document Pipeline illustrated in the figures above.\n",
    "\n",
    "Now switch to the Adroit working area and proceed to the Pipelines exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
